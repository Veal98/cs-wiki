# ğŸ‰ è‡ªå®šä¹‰å±‚å’Œæ¨¡å‹

---

[`tf.keras`](https://tensorflow.google.cn/api_docs/python/tf/keras?hl=zh_cn) åŒ…å«äº†å„ç§å†…ç½®å±‚ï¼Œä¾‹å¦‚ï¼š

- å·ç§¯å±‚ï¼š`Conv1D`ã€`Conv2D`ã€`Conv3D`ã€`Conv2DTranspose`
- æ± åŒ–å±‚ï¼š`MaxPooling1D`ã€`MaxPooling2D`ã€`MaxPooling3D`ã€`AveragePooling1D`
- RNN å±‚ï¼š`GRU`ã€`LSTM`ã€`ConvLSTM2D`
- `BatchNormalization`ã€`Dropout`ã€`Embedding` ç­‰

ä½†æ˜¯ï¼Œå¦‚æœæ‰¾ä¸åˆ°æ‰€éœ€å†…å®¹ï¼Œå¯ä»¥é€šè¿‡åˆ›å»ºæ‚¨è‡ªå·±çš„å±‚æ¥æ–¹ä¾¿åœ°æ‰©å±• APIã€‚æ‰€æœ‰å±‚éƒ½ä¼šå­ç±»åŒ– `Layer` ç±»å¹¶å®ç°ä¸‹åˆ—æ–¹æ³•ï¼š

- `call` æ–¹æ³•ï¼Œç”¨äºæŒ‡å®šç”±å±‚å®Œæˆçš„è®¡ç®—ã€‚
- `build` æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºå±‚çš„æƒé‡ï¼ˆè¿™åªæ˜¯ä¸€ç§æ ·å¼çº¦å®šï¼Œå› ä¸ºæ‚¨ä¹Ÿå¯ä»¥åœ¨ `__init__` ä¸­åˆ›å»ºæƒé‡ï¼‰ã€‚

## 1. åˆ›å»ºå±‚

Keras çš„ä¸€ä¸ªä¸­å¿ƒæŠ½è±¡æ˜¯ `Layer` ç±»ï¼ˆçŠ¶æ€ï¼ˆæƒé‡ï¼‰å’Œéƒ¨åˆ†è®¡ç®—çš„ç»„åˆï¼‰ã€‚å±‚å°è£…äº†çŠ¶æ€ï¼ˆå±‚çš„â€œæƒé‡â€ï¼‰å’Œä»è¾“å…¥åˆ°è¾“å‡ºçš„è½¬æ¢ï¼ˆâ€œè°ƒç”¨â€ï¼Œå³å±‚çš„å‰å‘ä¼ é€’ï¼‰ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯†é›†è¿æ¥çš„å±‚ã€‚å®ƒå…·æœ‰ä¸€ä¸ªçŠ¶æ€ï¼šå˜é‡ `w` å’Œ `b`ã€‚

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_dim, units), dtype="float32"),
            trainable=True, # å¯è®­ç»ƒæƒé‡
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(units,), dtype="float32"), trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

æ‚¨å¯ä»¥åœ¨æŸäº›å¼ é‡è¾“å…¥ä¸Šé€šè¿‡è°ƒç”¨æ¥ä½¿ç”¨å±‚ï¼Œè¿™ä¸€ç‚¹å¾ˆåƒ Python å‡½æ•°ã€‚

> ğŸ’¡ å±‚çš„ `__call__()` æ–¹æ³•å°†åœ¨é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨è¿è¡Œæ„å»ºã€‚

```python
x = tf.ones((2, 2))
linear_layer = Linear(4, 2)
y = linear_layer(x)
print(y)
tf.Tensor(
[[ 0.02562864 -0.09071901 -0.13720123  0.0189665 ]
 [ 0.02562864 -0.09071901 -0.13720123  0.0189665 ]], shape=(2, 4), dtype=float32)
```

è¯·æ³¨æ„ï¼Œ**æƒé‡ `w` å’Œ `b` åœ¨è¢«è®¾ç½®ä¸ºå±‚ç‰¹æ€§åä¼šç”±å±‚è‡ªåŠ¨è·Ÿè¸ª**ï¼š

```python
assert linear_layer.weights == [linear_layer.w, linear_layer.b]
```

è¯·æ³¨æ„ï¼Œ**æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¸€ç§æ›´åŠ å¿«æ·çš„æ–¹å¼ä¸ºå±‚æ·»åŠ æƒé‡ï¼š`add_weight()` æ–¹æ³•**ï¼š

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        self.w = self.add_weight(
            shape=(input_dim, units), initializer="random_normal", trainable=True
        )
        self.b = self.add_weight(shape=(units,), initializer="zeros", trainable=True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b


x = tf.ones((2, 2))
linear_layer = Linear(4, 2)
y = linear_layer(x)
print(y)
tf.Tensor(
[[-0.05742075 -0.05801918 -0.06137164 -0.06648195]
 [-0.05742075 -0.05801918 -0.06137164 -0.06648195]], shape=(2, 4), dtype=float32)
```

## 2. å±‚å¯ä»¥å…·æœ‰ä¸å¯è®­ç»ƒæƒé‡

**é™¤äº†å¯è®­ç»ƒæƒé‡å¤–ï¼Œæ‚¨è¿˜å¯ä»¥å‘å±‚æ·»åŠ ä¸å¯è®­ç»ƒæƒé‡**ã€‚å¯¹å±‚è¿›è¡Œè®­ç»ƒæ—¶ï¼Œä¸å¿…åœ¨åå‘ä¼ æ’­æœŸé—´è€ƒè™‘æ­¤ç±»æƒé‡ã€‚

ä»¥ä¸‹æ˜¯æ·»åŠ å’Œä½¿ç”¨ä¸å¯è®­ç»ƒæƒé‡çš„æ–¹æ³• `trainable = False`ï¼š

```python
class ComputeSum(keras.layers.Layer):
    def __init__(self, input_dim):
        super(ComputeSum, self).__init__()
        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)

    def call(self, inputs):
        self.total.assign_add(tf.reduce_sum(inputs, axis=0))
        return self.total


x = tf.ones((2, 2))
my_sum = ComputeSum(2)
y = my_sum(x)
print(y.numpy())
y = my_sum(x)
print(y.numpy())
[2. 2.]
[4. 4.]
```

å®ƒæ˜¯ `layer.weights` çš„ä¸€éƒ¨åˆ†ï¼Œä½†è¢«å½’ç±»ä¸ºä¸å¯è®­ç»ƒæƒé‡ï¼š

```python
print("weights:", len(my_sum.weights))
print("non-trainable weights:", len(my_sum.non_trainable_weights))

# It's not included in the trainable weights:
print("trainable_weights:", my_sum.trainable_weights)
weights: 1
non-trainable weights: 1
trainable_weights: []
```

## 3. æœ€ä½³åšæ³•ï¼šå°†æƒé‡åˆ›å»ºæ¨è¿Ÿåˆ°å¾—çŸ¥è¾“å…¥çš„å½¢çŠ¶ä¹‹å

ä¸Šé¢çš„ `Linear` å±‚æ¥å—äº†ä¸€ä¸ª `input_dim` å‚æ•°ï¼Œç”¨äºè®¡ç®— `__init__()` ä¸­æƒé‡ `w` å’Œ `b` çš„å½¢çŠ¶ï¼š

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        self.w = self.add_weight(
            shape=(input_dim, units), initializer="random_normal", trainable=True
        )
        self.b = self.add_weight(shape=(units,), initializer="zeros", trainable=True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

<u>åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½äº‹å…ˆä¸çŸ¥é“è¾“å…¥çš„å¤§å°ï¼Œå¹¶å¸Œæœ›åœ¨å¾—çŸ¥è¯¥å€¼æ—¶ï¼ˆå¯¹å±‚è¿›è¡Œå®ä¾‹åŒ–åçš„æŸä¸ªæ—¶é—´ï¼‰å†å»¶è¿Ÿåˆ›å»ºæƒé‡ã€‚</u>

ğŸš© **åœ¨ Keras API ä¸­ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åœ¨å±‚çš„ `build(self, inputs_shape)` æ–¹æ³•ä¸­åˆ›å»ºå±‚çš„æƒé‡**ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
class Linear(keras.layers.Layer):
    def __init__(self, units=32):
        super(Linear, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer="random_normal",
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,), initializer="random_normal", trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

ç°åœ¨ï¼Œæ‚¨æœ‰äº†ä¸€ä¸ªå»¶è¿Ÿå¹¶å› æ­¤æ›´æ˜“ä½¿ç”¨çš„å±‚ï¼š

```python
# At instantiation, we don't know on what inputs this is going to get called
linear_layer = Linear(32)

# The layer's weights are created dynamically the first time the layer is called
y = linear_layer(x)
```

## 4. å±‚å¯é€’å½’ç»„åˆ

å¦‚æœå°†ä¸€ä¸ªå±‚å®ä¾‹åˆ†é…ä¸ºå¦ä¸€ä¸ªå±‚çš„ç‰¹æ€§ï¼Œåˆ™å¤–éƒ¨å±‚å°†å¼€å§‹è·Ÿè¸ªå†…éƒ¨å±‚çš„æƒé‡ã€‚

æˆ‘ä»¬å»ºè®®åœ¨ `__init__()` æ–¹æ³•ä¸­åˆ›å»ºæ­¤ç±»å­å±‚ï¼ˆç”±äºå­å±‚é€šå¸¸å…·æœ‰æ„å»ºæ–¹æ³•ï¼Œå®ƒä»¬å°†ä¸å¤–éƒ¨å±‚åŒæ—¶æ„å»ºï¼‰ã€‚

```python
# Let's assume we are reusing the Linear class
# with a `build` method that we defined above.


class MLPBlock(keras.layers.Layer):
    def __init__(self):
        super(MLPBlock, self).__init__()
        self.linear_1 = Linear(32)
        self.linear_2 = Linear(32)
        self.linear_3 = Linear(1)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.linear_2(x)
        x = tf.nn.relu(x)
        return self.linear_3(x)


mlp = MLPBlock()
y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights
print("weights:", len(mlp.weights))
print("trainable weights:", len(mlp.trainable_weights))
weights: 6
trainable weights: 6
```

## 5. `add_loss()` æ–¹æ³•

åœ¨ç¼–å†™å±‚çš„ `call()` æ–¹æ³•æ—¶ï¼Œæ‚¨å¯ä»¥åœ¨ç¼–å†™è®­ç»ƒå¾ªç¯æ—¶åˆ›å»ºéœ€è¦ç¨åä½¿ç”¨çš„æŸå¤±å¼ é‡ã€‚è¿™å¯ä»¥é€šè¿‡è°ƒç”¨ `self.add_loss(value)` æ¥å®ç°ï¼š

```python
# A layer that creates an activity regularization loss
class ActivityRegularizationLayer(keras.layers.Layer):
    def __init__(self, rate=1e-2):
        super(ActivityRegularizationLayer, self).__init__()
        self.rate = rate

    def call(self, inputs):
        self.add_loss(self.rate * tf.reduce_sum(inputs))
        return inputs
```

è¿™äº›æŸå¤±ï¼ˆåŒ…æ‹¬ç”±ä»»ä½•å†…éƒ¨å±‚åˆ›å»ºçš„æŸå¤±ï¼‰å¯é€šè¿‡ `layer.losses` å–å›ã€‚æ­¤å±æ€§ä¼šåœ¨æ¯ä¸ª `__call__()` å¼€å§‹æ—¶é‡ç½®åˆ°é¡¶å±‚ï¼Œå› æ­¤ **`layer.losses` å§‹ç»ˆåŒ…å«åœ¨ä¸Šä¸€æ¬¡å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­åˆ›å»ºçš„æŸå¤±å€¼**ã€‚

```python
class OuterLayer(keras.layers.Layer):
    def __init__(self):
        super(OuterLayer, self).__init__()
        self.activity_reg = ActivityRegularizationLayer(1e-2)

    def call(self, inputs):
        return self.activity_reg(inputs)


layer = OuterLayer()
assert len(layer.losses) == 0  # No losses yet since the layer has never been called

_ = layer(tf.zeros(1, 1))
assert len(layer.losses) == 1  # We created one loss value

# `layer.losses` gets reset at the start of each __call__
_ = layer(tf.zeros(1, 1))
assert len(layer.losses) == 1  # This is the loss created during the call above
```

æ­¤å¤–ï¼Œ`loss` å±æ€§è¿˜åŒ…å«ä¸ºä»»ä½•å†…éƒ¨å±‚çš„æƒé‡åˆ›å»ºçš„æ­£åˆ™åŒ–æŸå¤±ï¼š

```python
class OuterLayerWithKernelRegularizer(keras.layers.Layer):
    def __init__(self):
        super(OuterLayerWithKernelRegularizer, self).__init__()
        self.dense = keras.layers.Dense(
            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)
        )

    def call(self, inputs):
        return self.dense(inputs)


layer = OuterLayerWithKernelRegularizer()
_ = layer(tf.zeros((1, 1)))

# This is `1e-3 * sum(layer.dense.kernel ** 2)`,
# created by the `kernel_regularizer` above.
print(layer.losses)
[<tf.Tensor: shape=(), dtype=float32, numpy=0.0013254517>]
```

è¿™äº›æŸå¤±è¿˜å¯ä»¥æ— ç¼ä½¿ç”¨ `fit()`ï¼ˆå®ƒä»¬ä¼šè‡ªåŠ¨æ±‚å’Œå¹¶æ·»åŠ åˆ°ä¸»æŸå¤±ä¸­ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰ï¼š

```python
import numpy as np

inputs = keras.Input(shape=(3,))
outputs = ActivityRegularizationLayer()(inputs)
model = keras.Model(inputs, outputs)

# If there is a loss passed in `compile`, thee regularization
# losses get added to it
model.compile(optimizer="adam", loss="mse")
model.fit(np.random.random((2, 3)), np.random.random((2, 3)))

# It's also possible not to pass any loss in `compile`,
# since the model already has a loss to minimize, via the `add_loss`
# call during the forward pass!
model.compile(optimizer="adam")
model.fit(np.random.random((2, 3)), np.random.random((2, 3)))
1/1 [==============================] - 0s 1ms/step - loss: 0.2453
1/1 [==============================] - 0s 1ms/step - loss: 0.0325

<tensorflow.python.keras.callbacks.History at 0x7fd6fc212240>
```

## 6. `add_metric()` æ–¹æ³•

ä¸ `add_loss()` ç±»ä¼¼ï¼Œå±‚è¿˜å…·æœ‰ `add_metric()` æ–¹æ³•ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªæ•°é‡çš„ç§»åŠ¨å¹³å‡å€¼ã€‚

è¯·æ€è€ƒä¸‹é¢çš„ "logistic endpoint" å±‚ã€‚å®ƒå°†é¢„æµ‹å’Œç›®æ ‡ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—é€šè¿‡ `add_loss()` è·Ÿè¸ªçš„æŸå¤±ï¼Œå¹¶è®¡ç®—é€šè¿‡ `add_metric()` è·Ÿè¸ªçš„å‡†ç¡®ç‡æ ‡é‡ã€‚

```python
class LogisticEndpoint(keras.layers.Layer):
    def __init__(self, name=None):
        super(LogisticEndpoint, self).__init__(name=name)
        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)
        self.accuracy_fn = keras.metrics.BinaryAccuracy()

    def call(self, targets, logits, sample_weights=None):
        # Compute the training-time loss value and add it
        # to the layer using `self.add_loss()`.
        loss = self.loss_fn(targets, logits, sample_weights)
        self.add_loss(loss)

        # Log accuracy as a metric and add it
        # to the layer using `self.add_metric()`.
        acc = self.accuracy_fn(targets, logits, sample_weights)
        self.add_metric(acc, name="accuracy")

        # Return the inference-time prediction tensor (for `.predict()`).
        return tf.nn.softmax(logits)
```

å¯é€šè¿‡ `layer.metrics` è®¿é—®ä»¥è¿™ç§æ–¹å¼è·Ÿè¸ªçš„æŒ‡æ ‡ï¼š

```python
layer = LogisticEndpoint()

targets = tf.ones((2, 2))
logits = tf.ones((2, 2))
y = layer(targets, logits)

print("layer.metrics:", layer.metrics)
print("current accuracy value:", float(layer.metrics[0].result()))
layer.metrics: [<tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7fd709b24240>]
current accuracy value: 1.0
```

å’Œ `add_loss()` ä¸€æ ·ï¼Œè¿™äº›æŒ‡æ ‡ä¹Ÿæ˜¯é€šè¿‡ `fit()` è·Ÿè¸ªçš„ï¼š

```python
inputs = keras.Input(shape=(3,), name="inputs")
targets = keras.Input(shape=(10,), name="targets")
logits = keras.layers.Dense(10)(inputs)
predictions = LogisticEndpoint(name="predictions")(logits, targets)

model = keras.Model(inputs=[inputs, targets], outputs=predictions)
model.compile(optimizer="adam")

data = {
    "inputs": np.random.random((3, 3)),
    "targets": np.random.random((3, 10)),
}
model.fit(data)
1/1 [==============================] - 0s 2ms/step - loss: 1.0657 - binary_accuracy: 0.0000e+00

<tensorflow.python.keras.callbacks.History at 0x7fd6fc0cca20>
```

## 7. `Model` ç±»

**é€šå¸¸ï¼Œæ‚¨ä¼šä½¿ç”¨ `Layer` ç±»æ¥å®šä¹‰å†…éƒ¨è®¡ç®—å—ï¼Œå¹¶ä½¿ç”¨ `Model` ç±»æ¥å®šä¹‰å¤–éƒ¨æ¨¡å‹ï¼Œå³æ‚¨å°†è®­ç»ƒçš„å¯¹è±¡**ã€‚

ä¾‹å¦‚ï¼Œåœ¨ ResNet50 æ¨¡å‹ä¸­ï¼Œæ‚¨ä¼šæœ‰å‡ ä¸ªå­ç±»åŒ– `Layer` çš„ ResNet å—ï¼Œå’Œä¸€ä¸ªåŒ…å«äº†æ•´ä¸ª ResNet50 ç½‘ç»œçš„å•ä¸ª `Model`ã€‚

`Model` ç±»å…·æœ‰ä¸ `Layer` ç›¸åŒçš„ APIï¼Œä½†æœ‰å¦‚ä¸‹åŒºåˆ«ï¼š

- å®ƒä¼šå…¬å¼€å†…ç½®è®­ç»ƒã€è¯„ä¼°å’Œé¢„æµ‹å¾ªç¯ï¼ˆ`model.fit()`ã€`model.evaluate()`ã€`model.predict()`ï¼‰ã€‚
- å®ƒä¼šé€šè¿‡ `model.layers` å±æ€§å…¬å¼€å…¶å†…éƒ¨å±‚çš„åˆ—è¡¨ã€‚
- å®ƒä¼šå…¬å¼€ä¿å­˜å’Œåºåˆ—åŒ– APIï¼ˆ`save()`ã€`save_weights()`â€¦ï¼‰

å®é™…ä¸Šï¼Œ**`Layer` ç±»å¯¹åº”äºæˆ‘ä»¬åœ¨æ–‡çŒ®ä¸­æ‰€ç§°çš„ â€œå±‚â€ **ï¼ˆå¦‚â€œå·ç§¯å±‚â€æˆ–â€œå¾ªç¯å±‚â€ï¼‰æˆ–â€œå—â€ï¼ˆå¦‚â€œResNet å—â€æˆ–â€œInception å—â€ï¼‰ã€‚

åŒæ—¶ï¼Œ**`Model` ç±»å¯¹åº”äºæ–‡çŒ®ä¸­æ‰€ç§°çš„ â€œæ¨¡å‹â€ **ï¼ˆå¦‚â€œæ·±åº¦å­¦ä¹ æ¨¡å‹â€ï¼‰æˆ–â€œç½‘ç»œâ€ï¼ˆå¦‚â€œæ·±åº¦ç¥ç»ç½‘ç»œâ€ï¼‰ã€‚

å› æ­¤ï¼Œå¦‚æœæ‚¨æƒ³çŸ¥é“â€œæˆ‘åº”è¯¥ç”¨ `Layer` ç±»è¿˜æ˜¯ `Model` ç±»ï¼Ÿâ€ï¼Œè¯·é—®è‡ªå·±ï¼šæˆ‘æ˜¯å¦éœ€è¦åœ¨å®ƒä¸Šé¢è°ƒç”¨ `fit()`ï¼Ÿæˆ‘æ˜¯å¦éœ€è¦åœ¨å®ƒä¸Šé¢è°ƒç”¨ `save()`ï¼Ÿå¦‚æœæ˜¯ï¼Œåˆ™ä½¿ç”¨ `Model`ã€‚å¦‚æœä¸æ˜¯ï¼ˆè¦ä¹ˆå› ä¸ºæ‚¨çš„ç±»åªæ˜¯æ›´å¤§ç³»ç»Ÿä¸­çš„ä¸€ä¸ªå—ï¼Œè¦ä¹ˆå› ä¸ºæ‚¨æ­£åœ¨è‡ªå·±ç¼–å†™è®­ç»ƒå’Œä¿å­˜ä»£ç ï¼‰ï¼Œåˆ™ä½¿ç”¨ `Layer`ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ‹¿ä¸Šé¢çš„ mini-resnet ç¤ºä¾‹ä¸ºä¾‹ï¼Œç”¨å®ƒæ¥æ„å»ºä¸€ä¸ª `Model`ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡ `fit()` è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯ä»¥é€šè¿‡ `save_weights()` æ¥ä¿å­˜ï¼š

```python
class ResNet(tf.keras.Model):      
    def __init__(self):         
        super(ResNet, self).__init__()         
        self.block_1 = ResNetBlock()         
        self.block_2 = ResNetBlock()         
        self.global_pool = layers.GlobalAveragePooling2D()         
        self.classifier = Dense(num_classes)   
        
    def call(self, inputs):         
        x = self.block_1(inputs)         
        x = self.block_2(x)         
        x = self.global_pool(x)         
        return self.classifier(x)   
    
resnet = ResNet() 
dataset = resnet.fit(dataset, epochs=10) 
resnet.save(filepath)
```

## 8. å°ç»“

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‚¨å·²å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š

- `Layer` å°è£…äº†ï¼ˆåœ¨ `__init__()` æˆ– `build()` ä¸­åˆ›å»ºçš„ï¼‰çŠ¶æ€å’Œï¼ˆåœ¨ `call()` ä¸­å®šä¹‰çš„ï¼‰éƒ¨åˆ†è®¡ç®—ã€‚
- å±‚å¯ä»¥é€’å½’åµŒå¥—ä»¥åˆ›å»ºæ–°çš„æ›´å¤§çš„è®¡ç®—å—ã€‚
- å±‚å¯ä»¥é€šè¿‡ `add_loss()` å’Œ `add_metric()` åˆ›å»ºå¹¶è·Ÿè¸ªæŸå¤±ï¼ˆé€šå¸¸æ˜¯æ­£åˆ™åŒ–æŸå¤±ï¼‰ä»¥åŠæŒ‡æ ‡ã€‚
- æ‚¨è¦è®­ç»ƒçš„å¤–éƒ¨å®¹å™¨æ˜¯ `Model`ã€‚`Model` å°±åƒ `Layer`ï¼Œä½†æ˜¯æ·»åŠ äº†è®­ç»ƒå’Œåºåˆ—åŒ–å®ç”¨å·¥å…·ã€‚

## ğŸ“š References

- [TensorFlow 2 å®˜æ–¹æ–‡æ¡£](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
- [TensorFlow 2 å®˜æ–¹æŒ‡å—](https://tensorflow.google.cn/guide/tensor?hl=zh_cn#%E6%93%8D%E4%BD%9C%E5%BD%A2%E7%8A%B6)
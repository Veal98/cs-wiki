# ğŸŒ¼ Keras ä¹‹åŸºæœ¬å›å½’é—®é¢˜

---

åœ¨ *å›å½’ (regression)* é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯é¢„æµ‹å‡ºå¦‚ä»·æ ¼æˆ–æ¦‚ç‡è¿™æ ·è¿ç»­å€¼çš„è¾“å‡ºã€‚ç›¸å¯¹äº*åˆ†ç±»(classification)* é—®é¢˜ï¼Œ*åˆ†ç±»(classification)* çš„ç›®çš„æ˜¯ä»ä¸€ç³»åˆ—çš„åˆ†ç±»å‡ºé€‰æ‹©å‡ºä¸€ä¸ªåˆ†ç±» ï¼ˆå¦‚ï¼Œç»™å‡ºä¸€å¼ åŒ…å«è‹¹æœæˆ–æ©˜å­çš„å›¾ç‰‡ï¼Œè¯†åˆ«å‡ºå›¾ç‰‡ä¸­æ˜¯å“ªç§æ°´æœï¼‰ã€‚

æœ¬æŒ‡å—ä½¿ç”¨ç»å…¸çš„ [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) æ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªç”¨æ¥é¢„æµ‹70å¹´ä»£æœ«åˆ°80å¹´ä»£åˆæ±½è½¦ **ç‡ƒæ²¹æ•ˆç‡ MPG** çš„æ¨¡å‹ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†ä¸ºè¯¥æ¨¡å‹æä¾›è®¸å¤šé‚£ä¸ªæ—¶æœŸçš„æ±½è½¦æè¿°ã€‚è¿™ä¸ªæè¿°åŒ…å«ï¼šæ°”ç¼¸æ•°ï¼Œæ’é‡ï¼Œé©¬åŠ›ä»¥åŠé‡é‡ã€‚æˆ‘ä»¬çš„ç›®çš„å°±æ˜¯æ ¹æ®è¿™äº›æ•°æ®æ‹Ÿåˆç‡ƒæ²¹æ•ˆç‡çš„ç›´çº¿/æ›²çº¿ã€‚

æœ¬æŒ‡å—ä½¿ç”¨äº† [tf.keras](https://tensorflow.google.cn/guide/keras?hl=zh_cn)ï¼Œå®ƒæ˜¯ TensorFlow ä¸­ç”¨æ¥æ„å»ºå’Œè®­ç»ƒæ¨¡å‹çš„é«˜çº§ APIã€‚

```python
import matplotlib.pyplot as plt
import pandas as pd

import tensorflow as tf

from tensorflow import keras

print(tf.__version__)
2.3.1
```

## 1. è·å– Auto MPG  æ•°æ®é›†

è¯¥æ•°æ®é›†å¯ä»¥ä» [UCIæœºå™¨å­¦ä¹ åº“](https://archive.ics.uci.edu/ml/) ä¸­è·å–.

é¦–å…ˆä¸‹è½½æ•°æ®é›†ã€‚

```python
# æ•°æ®æ–‡ä»¶è·¯å¾„
dataset_path = keras.utils.get_file("auto-mpg.data", "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
```

ä½¿ç”¨ pandas å¯¼å…¥æ•°æ®é›†ã€‚

```python
# åˆ—å
column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',
                'Acceleration', 'Model Year', 'Origin']

# read_csv è¯»å–æ–‡ä»¶
# å‚æ•°ï¼š
# 	dataset_pathï¼šæ–‡ä»¶è·¯å¾„
# 	names æŒ‡å®šåˆ—å
# 	sep æŒ‡å®šåˆ†å‰²ç¬¦ï¼Œé»˜è®¤æ˜¯ ,
# 	na_values: ç©ºå€¼å®šä¹‰ã€‚é»˜è®¤æƒ…å†µä¸‹, â€˜#N/Aâ€™, â€˜#N/A N/Aâ€™, â€˜#NAâ€™, â€˜-1.#INDâ€™, â€˜-1.#QNANâ€™, â€˜-NaNâ€™, â€˜-nanâ€™, â€˜1.#INDâ€™, â€˜1.#QNANâ€™, â€˜N/Aâ€™, â€˜NAâ€™, â€˜NULLâ€™, â€˜NaNâ€™, â€˜n/aâ€™, â€˜nanâ€™, â€˜nullâ€™. éƒ½è¡¨ç°ä¸º NAN
# 	skipinitialspace: å¿½ç•¥åˆ†éš”ç¬¦åçš„ç©ºæ ¼ é»˜è®¤false
# 	commentï¼šå¦‚æœè¯¥å­—ç¬¦å‡ºç°åœ¨è¡Œé¦–ï¼Œè¿™ä¸€è¡Œå°†è¢«å…¨éƒ¨å¿½ç•¥
raw_dataset = pd.read_csv(dataset_path, names=column_names,
                      na_values = "?", comment='\t',
                      sep=" ", skipinitialspace=True)

dataset = raw_dataset.copy()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107204750.png" style="zoom:67%;" />

## 2. æ•°æ®æ¸…æ´—

æ•°æ®é›†ä¸­åŒ…æ‹¬ä¸€äº›æœªçŸ¥å€¼ NANã€‚

```python
dataset.isna().sum()
MPG             0
Cylinders       0
Displacement    0
Horsepower      6
Weight          0
Acceleration    0
Model Year      0
Origin          0
dtype: int64
```

ä¸ºäº†ä¿è¯è¿™ä¸ªåˆå§‹ç¤ºä¾‹çš„ç®€å•æ€§ï¼Œåˆ é™¤è¿™äº›åŒ…å« NAN æ•°æ®çš„è¡Œã€‚

```python
dataset = dataset.dropna()
```

**`"Origin" å‘æºåœ°` åˆ—å®é™…ä¸Šä»£è¡¨åˆ†ç±»ï¼Œå‘æºåœ°å…±æœ‰ä¸‰ç±» USAï¼ˆ1 è¡¨ç¤ºï¼‰ï¼ŒEuropeï¼ˆ2 è¡¨ç¤ºï¼‰ï¼ŒJapanï¼ˆ3 è¡¨ç¤ºï¼‰**ï¼Œæˆ‘ä»¬æŠŠå®ƒè½¬æ¢ä¸ºç‹¬çƒ­ç  ï¼ˆone-hotï¼‰:

```python
origin = dataset.pop('Origin')
dataset['USA'] = (origin == 1)*1.0
dataset['Europe'] = (origin == 2)*1.0
dataset['Japan'] = (origin == 3)*1.0
dataset.tail()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107205133.png" style="zoom:67%;" />

## 3. æ‹†åˆ†è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†

ç°åœ¨éœ€è¦å°†æ•°æ®é›†æ‹†åˆ†ä¸ºä¸€ä¸ªè®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªæµ‹è¯•æ•°æ®é›†ã€‚

æˆ‘ä»¬æœ€åå°†ä½¿ç”¨æµ‹è¯•æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

```python
train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)
```

## 4. æ•°æ®æ£€æŸ¥

æŸ¥çœ‹æ€»ä½“çš„æ•°æ®ç»Ÿè®¡:

```python
train_stats = train_dataset.describe()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107205339.png" style="zoom:67%;" />

```python
train_stats.pop("MPG")
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107205410.png" style="zoom:62%;" />

```python
train_stats = train_stats.transpose()
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107205440.png" style="zoom:67%;" />

## 5. ä»æ ‡ç­¾ä¸­åˆ†ç¦»ç‰¹å¾

å°†ç‰¹å¾å€¼ä»ç›®æ ‡å€¼æˆ–è€…"æ ‡ç­¾"ä¸­åˆ†ç¦»ã€‚ è¿™ä¸ªæ ‡ç­¾æ˜¯ä½ ä½¿ç”¨è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹çš„å€¼ã€‚

```python
train_labels = train_dataset.pop('MPG')
test_labels = test_dataset.pop('MPG')
```

## 6. ç‰¹å¾å½’ä¸€åŒ–

ä» `train_stats` éƒ¨åˆ†æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªç‰¹å¾çš„èŒƒå›´éƒ½æ˜¯ä¸åŒçš„ã€‚æˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œ**ç‰¹å¾å½’ä¸€åŒ–**ã€‚

ğŸš¨ æ³¨æ„ï¼š**ä»è®­ç»ƒé›†ä¸­ç”Ÿæˆçš„è¿™äº›ç»Ÿè®¡æ•°æ® `train_stats` ä¹Ÿä¼šç”¨äºå½’ä¸€åŒ–æµ‹è¯•æ•°æ®é›†**ã€‚æˆ‘ä»¬éœ€è¦å°†æµ‹è¯•æ•°æ®é›†æ”¾å…¥åˆ°ä¸å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ç›¸åŒçš„åˆ†å¸ƒä¸­ã€‚

```python
def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)
```

## 7. æ„å»ºæ¨¡å‹

è®©æˆ‘ä»¬æ¥æ„å»ºæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¼šä½¿ç”¨ä¸€ä¸ªâ€œé¡ºåºâ€æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªç´§å¯†ç›¸è¿çš„éšè—å±‚ï¼Œä»¥åŠè¿”å›å•ä¸ªã€è¿ç»­å€¼çš„è¾“å‡ºå±‚ã€‚æ¨¡å‹çš„æ„å»ºæ­¥éª¤åŒ…å«äºä¸€ä¸ªåå« 'build_model' çš„å‡½æ•°ä¸­

> ğŸ’¡ ä¹‹åæˆ‘ä»¬ä¼šåˆ©ç”¨è¯¥å‡½æ•°åˆ›å»ºæ”¹å–„åçš„æ¨¡å‹

```python
def build_model():
  # è®¾ç½®å±‚
  model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.RMSprop(0.001)
 
  # ç¼–è¯‘æ¨¡å‹
  model.compile(loss='mse', # å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ˜¯ç”¨äºå›å½’é—®é¢˜çš„å¸¸è§æŸå¤±å‡½æ•°
                optimizer=optimizer,
                # å¸¸è§çš„å›å½’æŒ‡æ ‡æ˜¯å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰
                metrics=['mae', 'mse'])
  return model

model = build_model()
```

ä½¿ç”¨ `.summary` æ–¹æ³•æ¥æ‰“å°è¯¥æ¨¡å‹çš„ç®€å•æè¿°ã€‚

```python
model.summary()

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 64)                640       
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 4,865
Trainable params: 4,865
Non-trainable params: 0
_________________________________________________________________
```

## 8. è®­ç»ƒæ¨¡å‹

### â‘  å‘æ¨¡å‹é¦ˆé€æ•°æ® fit

ğŸš¨ **æ³¨æ„ï¼Œåœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è½¬æˆ numpy æ ¼å¼ï¼Œå¦åˆ™ä¼šæŠ¥é”™**ï¼š

```python
type(normed_train_data)
pandas.core.frame.DataFrame

type(train_labels)
pandas.core.series.Series

normed_train_data = normed_train_data.to_numpy()
train_labels = train_labels.to_numpy()
```

å¯¹æ¨¡å‹è¿›è¡Œ1000ä¸ªå‘¨æœŸçš„è®­ç»ƒï¼Œå¹¶åœ¨ `history` å¯¹è±¡ä¸­è®°å½•è®­ç»ƒå’ŒéªŒè¯çš„å‡†ç¡®æ€§ã€‚

```python
history = model.fit(
  normed_train_data, train_labels,
  epochs=1000,  
  # validation_split ç”¨äºåœ¨æ²¡æœ‰æä¾›éªŒè¯é›†çš„æ—¶å€™ï¼ŒæŒ‰ä¸€å®šæ¯”ä¾‹ä»è®­ç»ƒé›†ä¸­å–å‡ºä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
  validation_split = 0.2,
  verbose=1
)
```

ä½¿ç”¨ `history` å¯¹è±¡ä¸­å­˜å‚¨çš„ç»Ÿè®¡ä¿¡æ¯å¯è§†åŒ–æ¨¡å‹çš„è®­ç»ƒè¿›åº¦ã€‚

```python
history_dict = history.history

history_dict.keys()
dict_keys(['loss', 'mae', 'mse', 'val_loss', 'val_mae', 'val_mse'])
```



```python
def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mae'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mae'],
           label = 'Val Error')
  plt.ylim([0,5])
  plt.legend()

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$]')
  plt.plot(hist['epoch'], hist['mse'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mse'],
           label = 'Val Error')
  plt.ylim([0,20])
  plt.legend()
  plt.show()


plot_history(history)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107212720.png" style="zoom:67%;" />

è¯¥å›¾è¡¨æ˜¾ç¤ºåœ¨çº¦100ä¸ª epochs ä¹‹åè¯¯å·®éä½†æ²¡æœ‰æ”¹è¿›ï¼Œåè€Œå‡ºç°æ¶åŒ–ã€‚ æˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œæ”¹å–„ ğŸ‘‡

### â‘¡ æ—©æœŸåœæ­¢é˜²æ­¢è¿‡æ‹Ÿåˆ

è®©æˆ‘ä»¬æ›´æ–° `model.fit` è°ƒç”¨ï¼Œ**å½“éªŒè¯å€¼æ²¡æœ‰æé«˜æ—¶è‡ªåŠ¨åœæ­¢è®­ç»ƒ**ã€‚ æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ª *EarlyStopping callback* æ¥æµ‹è¯•æ¯ä¸ª epoch çš„è®­ç»ƒæ¡ä»¶ã€‚å¦‚æœç»è¿‡ä¸€å®šæ•°é‡çš„ epochs åæ²¡æœ‰æ”¹è¿›ï¼Œåˆ™è‡ªåŠ¨åœæ­¢è®­ç»ƒã€‚

> ğŸ’¡ `EarlyStopping` çš„å‚æ•°æœ‰
>
> - `monitor`: ç›‘æ§çš„æ•°æ®æ¥å£ï¼Œæœ‰â€™accâ€™,â€™val_accâ€™,â€™lossâ€™,â€™val_lossâ€™ç­‰ç­‰ã€‚æ­£å¸¸æƒ…å†µä¸‹å¦‚æœæœ‰éªŒè¯é›†ï¼Œå°±ç”¨â€™val_accâ€™æˆ–è€…â€™val_lossâ€™ã€‚
> - `min_delta`ï¼šå¢å¤§æˆ–å‡å°çš„é˜ˆå€¼ï¼Œåªæœ‰å¤§äºè¿™ä¸ªéƒ¨åˆ†æ‰ç®—ä½œ improvementã€‚è¿™ä¸ªå€¼çš„å¤§å°å–å†³äºmonitorï¼Œä¹Ÿåæ˜ äº†ä½ çš„å®¹å¿ç¨‹åº¦ã€‚ä¾‹å¦‚ monitor çš„å˜åŒ–èŒƒå›´åœ¨70%-90%ä¹‹é—´ï¼Œæ‰€ä»¥å¯¹äºå°äº0.01%çš„å˜åŒ–ä¸å…³å¿ƒã€‚åŠ ä¸Šè§‚å¯Ÿåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æŠ–åŠ¨çš„æƒ…å†µï¼ˆå³å…ˆä¸‹é™åä¸Šå‡ï¼‰ï¼Œæ‰€ä»¥é€‚å½“å¢å¤§å®¹å¿ç¨‹åº¦ï¼Œæœ€ç»ˆè®¾ä¸º0.003%ã€‚
> - `patience`ï¼šèƒ½å¤Ÿå®¹å¿å¤šå°‘ä¸ª epoch å†…éƒ½æ²¡æœ‰ improvementã€‚
> - `mode`: å°±â€™autoâ€™, â€˜minâ€™, â€˜,maxâ€™ä¸‰ä¸ªå¯èƒ½ã€‚å¦‚æœçŸ¥é“æ˜¯è¦ä¸Šå‡è¿˜æ˜¯ä¸‹é™ï¼Œå»ºè®®è®¾ç½®ä¸€ä¸‹ã€‚

```python
model = build_model()

# patience å€¼ç”¨æ¥æ£€æŸ¥æ”¹è¿› epochs çš„æ•°é‡
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

history = model.fit(normed_train_data, train_labels, epochs=1000,
                    validation_split = 0.2, verbose=0, callbacks=early_stop)

plot_history(history)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107213330.png" style="zoom:67%;" />

### â‘¢ åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡ evaluate

è®©æˆ‘ä»¬çœ‹çœ‹é€šè¿‡ä½¿ç”¨ **æµ‹è¯•é›†** æ¥æ³›åŒ–æ¨¡å‹çš„æ•ˆæœå¦‚ä½•ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶æ²¡æœ‰ä½¿ç”¨æµ‹è¯•é›†ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ï¼Œå½“æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›å®ƒé¢„æµ‹å¾—æœ‰å¤šå¥½ã€‚

```python
loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))
3/3 - 0s - loss: 5.9941 - mae: 1.8809 - mse: 5.9941
Testing set Mean Abs Error:  1.88 MPG
```

## 9. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ predict

ä½¿ç”¨æµ‹è¯•é›†ä¸­çš„æ•°æ®é¢„æµ‹ MPG å€¼:

```python
test_predictions = model.predict(normed_test_data).flatten()

plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
plt.axis('equal')
plt.axis('square')
plt.xlim([0,plt.xlim()[1]])
plt.ylim([0,plt.ylim()[1]])
_ = plt.plot([-100, 100], [-100, 100])
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201107213718.png" style="zoom:57%;" />

è¿™çœ‹èµ·æ¥æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹å¾—ç›¸å½“å¥½ã€‚

## 10. ç»“è®º

æœ¬æŒ‡å—ä»‹ç»äº†ä¸€äº›å¤„ç†å›å½’é—®é¢˜çš„æŠ€æœ¯ã€‚

- å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ˜¯ç”¨äºå›å½’é—®é¢˜çš„å¸¸è§æŸå¤±å‡½æ•°ï¼ˆåˆ†ç±»é—®é¢˜ä¸­ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ï¼‰ã€‚
- ç±»ä¼¼çš„ï¼Œç”¨äºå›å½’çš„è¯„ä¼°æŒ‡æ ‡ä¸åˆ†ç±»ä¸åŒã€‚ å¸¸è§çš„å›å½’æŒ‡æ ‡æ˜¯å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€‚
- å½“æ•°å­—è¾“å…¥æ•°æ®ç‰¹å¾çš„å€¼å­˜åœ¨ä¸åŒèŒƒå›´æ—¶ï¼Œæ¯ä¸ªç‰¹å¾åº”ç‹¬ç«‹ç¼©æ”¾åˆ°ç›¸åŒèŒƒå›´ã€‚
- å¦‚æœè®­ç»ƒæ•°æ®ä¸å¤šï¼Œä¸€ç§æ–¹æ³•æ˜¯é€‰æ‹©éšè—å±‚è¾ƒå°‘çš„å°ç½‘ç»œï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆã€‚
- æ—©æœŸåœæ­¢æ˜¯ä¸€ç§é˜²æ­¢è¿‡åº¦æ‹Ÿåˆçš„æœ‰æ•ˆæŠ€æœ¯ã€‚

## ğŸ“š References

- [TensorFlow 2 å®˜æ–¹æ–‡æ¡£](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
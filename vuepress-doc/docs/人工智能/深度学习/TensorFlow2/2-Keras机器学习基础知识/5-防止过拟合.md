# ğŸš é˜²æ­¢è¿‡æ‹Ÿåˆ

---

```python
import tensorflow
from tensorflow import keras
```

## 1. Add Weight Regularization

ç›¸æ¯”è¾ƒæ¥è¯´ï¼Œè¾ƒç®€å•çš„æ¨¡å‹æ¯”å¤æ‚çš„æ¨¡å‹æ›´ä¸å®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚

å°†æ¨¡å‹å˜ç®€å•ï¼Œé™¤äº†å°†æ¨¡å‹å˜å°ï¼ˆå‡å°‘ç½‘ç»œå±‚æ•°å’Œæ¯å±‚ç¥ç»å…ƒä¸ªæ•°ï¼‰ä»¥å¤–ï¼Œè¿˜æœ‰å¦ä¸€ç§æ–¹å¼ï¼Œå‡å°æ¨¡å‹æƒé‡(w)çš„ç†µ(entropy)ã€‚å³é™åˆ¶æƒé‡å€¼åœ¨ä¸€ä¸ªè¾ƒå°çš„èŒƒå›´å†…ï¼Œè¿™æ ·æ¨¡å‹ä¸­æƒé‡åˆ†å¸ƒçœ‹èµ·æ¥æ›´â€œregularâ€ï¼Œè¿™è¢«ç§°ä¸ºâ€œ`æƒé‡/å‚æ•°æ­£åˆ™åŒ–(weight regularization)`â€ï¼Œå®ƒæ˜¯é€šè¿‡å‘ç½‘ç»œçš„æŸå¤±å‡½æ•° loss ä¸­æ·»åŠ ä¸æƒé‡ç›¸å…³çš„ä»£ä»· cost æ¥å®Œæˆçš„ã€‚ æœ‰ä¸¤ç§ä»£ä»· costï¼Œä¹Ÿå³ä¸¤ç§æ­£åˆ™åŒ–æ–¹å¼ï¼š

- [L1 regularization](https://developers.google.cn/machine-learning/glossary/?hl=zh_cn#L1_regularization), where the cost added is proportionalï¼ˆæ­£æ¯”ï¼‰ to the <u>absolute value (ç»å¯¹å€¼)</u> of the <u>weights coefficients (æƒé‡ç³»æ•°)</u>
- [L2 regularization](https://developers.google.cn/machine-learning/glossary/?hl=zh_cn#L2_regularization), where the cost added is proportional to <u>the square of the value of the weights coefficients</u> .  L2 æ­£åˆ™åŒ–åœ¨ç¥ç»ç½‘ç»œä¸­ä¹Ÿè¢«ç§°ä¸º â€œæƒé‡è¡°å‡â€

L1 æ­£åˆ™åŒ–ä¼šå°†æƒé‡æ¨å‘ä¸º 0ï¼Œä»è€Œé¼“åŠ±ç¨€ç–æ¨¡å‹ã€‚è€Œ L2 æ­£åˆ™åŒ–è™½ç„¶æƒ©ç½šå‚æ•°ï¼Œä½†æ˜¯å¯¹äºå°æƒé‡æ¥è¯´ï¼Œä¸ä¼šä½¿ä»–ä»¬å˜ä¸º 0ï¼Œæ‰€ä»¥ **L2 æ­£åˆ™åŒ–æ›´ä¸ºå¸¸è§**ã€‚

In `tf.keras`, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now ğŸ‘‡

```python
l2_model = tf.keras.Sequential([
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001),
                 input_shape=(FEATURES,)),
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(512, activation='elu',
                 kernel_regularizer=regularizers.l2(0.001)),
    layers.Dense(1)
])
```

`l2(0.001)` means that <u>every coefficient in the weight matrix of the layer (è¯¥å±‚çš„æƒé‡çŸ©é˜µä¸­çš„æ¯ä¸ªç³»æ•°)</u> will add `0.001 * weight_coefficient_value**2` to the total **loss** of the network.

å› æ­¤ï¼Œå…·æœ‰ L2 æ­£åˆ™åŒ–æƒ©ç½šçš„ç›¸åŒâ€œå¤§å‹â€æ¨¡å‹çš„æ€§èƒ½è¦å¥½å¾—å¤š

## 2. Add Dropout

åœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒDropoutæ˜¯æœ€æœ‰æ•ˆçš„ä»¥åŠä½¿ç”¨æœ€å¹¿æ³›çš„æ­£åˆ™åŒ–æ–¹å¼ã€‚Dropoutä½œç”¨åœ¨ç½‘ç»œå±‚ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒ(dropping out)ä¸€éƒ¨åˆ†è¾“å‡ºå€¼ï¼ˆä¾‹å¦‚ç½®ä¸º0ï¼‰ï¼ŒDropoutçš„æ¯”ä¾‹ä¸€èˆ¬ç½®ä¸º0.2åˆ°0.5ä¹‹é—´ã€‚ä¾‹å¦‚ï¼š

```
[0.2, 0.3, 0.5, 0.7, 0.9]

# after 40% dropout

[0.2, 0, 0.5, 0.7, 0]
```

<u>In [`tf.keras`](https://tensorflow.google.cn/api_docs/python/tf/keras?hl=zh_cn) you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before.</u>

```python
dropout_model = tf.keras.Sequential([
    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),
    layers.Dropout(0.5),
    layers.Dense(512, activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(1)
])
```

## 3. Combined L2 + Dropout

```python
combined_model = tf.keras.Sequential([
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu', input_shape=(FEATURES,)),
    layers.Dropout(0.5),
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),
                 activation='elu'),
    layers.Dropout(0.5),
    layers.Dense(1)
])
```



## ğŸ“š References

- [TensorFlow 2 å®˜æ–¹æ–‡æ¡£](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
- [TensorFlow 2 å®˜æ–¹æŒ‡å—](https://tensorflow.google.cn/guide/tensor?hl=zh_cn#%E6%93%8D%E4%BD%9C%E5%BD%A2%E7%8A%B6)
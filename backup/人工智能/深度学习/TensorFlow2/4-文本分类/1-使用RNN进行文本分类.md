# ğŸ ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œ RNN è¿›è¡Œæ–‡æœ¬åˆ†ç±»

---

## 1. RNN æ¦‚è¿°

`å¾ªç¯ç¥ç»ç½‘ç»œ(Recurrent Neural Network, RNN)` å¹¿æ³›é€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸ (Natural Language Processing, NLP)ï¼ŒRNN æœ‰ä»€ä¹ˆæ˜¾è‘—çš„ç‰¹ç‚¹å‘¢ï¼Ÿæ™®é€šçš„ç¥ç»ç½‘ç»œï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºæ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œæ¯ä¸€å±‚ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œæ²¡æœ‰å…³ç³»ã€‚ä½†æ˜¯å¯¹äºè¯­è¨€æ¥è¯´ï¼Œä¸€å¥è¯ä¸­çš„å•è¯é¡ºåºä¸åŒï¼Œæ•´ä¸ªè¯­ä¹‰å°±å®Œå…¨å˜äº†ã€‚å› æ­¤è‡ªç„¶è¯­è¨€å¤„ç†å¾€å¾€éœ€è¦èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†åºåˆ—ä¿¡æ¯çš„ç¥ç»ç½‘ç»œï¼ŒRNN èƒ½å¤Ÿæ»¡è¶³è¿™ä¸ªéœ€æ±‚ã€‚

**RNN ä¸­éšè—å±‚çš„çŠ¶æ€ä¸ä»…å–å†³äºå½“å‰è¾“å…¥å±‚çš„è¾“å‡ºï¼Œè¿˜å’Œä¸Šä¸€æ­¥éšè—å±‚çš„çŠ¶æ€æœ‰å…³**ã€‚

é•¿çŸ­æœŸè®°å¿†æ¨¡å‹(Long short-term memory, LSTM)æ˜¯ä¸€ç§ç‰¹æ®Šçš„RNNï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³é•¿åºåˆ—è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯ç›¸æ¯”æ™®é€šçš„RNNï¼ŒLSTMèƒ½å¤Ÿåœ¨æ›´é•¿çš„åºåˆ—ä¸­æœ‰æ›´å¥½çš„è¡¨ç°ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨`tf.keras`æä¾›çš„ LSTM ç½‘ç»œå±‚æ­å»º RNN ç½‘ç»œæ¨¡å‹ï¼Œå¯¹ IMDB å½±è¯„æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚

## 2. ä¸‹è½½ IMDB æ•°æ®é›†

æ­¤æ–‡æœ¬åˆ†ç±»æ•™ç¨‹å°†åœ¨ [IMDB å¤§å‹ç”µå½±è¯„è®ºæ•°æ®é›†](http://ai.stanford.edu/~amaas/data/sentiment/)ä¸Šè®­ç»ƒ[å¾ªç¯ç¥ç»ç½‘ç»œ](https://developers.google.cn/machine-learning/glossary/?hl=zh_cn#recurrent_neural_network)ï¼ŒIMDB å¤§å‹ç”µå½±è¯„è®ºæ•°æ®é›†æ˜¯ä¸€ä¸ª*äºŒè¿›åˆ¶åˆ†ç±»*æ•°æ®é›†â€”â€”åˆ¤æ–­æŸæ¡ç”µå½±è¯„è®ºæ˜¯æ­£é¢è¯„è®ºè¿˜æ˜¯è´Ÿé¢è¯„è®ºã€‚

ä½¿ç”¨ [TFDS](https://tensorflow.google.cn/datasets?hl=zh_cn) ä¸‹è½½æ•°æ®é›†ã€‚

> ğŸ’¡ TensorFlow Datasets æä¾›äº†ä¸€ç³»åˆ—å¯ä»¥å’Œ TensorFlow é…åˆä½¿ç”¨çš„æ•°æ®é›†ã€‚å®ƒè´Ÿè´£ä¸‹è½½å’Œå‡†å¤‡æ•°æ®ï¼Œä»¥åŠæ„å»º [`tf.data.Dataset`](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset?hl=zh_cn)ã€‚

```python
import tensorflow_datasets as tfds
import tensorflow as tf
```

```python
dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,
                          as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']
```

æ•°æ®é›† `info` åŒ…æ‹¬ç¼–ç å™¨ ([`tfds.features.text.SubwordTextEncoder`](https://tensorflow.google.cn/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder?hl=zh_cn))ã€‚

```python
encoder = info.features['text'].encoder
print('Vocabulary size: {}'.format(encoder.vocab_size))
Vocabulary size: 8185 
```

## 3. å‡†å¤‡ç”¨äºè®­ç»ƒçš„æ•°æ®

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ `padded_batch` æ–¹æ³•å°†åºåˆ—å¡«å……è‡³æ‰¹æ¬¡ä¸­æœ€é•¿å­—ç¬¦ä¸²çš„é•¿åº¦ï¼š

```python
BUFFER_SIZE = 10000
BATCH_SIZE = 64
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE)

test_dataset = test_dataset.padded_batch(BATCH_SIZE)
```

## 4. åˆ›å»ºæ¨¡å‹

æ„å»ºä¸€ä¸ª [`tf.keras.Sequential`](https://tensorflow.google.cn/api_docs/python/tf/keras/Sequential?hl=zh_cn) æ¨¡å‹å¹¶ä»åµŒå…¥å‘é‡å±‚å¼€å§‹ã€‚åµŒå…¥å‘é‡å±‚æ¯ä¸ªå•è¯å­˜å‚¨ä¸€ä¸ªå‘é‡ã€‚è°ƒç”¨æ—¶ï¼Œå®ƒä¼šå°†å•è¯ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºå‘é‡åºåˆ—ã€‚è¿™äº›å‘é‡æ˜¯å¯è®­ç»ƒçš„ã€‚ï¼ˆåœ¨è¶³å¤Ÿçš„æ•°æ®ä¸Šï¼‰è®­ç»ƒåï¼Œå…·æœ‰ç›¸ä¼¼å«ä¹‰çš„å•è¯é€šå¸¸å…·æœ‰ç›¸ä¼¼çš„å‘é‡ã€‚

ä¸é€šè¿‡ [`tf.keras.layers.Dense`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Dense?hl=zh_cn) å±‚ä¼ é€’ç‹¬çƒ­ç¼–ç å‘é‡çš„ç­‰æ•ˆè¿ç®—ç›¸æ¯”ï¼Œè¿™ç§ç´¢å¼•æŸ¥æ‰¾æ–¹æ³•è¦é«˜æ•ˆå¾—å¤šã€‚

å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) é€šè¿‡éå†å…ƒç´ æ¥å¤„ç†åºåˆ—è¾“å…¥ã€‚RNN å°†è¾“å‡ºä»ä¸€ä¸ªæ—¶é—´æ­¥éª¤ä¼ é€’åˆ°å…¶è¾“å…¥ï¼Œç„¶åä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ­¥éª¤ã€‚

â­ æˆ‘ä»¬åœ¨ LSTM å±‚å¤–é¢å¥—äº†ä¸€ä¸ªå£³ (å±‚å°è£…å™¨, layer wrappers): `tf.keras.layers.Bidirectional`ï¼Œè¿™æ˜¯ RNN çš„**åŒå‘å°è£…å™¨**ï¼Œç”¨äºå¯¹åºåˆ—è¿›è¡Œå‰å‘å’Œåå‘è®¡ç®—ã€‚

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(encoder.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])
```

> ğŸ’¡ `Embedding` å¯ä»¥å°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°è¾ƒä½ç»´ç©ºé—´æ¥è§£å†³ç¨€ç–è¾“å…¥æ•°æ®é—®é¢˜
>
> IMDB æ•°æ®é›†çš„é¢„å¤„ç†æ˜¯æŒ‰ç…§å•è¯åœ¨ encoder ä¸­çš„ä¸‹æ ‡æ¥å¤„ç†çš„ï¼Œç»´åº¦(`encoder.vocab_size`)å¾ˆé«˜ä¹Ÿå¾ˆç¨€ç–ï¼Œç»è¿‡ `Embedding `å±‚çš„è½¬æ¢ï¼Œå°†äº§ç”Ÿå¤§å°å›ºå®šä¸º 64 çš„å‘é‡ã€‚è€Œä¸”è¿™ä¸ªè½¬æ¢æ˜¯å¯è®­ç»ƒçš„ï¼Œç»è¿‡è¶³å¤Ÿçš„è®­ç»ƒä¹‹åï¼Œç›¸ä¼¼è¯­ä¹‰çš„å¥å­å°†äº§ç”Ÿç›¸ä¼¼çš„å‘é‡ã€‚

**è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé€‰æ‹© Keras åºè´¯æ¨¡å‹ï¼Œå› ä¸ºæ¨¡å‹ä¸­çš„æ‰€æœ‰å±‚éƒ½åªæœ‰å•ä¸ªè¾“å…¥å¹¶äº§ç”Ÿå•ä¸ªè¾“å‡º**ã€‚å¦‚æœè¦ä½¿ç”¨æœ‰çŠ¶æ€ RNN å±‚ï¼Œåˆ™å¯èƒ½éœ€è¦ä½¿ç”¨ Keras å‡½æ•°å¼ API æˆ–æ¨¡å‹å­ç±»åŒ–æ¥æ„å»ºæ¨¡å‹ï¼Œä»¥ä¾¿å¯ä»¥æ£€ç´¢å’Œé‡ç”¨ RNN å±‚çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Keras RNN æŒ‡å—](https://tensorflow.google.cn/guide/keras/rnn?hl=zh_cn#rnn_state_reuse)ã€‚

ç¼–è¯‘ Keras æ¨¡å‹ä»¥é…ç½®è®­ç»ƒè¿‡ç¨‹ï¼š

```python
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])
```

## 5. è®­ç»ƒæ¨¡å‹

```python
history = model.fit(train_dataset, epochs=10,
                    validation_data=test_dataset, 
                    validation_steps=30)

test_loss, test_acc = model.evaluate(test_dataset)
391/391 [==============================] - 17s 43ms/step - loss: 0.4305 - accuracy: 0.8477
```

å¦‚æœé¢„æµ‹ >= 0.5ï¼Œåˆ™ä¸ºæ­£é¢è¯„è®ºï¼Œå¦åˆ™ä¸ºè´Ÿé¢è¯„è®ºã€‚

```python
def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])
  plt.show()

plot_graphs(history, 'accuracy')
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201109205410.png" style="zoom:67%;" />

```python
plot_graphs(history, 'loss')
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201109205432.png" style="zoom:67%;" />

## 6. å †å ä¸¤ä¸ªæˆ–æ›´å¤š LSTM å±‚

Keras å¾ªç¯å±‚æœ‰ä¸¤ç§å¯ç”¨çš„æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ç”± `return_sequences` æ„é€ å‡½æ•°å‚æ•°æ§åˆ¶ï¼š

- è¿”å›æ¯ä¸ªæ—¶é—´æ­¥éª¤çš„è¿ç»­è¾“å‡ºçš„å®Œæ•´åºåˆ—ï¼ˆå½¢çŠ¶ä¸º `(batch_size, timesteps, output_features)` çš„ 3D å¼ é‡ï¼‰ã€‚
- ä»…è¿”å›æ¯ä¸ªè¾“å…¥åºåˆ—çš„æœ€åä¸€ä¸ªè¾“å‡ºï¼ˆå½¢çŠ¶ä¸º (batch_size, output_features) çš„ 2D å¼ é‡ï¼‰ã€‚

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(encoder.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5), # éšæœºèˆå¼ƒ 0.5 çš„æƒé‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    tf.keras.layers.Dense(1)
])
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=10,
                    validation_data=test_dataset,
                    validation_steps=30)

test_loss, test_acc = model.evaluate(test_dataset)
391/391 [==============================] - 30s 78ms/step - loss: 0.5205 - accuracy: 0.8572

# predict on a sample text without padding.

sample_pred_text = ('The movie was not good. The animation and the graphics '
                    'were terrible. I would not recommend this movie.')
predictions = sample_predict(sample_pred_text, pad=False)
print(predictions)
[[-2.6377363]]
# predict on a sample text with padding

sample_pred_text = ('The movie was not good. The animation and the graphics '
                    'were terrible. I would not recommend this movie.')
predictions = sample_predict(sample_pred_text, pad=True)
print(predictions)
[[-3.0502243]]
plot_graphs(history, 'accuracy')
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201109210626.png" style="zoom:67%;" />

```python
plot_graphs(history, 'loss')
```

<img src="https://gitee.com/veal98/images/raw/master/img/20201109210635.png" style="zoom:67%;" />

## ğŸ“š References

- [TensorFlow 2 å®˜æ–¹æ–‡æ¡£](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
- [æå®¢å…”å…” - RNN LSTM æ–‡æœ¬åˆ†ç±»](https://geektutu.com/post/tf2doc-rnn-lstm-text.html#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86)
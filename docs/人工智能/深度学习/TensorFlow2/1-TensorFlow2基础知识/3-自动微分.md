# ğŸŒœ è‡ªåŠ¨å¾®åˆ†

---

æœ¬æ•™ç¨‹æ¶‰åŠ[è‡ªåŠ¨å¾®åˆ†ï¼ˆautomatic differentitationï¼‰](https://en.wikipedia.org/wiki/Automatic_differentiation)ï¼Œå®ƒæ˜¯ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…³é”®æŠ€å·§ä¹‹ä¸€ã€‚

## 1. æ¢¯åº¦å¸¦

TensorFlow ä¸ºè‡ªåŠ¨å¾®åˆ†æä¾›äº† [tf.GradientTape](https://tensorflow.google.cn/api_docs/python/tf/GradientTape?hl=zh_cn) API ï¼Œæ ¹æ®æŸä¸ªå‡½æ•°çš„è¾“å…¥å˜é‡æ¥è®¡ç®—å®ƒçš„å¯¼æ•°ã€‚Tensorflow ä¼šæŠŠ '`tf.GradientTape`' ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œçš„æ‰€æœ‰æ“ä½œéƒ½è®°å½•åœ¨ä¸€ä¸ªç£å¸¦ä¸Š ("tape")ã€‚ ç„¶ååŸºäºè¿™ä¸ªç£å¸¦å’Œæ¯æ¬¡æ“ä½œäº§ç”Ÿçš„å¯¼æ•°ï¼Œç”¨åå‘å¾®åˆ†æ³•ï¼ˆ"reverse mode differentiation"ï¼‰æ¥è®¡ç®—è¿™äº›è¢«â€œè®°å½•åœ¨æ¡ˆâ€çš„å‡½æ•°çš„å¯¼æ•°ã€‚

ä¾‹å¦‚ï¼š

```python
x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x) # å‡½æ•°è¡¨è¾¾å¼
  z = tf.multiply(y, y)

# Derivative of z with respect to the original input tensor x
dz_dx = t.gradient(z, x)
for i in [0, 1]:
  for j in [0, 1]:
    assert dz_dx[i][j].numpy() == 8.0
```

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ [`tf.GradientTape`](https://tensorflow.google.cn/api_docs/python/tf/GradientTape?hl=zh_cn) ä¸Šä¸‹æ–‡è®¡ç®—è¿‡ç¨‹äº§ç”Ÿçš„ä¸­é—´ç»“æœæ¥æ±‚å–å¯¼æ•°ã€‚

```python
x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x)
  z = tf.multiply(y, y)

# Use the tape to compute the derivative of z with respect to the
# intermediate value y.
dz_dy = t.gradient(z, y)
assert dz_dy.numpy() == 8.0
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œè°ƒç”¨ `GradientTape.gradient()` æ–¹æ³•æ—¶ï¼Œ GradientTape å ç”¨çš„èµ„æºä¼šç«‹å³å¾—åˆ°é‡Šæ”¾ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªæŒä¹…çš„æ¢¯åº¦å¸¦ï¼Œå¯ä»¥è®¡ç®—åŒä¸ªå‡½æ•°çš„å¤šä¸ªå¯¼æ•°ã€‚è¿™æ ·åœ¨ç£å¸¦å¯¹è±¡è¢«åƒåœ¾å›æ”¶æ—¶ï¼Œå°±å¯ä»¥å¤šæ¬¡è°ƒç”¨ '`gradient()`' æ–¹æ³•ã€‚ä¾‹å¦‚ï¼š

```python
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as t:
  t.watch(x)
  y = x * x  # å‡½æ•°è¡¨è¾¾å¼
  z = y * y 
dz_dx = t.gradient(z, x)  # y å¯¹ x çš„å¾®åˆ† 108.0 (4*x^3 at x = 3)
dy_dx = t.gradient(y, x)  # z å¯¹ x çš„å¾®åˆ† 6.0
del t  # Drop the reference to the tape
```

### â‘  è®°å½•æ§åˆ¶æµ

ç”±äºç£å¸¦ä¼šè®°å½•æ‰€æœ‰æ‰§è¡Œçš„æ“ä½œï¼ŒPython æ§åˆ¶æµï¼ˆå¦‚ä½¿ç”¨ if å’Œ while çš„ä»£ç æ®µï¼‰è‡ªç„¶å¾—åˆ°äº†å¤„ç†ã€‚

```python
def f(x, y):
  output = 1.0
  for i in range(y):
    if i > 1 and i < 5:
      output = tf.multiply(output, x)
  return output

def grad(x, y):
  with tf.GradientTape() as t:
    t.watch(x)
    out = f(x, y)
  return t.gradient(out, x)

x = tf.convert_to_tensor(2.0)

assert grad(x, 6).numpy() == 12.0
assert grad(x, 5).numpy() == 12.0
assert grad(x, 4).numpy() == 4.0
```

### â‘¡ é«˜é˜¶å¯¼æ•°

åœ¨ '`GradientTape`' ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­è®°å½•çš„æ“ä½œä¼šç”¨äºè‡ªåŠ¨å¾®åˆ†ã€‚å¦‚æœå¯¼æ•°æ˜¯åœ¨ä¸Šä¸‹æ–‡ä¸­è®¡ç®—çš„ï¼Œå¯¼æ•°çš„å‡½æ•°ä¹Ÿä¼šè¢«è®°å½•ä¸‹æ¥ã€‚å› æ­¤ï¼ŒåŒä¸ª API å¯ä»¥ç”¨äºé«˜é˜¶å¯¼æ•°ã€‚ä¾‹å¦‚ï¼š

```python
x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t:
  with tf.GradientTape() as t2:
    y = x * x * x
  # Compute the gradient inside the 't' context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t2.gradient(y, x)
d2y_dx2 = t.gradient(dy_dx, x)

assert dy_dx.numpy() == 3.0
assert d2y_dx2.numpy() == 6.0
```

## ğŸ“š References

- [TensorFlow 2 å®˜æ–¹æ–‡æ¡£](https://tensorflow.google.cn/tutorials/keras/classification?hl=zh_cn)
- [TensorFlow 2 å®˜æ–¹æŒ‡å—](https://tensorflow.google.cn/guide/tensor?hl=zh_cn#%E6%93%8D%E4%BD%9C%E5%BD%A2%E7%8A%B6)
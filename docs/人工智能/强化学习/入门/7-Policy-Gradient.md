# ğŸ’« ç­–ç•¥æ¢¯åº¦ Policy Gradient

---

> ğŸ’¡ **Policy Gradient** å±äº Value-Freeã€Policy-Based

å¯¹æ¯”èµ·ä»¥å€¼ä¸ºåŸºç¡€çš„æ–¹æ³•, Policy Gradients æ ¹æ®æ¦‚ç‡ç›´æ¥è¾“å‡ºåŠ¨ä½œçš„æœ€å¤§å¥½å¤„å°±æ˜¯, **å®ƒèƒ½åœ¨ä¸€ä¸ªè¿ç»­åŒºé—´å†…æŒ‘é€‰åŠ¨ä½œ**, è€Œ Value-Based çš„æ¯”å¦‚ Q-learning æ— æ³•å¾ˆå¥½çš„å¤„ç†è¿ç»­åŠ¨ä½œ

## 1. Policy Gradient æ ¸å¿ƒæ€æƒ³

å¦‚å›¾æ‰€ç¤º, è§‚æµ‹çš„ä¿¡æ¯é€šè¿‡ç¥ç»ç½‘ç»œåˆ†æ, é€‰å‡ºäº†å·¦è¾¹çš„è¡Œä¸º, æˆ‘ä»¬ç›´æ¥è¿›è¡Œåå‘ä¼ é€’, ä½¿ä¹‹ä¸‹æ¬¡è¢«é€‰çš„å¯èƒ½æ€§å¢åŠ , ä½†æ˜¯å¥–æƒ©ä¿¡æ¯å´å‘Šè¯‰æˆ‘ä»¬, è¿™æ¬¡çš„è¡Œä¸ºæ˜¯ä¸å¥½çš„, é‚£æˆ‘ä»¬çš„åŠ¨ä½œå¯èƒ½æ€§å¢åŠ çš„å¹…åº¦ éšä¹‹è¢«å‡ä½. è¿™æ ·å°±èƒ½**é å¥–åŠ±æ¥å·¦å³æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œåå‘ä¼ é€’**. 

![](https://gitee.com/veal98/images/raw/master/img/20201102094845.png)

å‡å¦‚è¿™æ¬¡çš„è§‚æµ‹ä¿¡æ¯è®©ç¥ç»ç½‘ç»œé€‰æ‹©äº†å³è¾¹çš„è¡Œä¸º, å³è¾¹çš„è¡Œä¸ºéšä¹‹æƒ³è¦è¿›è¡Œåå‘ä¼ é€’, ä½¿å³è¾¹çš„è¡Œä¸ºä¸‹æ¬¡è¢«å¤šé€‰ä¸€ç‚¹, è¿™æ—¶, å¥–æƒ©ä¿¡æ¯ä¹Ÿæ¥äº†, å‘Šè¯‰æˆ‘ä»¬è¿™æ˜¯å¥½è¡Œä¸º, é‚£æˆ‘ä»¬å°±åœ¨è¿™æ¬¡åå‘ä¼ é€’çš„æ—¶å€™åŠ å¤§åŠ›åº¦, è®©å®ƒä¸‹æ¬¡è¢«å¤šé€‰çš„å¹…åº¦æ›´çŒ›çƒˆã€‚è¿™å°±æ˜¯ Policy Gradients çš„æ ¸å¿ƒæ€æƒ³.

> ğŸ’¡ åæ˜ åˆ°ä»£ç ä¸Šå…¶å®å°±æ˜¯ç»™æŸå¤±å‡½æ•°åŠ æƒé‡ï¼š`loss = discount_reward * loss`ã€‚å¯¹äºç´¯åŠ æœŸæœ›å¤§çš„åŠ¨ä½œï¼Œå¯ä»¥æ”¾å¤§ `loss` çš„å€¼ï¼Œè€Œå¯¹äºç´¯åŠ æœŸæœ›å°çš„åŠ¨ä½œï¼Œé‚£ä¹ˆå°±å‡å° loss çš„å€¼ã€‚è¿™æ ·ç¥ç»ç½‘ç»œå°±èƒ½å¿«é€Ÿæœç€ç´¯åŠ æœŸæœ›å¤§çš„æ–¹å‘ä¼˜åŒ–äº†ã€‚
>

Policy gradient åŒæ ·ä¹Ÿè¦æ¥å—ç¯å¢ƒä¿¡æ¯ (observation), **ä¸åŒçš„æ˜¯ä»–è¦è¾“å‡ºä¸æ˜¯ action çš„ value, è€Œæ˜¯å…·ä½“çš„é‚£ä¸€ä¸ª action**, è¿™æ · policy gradient å°±è·³è¿‡äº† value è¿™ä¸ªé˜¶æ®µ

<u>ä¹Ÿå°±æ˜¯è¯´ Policy Gradient ç½‘ç»œçš„è¾“å…¥ä¹Ÿæ˜¯çŠ¶æ€(State)ï¼Œè¾“å‡ºæ˜¯æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œä¾‹å¦‚ `[0.7, 0.3]` ï¼Œè¿™æ„å‘³ç€æœ‰70% çš„å‡ ç‡ä¼šé€‰æ‹©åŠ¨ä½œ 0ï¼Œ30% çš„å‡ ç‡é€‰æ‹©åŠ¨ä½œ 1</u>

## 2. Policy Gradient æ•´ä½“ç®—æ³•

æˆ‘ä»¬ä»‹ç»çš„ policy gradient çš„åŸºç¡€ç®—æ³•æ˜¯ä¸€ç§åŸºäº **æ•´æ¡å›åˆæ•°æ®** çš„æ›´æ–°, ä¹Ÿå« **REINFORCE** æ–¹æ³•. è¿™ç§æ–¹æ³•æ˜¯ policy gradient çš„æœ€åŸºæœ¬æ–¹æ³•

Î¸ å°±æ˜¯æˆ‘ä»¬éœ€è¦ä¸æ–­æ›´æ–°çš„ç¥ç»ç½‘ç»œå‚æ•°

![](https://gitee.com/veal98/images/raw/master/img/20201102095848.png)

> ğŸ’¡ $\pi$ è¡¨ç¤º Policyï¼Œ$\pi(s,a)$ è¡¨ç¤ºåœ¨çŠ¶æ€ s ä¸‹é€‰æ‹©åŠ¨ä½œ a çš„æ¦‚ç‡ 
>
> **`Policy` è¾“å‡ºæŸä¸ªçŠ¶æ€ä¸‹æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„æ‰§è¡Œæ¦‚ç‡ï¼Œå®ƒå…¶å®æ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒæŠŠè¾“å…¥çš„çŠ¶æ€å˜æˆè¡Œä¸º**ã€‚å‡è®¾ä½ æ˜¯ç”¨ deep learning çš„æŠ€æœ¯æ¥åš reinforcement learning çš„è¯ï¼Œ**`policy` å°±æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œ**ï¼Œç¥ç»ç½‘ç»œé‡Œé¢æœ‰ä¸€å †å‚æ•°ï¼Œ <u>æˆ‘ä»¬ç”¨ Î¸ æ¥ä»£è¡¨ Ï€ çš„å‚æ•°</u>ï¼š
>
> <img src="https://gitee.com/veal98/images/raw/master/img/20201026173154.png" style="zoom:40%;" />

<img src="https://gitee.com/veal98/images/raw/master/img/20201102095924.png" style="zoom:67%;" /> è¡¨ç¤ºåœ¨ çŠ¶æ€ `s` å¯¹æ‰€é€‰åŠ¨ä½œ `a` çš„åƒæƒŠåº¦, å¦‚æœ $\pi(s,a)$ æ¦‚ç‡è¶Šå°, åå‘çš„ $log\pi(s,a)$ åè€Œè¶Šå¤§. å¦‚æœåœ¨ $\pi(s,a)$ å¾ˆå°çš„æƒ…å†µä¸‹, æ‹¿åˆ°äº†ä¸€ä¸ªå¤§çš„ `R`, ä¹Ÿå°±æ˜¯ å¤§çš„ `V`, é‚£ $log\pi(s,a)V$ å°±æ›´å¤§, è¡¨ç¤ºæ›´åƒæƒŠ

ğŸ‘ **é€šä¿—æ¥è¯´ï¼Œæˆ‘é€‰äº†ä¸€ä¸ªä¸å¸¸é€‰çš„åŠ¨ä½œï¼ˆæ¦‚ç‡å°ï¼‰, å´å‘ç°åŸæ¥å®ƒèƒ½å¾—åˆ°äº†ä¸€ä¸ªå¥½çš„ reward, é‚£æˆ‘å°±å¾—å¯¹è¿™æ¬¡çš„å‚æ•°è¿›è¡Œä¸€ä¸ªå¤§å¹…ä¿®æ”¹. è¿™å°±æ˜¯åƒæƒŠåº¦çš„ç‰©ç†æ„ä¹‰**

## 3. åŸºäº CartPole-v0 çš„ä»£ç å®ç°

**CartPole-v0** çš„å‡ ä¸ªé‡è¦æ¦‚å¿µï¼š

| æ¦‚å¿µ   | è§£é‡Š                                   | ç¤ºä¾‹                     |
| :----- | :------------------------------------- | :----------------------- |
| State  | çŠ¶æ€ï¼Œ[è½¦ä½ç½®, è½¦é€Ÿåº¦, æ†è§’åº¦, æ†é€Ÿåº¦] | 0.02, 0.95, -0.07, -1.53 |
| Action | åŠ¨ä½œ(0å‘å·¦/1å‘å³)                      | 1                        |
| Reward | å¥–åŠ±(æ¯èµ°ä¸€æ­¥å¾—1åˆ†)                    | 1.0                      |

å…³äº CartPole çš„è¯¦ç»†æ–‡æ¡£è¯·å‚è§ [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control)

### â‘  æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹

```python
import matplotlib.pyplot as plt
import gym
import numpy as np
import tensorflow as tf # tensorflow 2.x
from tensorflow import keras
```

```python
env = gym.make('CartPole-v0')

STATE_DIM = 4 # 4 ä¸ªçŠ¶æ€ï¼ˆè¾“å…¥å±‚ï¼‰
ACTION_DIM = 2 # 2 ä¸ªåŠ¨ä½œï¼ˆè¾“å‡ºå±‚ï¼‰

# æ„å»ºæ¨¡å‹

# è®¾ç½®å±‚
model = keras.Sequential([
    # è¾“å…¥å±‚ä¸º4ï¼Œè¾“å‡ºå±‚ä¸º2ï¼Œéšè—å±‚ä¸º100
    keras.layers.Dense(100, input_dim = STATE_DIM, activation = 'relu'),
    keras.layers.Dropout(0.1), # éšæœºå¿˜è®°10%çš„æƒé‡
    keras.layers.Dense(ACTION_DIM, activation = 'softmax')
])

# ç¼–è¯‘æ¨¡å‹
model.compile(loss='mean_squared_error',
              optimizer='adam')
```

æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå¾ˆç®€å•ï¼Œè¾“å…¥å±‚ä¸º4ï¼Œè¾“å‡ºå±‚ä¸º2ï¼Œéšè—å±‚ä¸º100ã€‚`Dropout(0.1)` çš„å«ä¹‰æ˜¯ï¼Œéšæœºå¿˜è®° 10% çš„æƒé‡ã€‚å­¦ä¹ åˆæœŸï¼Œä¸€å¼€å§‹çš„æ•°æ®è´¨é‡ä¸é«˜ï¼Œéšç€å­¦ä¹ çš„è¿›è¡Œï¼Œè´¨é‡æ‰é€æ­¥é«˜äº†èµ·æ¥ï¼Œä¸€å¼€å§‹å®¹æ˜“é™·å…¥**å±€éƒ¨æœ€ä¼˜**å’Œ**è¿‡æ‹Ÿåˆ**ï¼Œä½¿ç”¨ Dropout å¯ä»¥æœ‰æ•ˆé¿å…ã€‚

Policy Gradient ç½‘ç»œçš„è¾“å…¥æ˜¯çŠ¶æ€(State)ï¼Œè¾“å‡ºæ˜¯æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œä¾‹å¦‚ `[0.7, 0.3]` ï¼Œè¿™æ„å‘³ç€æœ‰ 70% çš„å‡ ç‡ä¼šé€‰æ‹©åŠ¨ä½œ 0ï¼Œ30% çš„å‡ ç‡é€‰æ‹©åŠ¨ä½œ 1ã€‚ä½¿ç”¨ `np.random.choice` æ ¹æ®æ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼š

```python
# åœ¨çŠ¶æ€ s ä¸‹é€‰æ‹©åŠ¨ä½œ actionï¼ˆä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼‰
def choose_action(s):
    # å³ä½¿æ˜¯å•ä¸ªæ•°æ®ï¼Œpredict ä¹Ÿå¿…é¡»ä¼ å…¥åˆ—è¡¨
    # predictions è¡¨ç¤ºåœ¨çŠ¶æ€ s ä¸‹é€‰æ‹©å„ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
    predictions = model.predict(np.array([s]))[0]
    # ä» len(predictions) ä¸­ä»¥æ¦‚ç‡ predictions éšæœºé€‰æ‹©ä¸€ä¸ªå€¼ï¼ˆåŠ¨ä½œçš„ä¸‹æ ‡ï¼‰
    return np.random.choice(len(predictions), p = predictions)
```

> ğŸ’¡ `numpy.random.choice(a, size=None, replace=True, p=None)`
>
> Generates a random sample from a given 1-D array
>
> Parameters:	
>
> - `a`: 1-D array-like or int
>   If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if a were np.arange(a)
>
> - `size `: int or tuple of ints, optional
>   Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.
>
> - `replace `: boolean, optional
>   Whether the sample is with or without replacement
>
> - `p` : 1-D array-like, optional
> The probabilities associated with each entry in a. If not given the sample assumes a uniform distribution over all entries in a.

### â‘¡ ä¼˜åŒ–ç­–ç•¥

![](https://gitee.com/veal98/images/raw/master/img/20201105144617.png)

#### â…  è¡°å‡çš„ç´¯åŠ æœŸæœ›

åŒ Q-Learning ä¸€æ ·ï¼Œæˆ‘ä»¬å¼•å…¥ **æŠ˜æ‰£ç³»æ•°/è¡°å‡å› å­ Î³**

â­ `discount_reward[i] = reward[i] + gamma * discount_reward[i+1]`

æŸä¸€æ­¥çš„ç´¯åŠ æœŸæœ›ç­‰äºä¸‹ä¸€æ­¥çš„ç´¯åŠ æœŸæœ›ä¹˜è¡°å‡ç³»æ•°`gamma`ï¼ŒåŠ ä¸Š`reward`ã€‚

æ‰‹å·¥ç®—ä¸€ç®—ã€‚

```
æœ€åä¸€æ­¥ï¼š1
å€’æ•°ç¬¬äºŒæ­¥ï¼š1 + 0.95 * 1 = 1.95
å€’æ•°ç¬¬ä¸‰æ­¥ï¼š1 + 0.95 * 1.95 = 2.8525
å€’æ•°ç¬¬å››æ­¥ï¼š1 + 0.95 * 2.8525 = 3.709875
```

å‡è®¾æŸä¸ªå›åˆåªå¾—äº†10åˆ†ï¼Œé‚£ä¹ˆè¿™ä¸ªå›åˆçš„æ¯ä¸€æ­¥çš„ç´¯åŠ æœŸæœ›éƒ½ä¸ä¼šé«˜ã€‚å‡è®¾å¾—åˆ°äº†æ»¡åˆ†200åˆ†ï¼Œé‚£ä¹ˆå›åˆä¸­çš„å¤§éƒ¨åˆ†æ­¥éª¤çš„ç´¯åŠ æœŸæœ›å¾ˆä¼šå¾ˆé«˜ï¼Œè¶Šæ˜¯å‰é¢çš„æ­¥éª¤ï¼Œç´¯åŠ æœŸæœ›è¶Šé«˜ã€‚

ä»£ç å®ç°å°±å¾ˆç®€å•äº†ï¼Œå”¯ä¸€çš„ä¸åŒæ˜¯æœ€ååŠ äº†ä¸­å¿ƒåŒ–å’Œæ ‡å‡†åŒ–çš„å¤„ç†ã€‚è¿™æ ·å¤„ç†çš„ç›®çš„æ˜¯å¸Œæœ›å¾—åˆ°ç›¸åŒå°ºåº¦çš„æ•°æ®ï¼Œé¿å…å› ä¸ºæ•°å€¼ç›¸å·®è¿‡å¤§è€Œå¯¼è‡´ç½‘ç»œæ— æ³•æ”¶æ•›ã€‚

```python
# è®¡ç®—æŸä¸ªåŠ¨ä½œè¡°å‡çš„ç´¯åŠ æœŸæœ›reward ï¼Œå¹¶ä¸­å¿ƒåŒ–å’Œæ ‡å‡†åŒ–å¤„ç†
def discount_rewards(rewards, gamma = 0.95):
    prior = 0
    out = np.zeros_like(rewards)
    for i in reversed(range(len(rewards))):
        prior = prior * gamma + rewards[i]
        out[i] = prior
    return out / np.std(out - np.mean(out))
```

#### â…¡ ç»™ loss åŠ æƒé‡

**ä¸€ä¸ªåŠ¨ä½œçš„`ç´¯åŠ æœŸæœ› reward`å¾ˆé«˜ï¼Œè‡ªç„¶å¸Œæœ›è¯¥åŠ¨ä½œå‡ºç°çš„æ¦‚ç‡å˜å¤§ï¼Œè¿™å°±æ˜¯å­¦ä¹ çš„ç›®çš„**ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜**æŸå¤±å‡½æ•°(loss function)**è¾¾åˆ°ç›®çš„ã€‚å¯¹äºç´¯åŠ æœŸæœ›å¤§çš„åŠ¨ä½œï¼Œå¯ä»¥æ”¾å¤§`loss`çš„å€¼ï¼Œè€Œå¯¹äºç´¯åŠ æœŸæœ›å°çš„åŠ¨ä½œï¼Œé‚£ä¹ˆå°±å‡å°lossçš„å€¼ã€‚è¿™æ ·å‘¢ï¼Ÿç¥ç»ç½‘ç»œå°±èƒ½å¿«é€Ÿæœç€ç´¯åŠ æœŸæœ›å¤§çš„æ–¹å‘ä¼˜åŒ–äº†ã€‚æœ€ç®€å•çš„æ–¹æ³•ï¼Œç»™`loss`åŠ ä¸€ä¸ªæƒé‡ã€‚

æ‰€ä»¥æˆ‘ä»¬çš„æœ€ç»ˆçš„æŸå¤±å‡½æ•°å°±å˜æˆäº†ï¼š

`loss = discount_reward * loss`

> ğŸ’¡ è¿™ä¸ªåœ°æ–¹å¯èƒ½æœ‰äº›ä¸å¥½ç†è§£ï¼Œä¸ºä»€ä¹ˆå¥½çš„åŠ¨ä½œè¿˜å¢å¤§æŸå¤±å‡½æ•°çš„å€¼ï¼ŒæŸå¤±å‡½æ•°ä¸æ˜¯ä»£è¡¨çœŸå®æƒ…å†µä¸é¢„æµ‹æƒ…å†µçš„å·®å—ï¼Œä¸¤è€…è¶Šæ¥è¿‘ï¼ŒæŸå¤±å‡½æ•°å€¼è¶Šå°ã€‚
>
> å…¶å®è¿™é‡Œé¢çš„æŸå¤±å¯ä»¥ç†è§£ä¸ºæ¢¯åº¦ï¼Œé‡‡ç”¨**æ¢¯åº¦ä¸‹é™**çš„æ–¹æ³•ï¼Œæ‰¾åˆ°æœ€å¿«çš„ä¼˜åŒ–æ–¹å‘ï¼ˆä½¿æŸå¤±å‡½æ•°æœ€å°ï¼‰ï¼Œè°ƒæ•´å‚æ•°å€¼ã€‚ä¹Ÿå°±æ˜¯è¯´å¦‚æœè¯¥åŠ¨ä½œçš„ç´¯åŠ æœŸæœ›å¤§ï¼Œæˆ‘ä»¬å°±è¿ˆä¸€å¤§æ­¥è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå¦‚æœè¯¥åŠ¨ä½œçš„ç´¯åŠ æœŸæœ›å°ï¼Œæˆ‘ä»¬å°±è¿ˆä¸€å°æ­¥è¿›è¡Œæ¢¯åº¦ä¸‹é™
>
> ![](https://gitee.com/veal98/images/raw/master/img/20201105143446.png)

åœ¨**TensorFlow 1.x**çš„ç‰ˆæœ¬ä¸­ï¼Œæ­å»ºä¸€ä¸ªè‡ªå®šä¹‰lossçš„ç½‘ç»œå¾ˆå¤æ‚ï¼Œè€Œä½¿ç”¨**TensorFlow 2.0**ï¼Œå€ŸåŠ©`Keras`ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºéå¸¸ç®€æ´çš„ä»£ç ã€‚

```python
# ç»™ loss åŠ æƒé‡ å¹¶è®­ç»ƒæ¨¡å‹
def train(records):
    # records å­˜å‚¨æ¯ä¸€ä¸ªçŠ¶æ€å¯¹åº”çš„åŠ¨ä½œä»¥åŠå¥–åŠ± record[state,action,reward]
    state_batch = np.array([record[0] for record in records]) # æ‰€æœ‰çš„çŠ¶æ€
    action_batch = np.array([[1 if record[1] == i else 0 for i in range(ACTION_DIM)]
                            for record in records]) # å°† action æ˜ å°„æˆ 0 æˆ– 1
    # å‡è®¾predictçš„æ¦‚ç‡æ˜¯ [0.3, 0.7]ï¼Œé€‰æ‹©çš„åŠ¨ä½œæ˜¯ [0, 1]
    # åˆ™åŠ¨ä½œ[0, 1]çš„æ¦‚ç‡ç­‰äº [0, 0.7] = [0.3, 0.7] * [0, 1]
    prediction_batch = model.predict(state_batch) * action_batch
    reward_batch = discount_rewards([record[2] for record in records])
    
    # è®¾ç½®å‚æ•° sample_weightï¼Œå³ç»™lossè®¾æƒé‡ loss = discount_reward * loss
    model.fit(state_batch, prediction_batch, sample_weight = reward_batch, verbose = 2)
```

### â‘¢ ä¸»å¾ªç¯ / è®­ç»ƒæ¨¡å‹

æ¥ä¸‹æ¥ï¼ŒæŠŠ OpenAI gym çš„ä»£ç èå…¥è¿›æ¥å¹¶è¿›è¡Œè®­ç»ƒæ¨¡å‹ï¼š

```python
episodes = 2000  # è‡³å¤šå¾ªç¯ 2000 æ¬¡
score_list = []  # è®°å½•æ‰€æœ‰åˆ†æ•°

for i in range(episodes):
    s = env.reset() # é‡ç½®ç¯å¢ƒçš„çŠ¶æ€
    score = 0
    replay_records = []
    
    while True:
        a = choose_action(s)
        next_s, r, done, _ = env.step(a)
        replay_records.append((s, a, r))

        score += r
        s = next_s
        if done:
            train(replay_records)
            score_list.append(score)
            print('episode:', i, 'score:', score, 'max:', max(score_list))
            break
    # æœ€å10æ¬¡çš„å¹³å‡åˆ†å¤§äº 195 æ—¶ï¼Œåœæ­¢å¹¶ä¿å­˜æ¨¡å‹
    if np.mean(score_list[-10:]) > 195:
        model.save('CartPole-v0-pg.h5')
        break
env.close()
```

å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜ä¸º `CartPole-v0-pg.h5`

<img src="https://gitee.com/veal98/images/raw/master/img/20201105145334.png" style="zoom: 67%;" />

### â‘£ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•

æ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹ `CartPole-v0-pg.h5`

```python
import time
import gym
import numpy as np
import tensorflow as tf
from tensorflow import keras

saved_model = keras.models.load_model('CartPole-v0-pg.h5') # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
env = gym.make("CartPole-v0")

for i in range(10):
    s = env.reset()
    score = 0
    while True:
        time.sleep(0.01)
        env.render()
        predictions = saved_model.predict(np.array([s]))[0]
        a = np.random.choice(len(predictions), p=predictions) # æ ¹æ®æ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œ
        s, r, done, _ = env.step(a)
        score += r
        if done:
            print('using policy gradient, score: ', score)  # æ‰“å°åˆ†æ•°
            break
env.close()
```

æ•ˆæœå¦‚ä¸‹

![Geektutu Policy Gradient Success](https://geektutu.com/post/tensorflow2-gym-pg/pg_success.gif)

## ğŸ“š References

- [Bilibili - æå®æ¯…ã€Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - æå®æ¯…æ·±åº¦å¼ºåŒ–å­¦ä¹ ç¬”è®° - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [æå®¢å…”å…” - TensorFlow 2.0 (ä¹) - å¼ºåŒ–å­¦ä¹  70è¡Œä»£ç å®æˆ˜ Policy Gradient](https://geektutu.com/post/tensorflow2-gym-pg.html#CartPole-%E7%AE%80%E4%BB%8B)
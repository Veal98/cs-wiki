# ğŸˆ Sarsa ç®—æ³•

---

> ğŸ’¡ **Sarsa** å±äº Value-Basedã€Model-Freeã€å•æ­¥æ›´æ–°(MC)ã€On-Policy
>
> å’Œ Q-Learning çš„ä¸åŒä¹‹å¤„å°±æ˜¯ Sarsa æ˜¯åœ¨çº¿å­¦ä¹ çš„ï¼Œè€Œ Q-Learning æ˜¯ç¦»çº¿å­¦ä¹ 

## 1. Sarsa å†³ç­–

![](https://gitee.com/veal98/images/raw/master/img/20201031092605.png)

**Sarsa çš„å†³ç­–éƒ¨åˆ†å’Œ Q learning ä¸€æ¨¡ä¸€æ ·**, å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ Q è¡¨çš„å½¢å¼å†³ç­–, æ‰€ä»¥æˆ‘ä»¬ä¼šåœ¨ Q è¡¨ä¸­æŒ‘é€‰å€¼è¾ƒå¤§çš„åŠ¨ä½œå€¼æ–½åŠ åœ¨ç¯å¢ƒä¸­æ¥æ¢å–å¥–æƒ©. ä½†æ˜¯**ä¸åŒçš„åœ°æ–¹åœ¨äº Sarsa çš„æ›´æ–°æ–¹å¼æ˜¯ä¸ä¸€æ ·çš„** ğŸ‘‡

## 2. æ›´æ–° Q-Table

åŒæ ·, æˆ‘ä»¬ä¼šç»å†æ­£åœ¨å†™ä½œä¸šçš„çŠ¶æ€ s1, ç„¶åå†æŒ‘é€‰ä¸€ä¸ªå¸¦æ¥æœ€å¤§æ½œåœ¨å¥–åŠ±çš„åŠ¨ä½œ a2, è¿™æ ·æˆ‘ä»¬å°±åˆ°è¾¾äº† ç»§ç»­å†™ä½œä¸šçŠ¶æ€ s2, è€Œåœ¨è¿™ä¸€æ­¥, **å¦‚æœä½ ç”¨çš„æ˜¯ Q learning, ä½ ä¼šè§‚çœ‹ä¸€ä¸‹åœ¨ s2 ä¸Šé€‰å–å“ªä¸€ä¸ªåŠ¨ä½œä¼šå¸¦æ¥æœ€å¤§çš„å¥–åŠ± Q å€¼, ä½†æ˜¯åœ¨çœŸæ­£è¦åšå†³å®šæ—¶, å´ä¸ä¸€å®šä¼šé€‰å–åˆ°é‚£ä¸ªå¸¦æ¥æœ€å¤§å¥–åŠ±çš„åŠ¨ä½œï¼ˆç”±äº `Îµ-greedy` ç­–ç•¥çš„å­˜åœ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„å†³ç­–å¹¶ä¸ä¸€å®šå°±æ˜¯ Q å€¼æœ€å¤§çš„é‚£ä¸ªè¡Œä¸ºï¼Œæœ‰ `1-Îµ` çš„æ¦‚ç‡ä¼šè¿›è¡Œéšæœºé€‰æ‹©ï¼‰**, Q-learning åœ¨è¿™ä¸€æ­¥åªæ˜¯ä¼°è®¡äº†ä¸€ä¸‹æ¥ä¸‹æ¥çš„åŠ¨ä½œå€¼. 

**è€Œ Sarsa æ˜¯å®è·µæ´¾, ä»–è¯´åˆ°åšåˆ°, åœ¨ s2 è¿™ä¸€æ­¥ä¼°ç®—çš„åŠ¨ä½œä¹Ÿæ˜¯æ¥ä¸‹æ¥è¦åšçš„åŠ¨ä½œï¼ˆä¼°ç®—çš„æ—¶å€™å°±ä¼šä½¿ç”¨ `Îµ-greedy` ç­–ç•¥ï¼Œè€Œä¸æ˜¯ç›´æ¥æ ¹æ®æœ€å¤§ Q å€¼ï¼‰**. æ‰€ä»¥ Q(s1, a2) ç°å®çš„è®¡ç®—å€¼, æˆ‘ä»¬ä¹Ÿä¼šç¨ç¨æ”¹åŠ¨, å»æ‰maxQ, å–è€Œä»£ä¹‹çš„æ˜¯åœ¨ s2 ä¸Šæˆ‘ä»¬å®å®åœ¨åœ¨é€‰å–çš„ a2 çš„ Q å€¼. æœ€ååƒ Q learning ä¸€æ ·, æ±‚å‡ºç°å®å’Œä¼°è®¡çš„å·®è· å¹¶æ›´æ–° Q è¡¨é‡Œçš„ Q(s1, a2).

![](https://gitee.com/veal98/images/raw/master/img/20201031092648.png)

ä¹Ÿå°±æ˜¯è¯´ï¼ŒQlearning çš„ä¸‹ä¸€ä¸ª çŠ¶æ€ä¸‹çš„ action åœ¨ç®—æ³•æ›´æ–°çš„æ—¶å€™éƒ½è¿˜æ˜¯ä¸ç¡®å®šçš„ (off-policy). è€Œ Sarsa çš„ä¸‹ä¸€ä¸ª çŠ¶æ€ä¸‹çš„ action åœ¨è¿™æ¬¡ç®—æ³•æ›´æ–°çš„æ—¶å€™å·²ç»ç¡®å®šå¥½äº† (on-policy).

## 3. Sarsa æ•´ä½“ç®—æ³•

![](https://gitee.com/veal98/images/raw/master/img/20201031094353.png)

ä»ç®—æ³•æ¥çœ‹, è¿™å°±æ˜¯ä»–ä»¬ä¸¤æœ€å¤§çš„ä¸åŒä¹‹å¤„äº†. å› ä¸º Sarsa æ˜¯è¯´åˆ°åšåˆ°å‹, æ‰€ä»¥æˆ‘ä»¬ä¹Ÿå«ä»– on-policy, åœ¨çº¿å­¦ä¹ , å­¦ç€è‡ªå·±åœ¨åšçš„äº‹æƒ…. è€Œ Q learning æ˜¯è¯´åˆ°ä½†å¹¶ä¸ä¸€å®šåšåˆ°, æ‰€ä»¥å®ƒä¹Ÿå«ä½œ Off-policy, ç¦»çº¿å­¦ä¹ . 

è€Œå› ä¸ºæœ‰äº† maxQ, Q-learning ä¹Ÿæ˜¯ä¸€ä¸ªç‰¹åˆ«å‹‡æ•¢çš„ç®—æ³•ã€‚å› ä¸º Q learning æœºå™¨äºº æ°¸è¿œéƒ½ä¼šé€‰æ‹©æœ€è¿‘çš„ä¸€æ¡é€šå¾€æˆåŠŸçš„é“è·¯, ä¸ç®¡è¿™æ¡è·¯ä¼šæœ‰å¤šå±é™©. è€Œ Sarsa åˆ™æ˜¯ç›¸å½“ä¿å®ˆ, ä»–ä¼šé€‰æ‹©ç¦»å±é™©è¿œè¿œçš„, æ‹¿åˆ°å®è—æ˜¯æ¬¡è¦çš„, ä¿ä½è‡ªå·±çš„å°å‘½æ‰æ˜¯ç‹é“. è¿™å°±æ˜¯ä½¿ç”¨ Sarsa æ–¹æ³•çš„ä¸åŒä¹‹å¤„.

![](https://gitee.com/veal98/images/raw/master/img/20201031100809.png)

## 4. Sarsa å®ä¾‹

ä»¥ä¸Šæ¬¡ Q-Learning çš„è¿·å®«é—®é¢˜ä¸ºä¾‹ï¼Œç¯å¢ƒæ¨¡å—æˆ‘ä»¬ä¸å˜ï¼Œé¦–å…ˆç¼–å†™ç®—æ³•çš„æ•´ä½“æ¡†æ¶ï¼š

```python
from maze_env import Maze
from RL_brain import SarsaTable

def update():
    for episode in range(100):
        # åˆå§‹åŒ–ç¯å¢ƒ
        observation = env.reset()

        # Sarsa æ ¹æ® state è§‚æµ‹é€‰æ‹©è¡Œä¸º
        action = RL.choose_action(str(observation))

        while True:
            # åˆ·æ–°ç¯å¢ƒ
            env.render()

            # åœ¨ç¯å¢ƒä¸­é‡‡å–è¡Œä¸º, è·å¾—ä¸‹ä¸€ä¸ª state_ (obervation_), reward, å’Œæ˜¯å¦ç»ˆæ­¢
            observation_, reward, done = env.step(action)

            # æ ¹æ®ä¸‹ä¸€ä¸ª state (obervation_) é€‰å–ä¸‹ä¸€ä¸ª action_
            action_ = RL.choose_action(str(observation_))

            # ä» (s, a, r, s, a) ä¸­å­¦ä¹ , æ›´æ–° Q_tabel çš„å‚æ•° ==> Sarsa
            RL.learn(str(observation), action, reward, str(observation_), action_)

            # å°†ä¸‹ä¸€ä¸ªå½“æˆä¸‹ä¸€æ­¥çš„ state (observation) and action
            observation = observation_
            action = action_

            # ç»ˆæ­¢æ—¶è·³å‡ºå¾ªç¯
            if done:
                break

    # å¤§å¾ªç¯å®Œæ¯•
    print('game over')
    env.destroy()

if __name__ == "__main__":
    env = Maze()
    RL = SarsaTable(actions=list(range(env.n_actions)))

    env.after(100, update)
    env.mainloop()
```

å…¶å®å’Œ Q-Learning çš„å·®åˆ«åªæœ‰ä¸‹é¢è¿™å‡ è¡Œï¼š

```python
------------------Sarsa-----------------------------
# ä» (å½“å‰çŠ¶æ€ s, åœ¨è¯¥çŠ¶æ€ä¸‹é€‰æ‹©çš„ action,è¯¥ action è·å¾—çš„ reward, ä¸‹ä¸€ä¸ªçŠ¶æ€ state_, åœ¨ä¸‹ä¸€ä¸ªçŠ¶æ€ä¸‹é€‰æ‹©çš„ action_) ä¸­å­¦ä¹ , æ›´æ–° Q_tabel çš„å‚æ•° ==> Sarsa
RL.learn(str(observation), action, reward, str(observation_), action_)

# å°†ä¸‹ä¸€ä¸ªå½“æˆä¸‹ä¸€æ­¥çš„ state (observation) and action
observation = observation_
action = action_
            
------------------Q-Learning-----------------------------
 # RL ä»è¿™ä¸ªåºåˆ— (å½“å‰çŠ¶æ€ state, åœ¨è¯¥çŠ¶æ€ä¸‹é€‰æ‹©çš„ action, è¯¥ action è·å¾—çš„ reward, ä¸‹ä¸€ä¸ªçŠ¶æ€ state_) ä¸­å­¦ä¹ 
RL.learn(str(observation), action, reward, str(observation_))

# å°†ä¸‹ä¸€ä¸ª state çš„å€¼ä¼ åˆ°ä¸‹ä¸€æ¬¡å¾ªç¯
observation = observation_
```

æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰ Q-Table çš„æ„é€ ä»¥åŠæ›´æ–°æ–¹æ³•ã€‚å’Œä¹‹å‰å®šä¹‰ Qlearning ä¸­çš„ `QLearningTable` å·®ä¸å¤š

```python
class SarsaTable:
    # åˆå§‹åŒ– (ä¸ä¹‹å‰ä¸€æ ·)
    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):

    # é€‰è¡Œä¸º (ä¸ä¹‹å‰ä¸€æ ·)
    def choose_action(self, observation):

    # å­¦ä¹ æ›´æ–°å‚æ•° (æœ‰æ”¹å˜)
    def learn(self, s, a, r, s_):

    # æ£€æµ‹ state æ˜¯å¦å­˜åœ¨ (ä¸ä¹‹å‰ä¸€æ ·)
    def check_state_exist(self, state):
```

æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ª çˆ¶ç±» `RL`, ç„¶åå°† `QLearningTable` å’Œ `SarsaTable` ä½œä¸º `RL` çš„è¡ç”Ÿ, æˆ‘ä»¬å°†ä¹‹å‰çš„ `__init__`, `check_state_exist`, `choose_action`, `learn` å…¨éƒ¨éƒ½æ”¾åœ¨è¿™ä¸ªä¸»ç»“æ„ä¸­, ä¹‹åæ ¹æ®ä¸åŒçš„ç®—æ³•æ›´æ”¹å¯¹åº”çš„å†…å®¹å°±å¥½äº†. 

```python
import numpy as np
import pandas as pd


class RL(object):
    def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        ... # å’Œ QLearningTable ä¸­çš„ä»£ç ä¸€æ ·

    def check_state_exist(self, state):
        ... # å’Œ QLearningTable ä¸­çš„ä»£ç ä¸€æ ·

    def choose_action(self, observation):
        ... # å’Œ QLearningTable ä¸­çš„ä»£ç ä¸€æ ·

    def learn(self, *args):
        pass # æ¯ç§çš„éƒ½æœ‰ç‚¹ä¸åŒ, æ‰€ä»¥ç”¨ pass
```

> ğŸ’¡ å¦‚æœæ˜¯è¿™æ ·å®šä¹‰çˆ¶ç±»çš„ `RL` class, é€šè¿‡ç»§æ‰¿å…³ç³», é‚£ä¹‹å­ç±» `QLearningTable` class å°±èƒ½ç®€åŒ–æˆè¿™æ ·:
>
> ```python
> class QLearningTable(RL):   # ç»§æ‰¿äº†çˆ¶ç±» RL
>     def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
>         super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    # è¡¨ç¤ºç»§æ‰¿å…³ç³»
> 
>     def learn(self, s, a, r, s_):   # learn çš„æ–¹æ³•åœ¨æ¯ç§ç±»å‹ä¸­æœ‰ä¸ä¸€æ ·, éœ€é‡æ–°å®šä¹‰
>         self.check_state_exist(s_)
>         q_predict = self.q_table.loc[s, a]
>         if s_ != 'terminal':
>             q_target = r + self.gamma * self.q_table.loc[s_, :].max()
>         else:
>             q_target = r
>         self.q_table.loc[s, a] += self.lr * (q_target - q_predict)
> ```

æœ‰äº†çˆ¶ç±»çš„ `RL`, æˆ‘ä»¬è¿™æ¬¡çš„ç¼–å†™å°±å¾ˆç®€å•, åªéœ€è¦ç¼–å†™ `SarsaTable` ä¸­ `learn` è¿™ä¸ªåŠŸèƒ½å°±å®Œæˆäº†. å› ä¸ºå…¶ä»–åŠŸèƒ½éƒ½å’Œçˆ¶ç±»æ˜¯ä¸€æ ·çš„. è¿™å°±æ˜¯æˆ‘ä»¬æ‰€æœ‰çš„ `SarsaTable` äºçˆ¶ç±» `RL` ä¸åŒä¹‹å¤„çš„ä»£ç  ğŸ‘‡

```python
class SarsaTable(RL):   # ç»§æ‰¿ RL class

    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
        super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    # è¡¨ç¤ºç»§æ‰¿å…³ç³»

    def learn(self, s, a, r, s_, a_):
        self.check_state_exist(s_)
        q_predict = self.q_table.loc[s, a]
        if s_ != 'terminal':
            q_target = r + self.gamma * self.q_table.loc[s_, a_]  # q_target åŸºäºé€‰å¥½çš„ a_ è€Œä¸æ˜¯ Q(s_) çš„æœ€å¤§å€¼
        else:
            q_target = r  # å¦‚æœ s_ æ˜¯ç»ˆæ­¢ç¬¦
        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # æ›´æ–° q_table
```

## ğŸ“š References

- [Bilibili - æå®æ¯…ã€Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - æå®æ¯…æ·±åº¦å¼ºåŒ–å­¦ä¹ ç¬”è®° - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [å¼ºåŒ–å­¦ä¹ çº²è¦](https://github.com/zhoubolei/introRL)
- [è«çƒ¦ Python â€” å¼ºåŒ–å­¦ä¹ ](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL-methods/)
# ğŸŒŒ Deep Q Network (DQN)

---

> ğŸ’¡ **DQN** å±äº Value-Basedã€Model-Basedã€On-Policy

ä»Šå¤©æˆ‘ä»¬æ¥è¯´è¯´å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ç§å¼ºå¤§æ­¦å™¨, Deep Q Network ç®€ç§°ä¸º DQN. Google Deep mind å›¢é˜Ÿå°±æ˜¯é ç€è¿™ DQN ä½¿è®¡ç®—æœºç©ç”µåŠ¨ç©å¾—æ¯”æˆ‘ä»¬è¿˜å‰å®³.

## 1. æ·±åº¦å¼ºåŒ–å­¦ä¹  DRL

ä¹‹å‰æˆ‘ä»¬æ‰€è°ˆè®ºåˆ°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•éƒ½æ˜¯æ¯”è¾ƒä¼ ç»Ÿçš„æ–¹å¼, è€Œå¦‚ä»Š, éšç€æœºå™¨å­¦ä¹ åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ç§åº”ç”¨, å„ç§æœºå™¨å­¦ä¹ æ–¹æ³•ä¹Ÿåœ¨èæ±‡, åˆå¹¶, å‡çº§.  **Deep Q Network å°±æ˜¯èåˆäº†ç¥ç»ç½‘ç»œï¼ˆæ·±åº¦å­¦ä¹ ï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä¸­ Q learning çš„æ–¹æ³•**. 

> ğŸ’¡`æ·±åº¦å¼ºåŒ–å­¦ä¹  DRL`ï¼šå°†æ·±åº¦å­¦ä¹ æŠ€æœ¯å¼•å…¥å¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ä¸­ï¼Œä½¿æ·±åº¦å­¦ä¹ çš„æŠ½è±¡æ„ŸçŸ¥èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ çš„å†³ç­–æ§åˆ¶èƒ½åŠ›å®Œç¾ç»“åˆï¼Œå®ç°äº†ä»åŸå§‹é«˜ç»´æ•°æ®çš„æŠ½è±¡æ„ŸçŸ¥åˆ°ç›´æ¥æ§åˆ¶è¾“å‡ºçš„**ç«¯åˆ°ç«¯ End-to-End** ç³»ç»Ÿæ¡†æ¶
>
> <img src="https://gitee.com/veal98/images/raw/master/img/20201119110824.png" style="zoom:60%;" />

è¿™ç§æ–°å‹ç»“æ„æ˜¯ä¸ºä»€ä¹ˆè¢«æå‡ºæ¥å‘¢? åŸæ¥, ä¼ ç»Ÿçš„è¡¨æ ¼å½¢å¼çš„å¼ºåŒ–å­¦ä¹ æœ‰è¿™æ ·ä¸€ä¸ªç“¶é¢ˆï¼š**ã€ç»´åº¦ç¾éš¾ã€‘** ğŸ‘‡

æˆ‘ä»¬ä½¿ç”¨è¡¨æ ¼ï¼ˆQ-Tableï¼‰æ¥å­˜å‚¨æ¯ä¸€ä¸ªçŠ¶æ€ state, å’Œåœ¨è¿™ä¸ª state æ¯ä¸ªè¡Œä¸º action æ‰€æ‹¥æœ‰çš„ Q å€¼. <u>è€Œå½“ä»Šé—®é¢˜å®åœ¨å¤ªå¤æ‚, å¦‚å›´æ£‹çš„çŠ¶æ€æ•°å…±æœ‰çº¦ $10^{170}$ ä¹‹å¤šã€‚å¦‚æœå…¨ç”¨è¡¨æ ¼æ¥å­˜å‚¨å®ƒä»¬, ææ€•æˆ‘ä»¬çš„è®¡ç®—æœºæœ‰å†å¤§çš„å†…å­˜éƒ½ä¸å¤Ÿ, è€Œä¸”æ¯æ¬¡åœ¨è¿™ä¹ˆå¤§çš„è¡¨æ ¼ä¸­æœç´¢å¯¹åº”çš„çŠ¶æ€ä¹Ÿæ˜¯ä¸€ä»¶å¾ˆè€—æ—¶çš„äº‹</u>. 

![](https://gitee.com/veal98/images/raw/master/img/20201031105420.png)

æ‰€ä»¥æˆ‘ä»¬æœ‰å¿…è¦å¯¹çŠ¶æ€çš„ç»´åº¦è¿›è¡Œå‹ç¼©ï¼Œè§£å†³åŠæ³•å°±æ˜¯ **ä»·å€¼å‡½æ•°è¿‘ä¼¼ Value Function Approximation** ğŸ‘‡

ä»€ä¹ˆæ˜¯ä»·å€¼å‡½æ•°è¿‘ä¼¼å‘¢ï¼Ÿè¯´èµ·æ¥å¾ˆç®€å•ï¼Œå°±æ˜¯ç”¨ä¸€ä¸ªå‡½æ•°æ¥è¡¨ç¤ºQ(s,a)ã€‚å³

<img src="https://gitee.com/veal98/images/raw/master/img/20201110220825.png" style="zoom:80%;" />

få¯ä»¥æ˜¯ä»»æ„ç±»å‹çš„å‡½æ•°ï¼Œæ¯”å¦‚çº¿æ€§å‡½æ•°ï¼š

<img src="https://gitee.com/veal98/images/raw/master/img/20201110220838.png" style="zoom:80%;" />

 å…¶ä¸­ $w_1, w_2, b$ æ˜¯å‡½æ•° f çš„å‚æ•°ã€‚

<u>é€šè¿‡å‡½æ•°è¡¨ç¤ºï¼Œæˆ‘ä»¬å°±å¯ä»¥æ— æ‰€è°“ s åˆ°åº•æ˜¯å¤šå¤§çš„ç»´åº¦ï¼Œåæ­£æœ€åéƒ½é€šè¿‡çŸ©é˜µè¿ç®—é™ç»´è¾“å‡ºä¸ºå•å€¼çš„ Qã€‚è¿™å°±æ˜¯ä»·å€¼å‡½æ•°è¿‘ä¼¼çš„åŸºæœ¬æ€è·¯ã€‚</u>

å¦‚æœæˆ‘ä»¬ç”¨ w æ¥ç»Ÿä¸€è¡¨ç¤ºå‡½æ•°fçš„å‚æ•°ï¼Œé‚£ä¹ˆå°±æœ‰

<img src="https://gitee.com/veal98/images/raw/master/img/20201110221013.png" style="zoom:80%;" />

ä¸ºä»€ä¹ˆå«è¿‘ä¼¼ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸çŸ¥é“Qå€¼çš„å®é™…åˆ†å¸ƒæƒ…å†µï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ç”¨ä¸€ä¸ªå‡½æ•°æ¥è¿‘ä¼¼Qå€¼çš„åˆ†å¸ƒï¼Œæ‰€ä»¥ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯

<img src="https://gitee.com/veal98/images/raw/master/img/20201110221028.png" style="zoom:80%;" />

**DQN å°±æ˜¯å°† Q-Learning å’Œç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œç”¨ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºè¿™ä¸ªå‡½æ•° fï¼ˆå³ Q å‡½æ•°çš„è¿‘ä¼¼ï¼‰**



## 2. DQN ç®—æ³•

æˆ‘ä»¬çŸ¥é“ï¼Œç¥ç»ç½‘ç»œçš„è®­ç»ƒæ˜¯ä¸€ä¸ªæœ€ä¼˜åŒ–é—®é¢˜ï¼Œæœ€ä¼˜åŒ–ä¸€ä¸ªæŸå¤±å‡½æ•° loss functionï¼Œä¹Ÿå°±æ˜¯æ ‡ç­¾å’Œç½‘ç»œè¾“å‡ºçš„åå·®ï¼Œç›®æ ‡æ˜¯è®©æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æœ‰æ ·æœ¬ï¼Œå·¨é‡çš„æœ‰æ ‡ç­¾æ•°æ®ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•æ¥æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚

æ‰€ä»¥ï¼Œè¦è®­ç»ƒ Q ç½‘ç»œï¼Œæˆ‘ä»¬è¦èƒ½å¤Ÿä¸º Q ç½‘ç»œæä¾›æœ‰æ ‡ç­¾çš„æ ·æœ¬ã€‚

æ‰€ä»¥ï¼Œé—®é¢˜å˜æˆï¼šå¦‚ä½•ä¸º Q ç½‘ç»œæä¾›æœ‰æ ‡ç­¾çš„æ ·æœ¬ï¼Ÿ

å¤§å®¶å›æƒ³ä¸€ä¸‹ Q-Learning ç®—æ³•ï¼ŒQ å€¼çš„æ›´æ–°ä¾é ä»€ä¹ˆï¼Ÿä¾é çš„æ˜¯åˆ©ç”¨ Reward å’Œ Q è®¡ç®—å‡ºæ¥çš„ç›®æ ‡ Q å€¼ / Q ç°å®ï¼š

<img src="https://gitee.com/veal98/images/raw/master/img/20201110221728.png" style="zoom: 50%;" />

å› æ­¤ï¼Œæˆ‘ä»¬æŠŠç›®æ ‡Qå€¼ / Q ç°å® ä½œä¸ºæ ‡ç­¾ä¸å°±å®Œäº†ï¼Ÿæˆ‘ä»¬çš„ç›®æ ‡ä¸å°±æ˜¯è®©Qä¼°è®¡å€¼è¶‹è¿‘äºç›®æ ‡Qå€¼/Qç°å®å€¼å—ï¼Ÿ

å› æ­¤ï¼ŒDQN çš„æŸå¤±å‡½æ•°å°±æ˜¯

<img src="https://gitee.com/veal98/images/raw/master/img/20201110222021.png" style="zoom:67%;" />

> ğŸ’¡ $s'ï¼Œa'$ å³ä¸‹ä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œã€‚è¿™é‡Œç”¨äº†David Silverçš„è¡¨ç¤ºæ–¹å¼ï¼Œçœ‹èµ·æ¥æ¯”è¾ƒæ¸…æ™°ã€‚

## 3. DQN ä¸¤å¤§åˆ©å™¨

<u>ä»¥ä¸Šå¹¶ä¸æ˜¯ DQN ä¼šç©ç”µåŠ¨çš„æ ¹æœ¬åŸå› . è¿˜æœ‰ä¸¤å¤§å› ç´ æ”¯æ’‘ç€ DQN ä½¿å¾—å®ƒå˜å¾—æ— æ¯”å¼ºå¤§. è¿™ä¸¤å¤§å› ç´ å°±æ˜¯ `ç»éªŒå›æ”¾ Experience replay` å’Œ `å†»ç»“ç›®æ ‡ç½‘ç»œ Fixed Target Newtwork`</u>.

### â‘  ç»éªŒå›æ”¾ Experienced Replay

`Experience replay` ç®€å•æ¥è¯´, DQN æœ‰ä¸€ä¸ªè®°å¿†åº“ç”¨äºå­¦ä¹ ä¹‹å‰çš„ç»å†. **æ¯æ¬¡è®­ç»ƒæ—¶ï¼Œå°†æœ€æ–°ç­–ç•¥äº§ç”Ÿçš„æ•°æ®å¯¹ $(s,a,r,s')$ å­˜å…¥è®°å¿†åº“**ã€‚ï¼ˆä¹‹å‰æåˆ°è¿‡, Q learning æ˜¯ä¸€ç§ off-policy ç¦»çº¿å­¦ä¹ æ³•, å®ƒèƒ½å­¦ä¹ å½“å‰ç»å†ç€çš„, ä¹Ÿèƒ½å­¦ä¹ è¿‡å»ç»å†è¿‡çš„, ç”šè‡³æ˜¯å­¦ä¹ åˆ«äººçš„ç»å†. ï¼‰**æ¯æ¬¡ DQN æ›´æ–°çš„æ—¶å€™, æˆ‘ä»¬ä»è®°å¿†åº“ä¸­ã€éšæœºæŠ½å–ã€‘ä¸€äº›ä¹‹å‰çš„ç»å†è¿›è¡Œå­¦ä¹ ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºç»éªŒå›æ”¾ã€‚** 

ç»éªŒå›æ”¾æ± æŠ€æœ¯æ‰“ç ´äº†ç»éªŒæ•°æ®é—´çš„å¼ºç›¸å…³æ€§ï¼Œä½¿æ ·æœ¬è¿‘ä¹æ»¡è¶³ç‹¬ç«‹åŒåˆ†å¸ƒçš„ç‰¹æ€§ï¼ŒåŒæ—¶æå¤§åœ°æé«˜äº†æ•°æ®çš„ä½¿ç”¨æ•ˆç‡ã€‚

#### ä¼˜å…ˆç»éªŒå›æ”¾ Prioritized Experience Replay

**ã€ä¼˜å…ˆç»éªŒå›æ”¾ `Prioritized Experience Replay`ã€‘**ï¼šåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå¯¹äºåœ¨ç»éªŒ buffer é‡Œé¢çš„æ ·æœ¬ï¼Œé‚£äº›å…·æœ‰æ›´å¥½çš„ **TD è¯¯å·®ï¼ˆ Qç°å® - Qä¼°è®¡ï¼‰**çš„æ ·æœ¬ä¼šæœ‰æ›´é«˜çš„æ¦‚ç‡è¢«é‡‡æ ·ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

<img src="https://gitee.com/veal98/images/raw/master/img/20201028213309.png" style="zoom:45%;" />

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå‚æ•°æ›´æ–°çš„è¿‡ç¨‹ä¹Ÿä¼šæœ‰ç›¸åº”çš„æ›´æ”¹ã€‚

### â‘¡ å†»ç»“ç›®æ ‡ç½‘ç»œ Fixed Target Newtwork

`Fixed Target Newtwork` ä¹Ÿæ˜¯ä¸€ç§æ‰“ä¹±ç›¸å…³æ€§çš„æœºç†, **è®¡ç®— Q ç°å®çš„ç›®æ ‡ç½‘ç»œ $Q(s')$ å’Œè®¡ç®— Q ä¼°è®¡çš„ç½‘ç»œ $Q(s)$ éƒ½æ¥è‡ªåŒä¸€ç½‘ç»œï¼Œä½†æ˜¯è®¡ç®— Q ä¼°è®¡çš„ç¥ç»ç½‘ç»œå…·å¤‡æœ€æ–°çš„å‚æ•°, è€Œè®¡ç®— Q ç°å® / ç›®æ ‡ Q å€¼çš„ç¥ç»ç½‘ç»œä½¿ç”¨çš„å‚æ•°åˆ™æ˜¯å¾ˆä¹…ä»¥å‰çš„ï¼Œç›¸å½“äºåœ¨ $Q(s')$ æœªæ›´æ–°æ—¶å¤„äºå†»ç»“çŠ¶æ€**ã€‚

ç›®æ ‡ç½‘ç»œï¼ˆQ ç°å®ï¼‰å‚æ•°å®é™…ä¸Šæ˜¯å¯¹ Q ä¼°è®¡ç½‘ç»œå‚æ•°çš„å¤åˆ¶ï¼Œæ¯éš”ä¸€å®šçš„æ—¶é•¿å°±ç”¨ Q ç°å®ç½‘ç»œå‚æ•°æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°

> ğŸ’¡ ä¸€èˆ¬æˆ‘ä»¬ç§°è®¡ç®— Q ç°å®çš„ç½‘ç»œä¸ºç›®æ ‡ç½‘ç»œï¼Œè®¡ç®— Q ä¼°è®¡çš„ç½‘ç»œä¸º Q ç½‘ç»œã€‚

ç›®æ ‡ç½‘ç»œçš„ä½¿ç”¨æé«˜äº†ç®—æ³•çš„ç¨³å®šæ€§ï¼ŒåŠ å¿«äº†ç½‘ç»œè®­ç»ƒçš„é€Ÿåº¦ã€‚

## 4. DQN æ•´ä½“ç®—æ³•

<img src="https://gitee.com/veal98/images/raw/master/img/20201110222224.png" style="zoom: 50%;" />

ä¸ºäº†ä½¿ç”¨ Tensorflow æ¥å®ç° DQN, æ¯”è¾ƒæ¨èçš„æ–¹å¼æ˜¯**æ­å»ºä¸¤ä¸ªç¥ç»ç½‘ç»œ**, ä¸€ä¸ªç”¨äºé¢„æµ‹ Q ç°å® , ä»–ä¸ä¼šåŠæ—¶æ›´æ–°å‚æ•°. å¦ä¸€ä¸ªç”¨äºé¢„æµ‹ Q ä¼°è®¡, è¿™ä¸ªç¥ç»ç½‘ç»œæ‹¥æœ‰æœ€æ–°çš„ç¥ç»ç½‘ç»œå‚æ•°. ä¸è¿‡è¿™ä¸¤ä¸ªç¥ç»ç½‘ç»œç»“æ„æ˜¯å®Œå…¨ä¸€æ ·çš„, åªæ˜¯é‡Œé¢çš„å‚æ•°ä¸ä¸€æ ·. 

â­ **ä¸¤ä¸ªç¥ç»ç½‘ç»œæ˜¯ä¸ºäº†å›ºå®šä½ä¸€ä¸ªç¥ç»ç½‘ç»œ (Q ç°å®) çš„å‚æ•°, Q ç°å® æ˜¯ Q ä¼°è®¡ çš„ä¸€ä¸ªå†å²ç‰ˆæœ¬, æ‹¥æœ‰ Q ä¼°è®¡ å¾ˆä¹…ä¹‹å‰çš„ä¸€ç»„å‚æ•°, è€Œä¸”è¿™ç»„å‚æ•°è¢«å›ºå®šä¸€æ®µæ—¶é—´, ç„¶åå†è¢« Q ä¼°è®¡ çš„æ–°å‚æ•°æ‰€æ›¿æ¢. è€Œ Q ä¼°è®¡ çš„å‚æ•°æ˜¯ä¸æ–­åœ¨æ›´æ–°çš„**.

> â“ **DQN å’Œ Q-learning æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ**
>
> æ•´ä½“æ¥è¯´ï¼ŒDQN ä¸ Q-learning çš„ç›®æ ‡ä»·å€¼ä»¥åŠä»·å€¼çš„æ›´æ–°æ–¹å¼éƒ½éå¸¸ç›¸ä¼¼ï¼Œä¸»è¦çš„ä¸åŒç‚¹åœ¨äºï¼š
>
> - DQN å°† Q-learning ä¸æ·±åº¦å­¦ä¹ ç»“åˆï¼Œç”¨æ·±åº¦ç½‘ç»œæ¥è¿‘ä¼¼åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œè€Œ Q-learning åˆ™æ˜¯é‡‡ç”¨è¡¨æ ¼å­˜å‚¨ï¼›
> - DQN é‡‡ç”¨äº†ç»éªŒå›æ”¾çš„è®­ç»ƒæ–¹æ³•ï¼Œä»å†å²æ•°æ®ä¸­éšæœºé‡‡æ ·ï¼Œè€Œ Q-learning ç›´æ¥é‡‡ç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æ•°æ®è¿›è¡Œå­¦ä¹ 

## 5. Double DQN

**ç”±äº DQN æ˜¯åŸºäº Q-learning çš„ï¼Œ Q å€¼æ€»æ˜¯åŸºäºä½¿å¾— Q æœ€å¤§çš„ action å¾—å‡ºï¼Œå› æ­¤ Q å€¼ä¼šè¶‹å‘äºè¢«é«˜ä¼°  (overestimate)ï¼Œäºæ˜¯å¼•å…¥ double DQN**

<img src="https://gitee.com/veal98/images/raw/master/img/20201101121041.png" style="zoom: 42%;" />

æˆ‘ä»¬çŸ¥é“ DQN çš„ç¥ç»ç½‘ç»œéƒ¨åˆ†å¯ä»¥çœ‹æˆä¸€ä¸ª `æœ€æ–°çš„ç¥ç»ç½‘ç»œ` + `è€ç¥ç»ç½‘ç»œ`, ä»–ä»¬æœ‰ç›¸åŒçš„ç»“æ„, ä½†å†…éƒ¨çš„å‚æ•°æ›´æ–°å´æœ‰æ—¶å·®. å®ƒçš„ `Qç°å®/ç›®æ ‡Qå€¼` éƒ¨åˆ†æ˜¯è¿™æ ·çš„:

![](https://gitee.com/veal98/images/raw/master/img/20201101121807.png)

å› ä¸ºæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œé¢„æµ‹ `Qmax` æœ¬æ¥å°±æœ‰è¯¯å·®, æ¯æ¬¡ä¹Ÿå‘ç€æœ€å¤§è¯¯å·®çš„ `Qç°å®` æ”¹è¿›ç¥ç»ç½‘ç»œ, å°±æ˜¯å› ä¸ºè¿™ä¸ª `Qmax` å¯¼è‡´äº† overestimate.

Double DQN çš„æƒ³æ³•å°±æ˜¯å¼•å…¥å¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ‰“æ¶ˆä¸€äº›æœ€å¤§è¯¯å·®çš„å½±å“. è€Œ DQN ä¸­æœ¬æ¥å°±æœ‰ä¸¤ä¸ªç¥ç»ç½‘ç»œ, æˆ‘ä»¬ä½•ä¸åˆ©ç”¨ä¸€ä¸‹ä¼˜åŠ¿å‘¢.ï¼š

 **ä¸€ä¸ª Q-networkï¼ˆ$\hat Q$ï¼‰ ç”¨æ¥è®¡ç®— Q ä¼°è®¡ï¼ˆæœ€æ–°å‚æ•°ï¼‰ï¼Œå¦å¤–ä¸€ä¸ª Q-network ç”¨æ¥è®¡ç®— Q ç°å®ï¼ˆå‚æ•°è¾ƒè€ï¼‰**ï¼Œæˆ‘ä»¬ç”¨ `Qä¼°è®¡` çš„ç¥ç»ç½‘ç»œä¼°è®¡ `Qç°å®` ä¸­ `Qmax(s', a')` çš„æœ€å¤§åŠ¨ä½œå€¼. ç„¶åç”¨è¿™ä¸ªè¢« `Qä¼°è®¡` ä¼°è®¡å‡ºæ¥çš„åŠ¨ä½œæ¥é€‰æ‹© `Qç°å®` ä¸­çš„ `Q(s')`. 

<img src="https://gitee.com/veal98/images/raw/master/img/20201111093036.png" style="zoom:67%;" />

å³ Double DQN çš„æŸå¤±å‡½æ•°ä¸ºï¼š

<img src="https://gitee.com/veal98/images/raw/master/img/20201111092624.png" style="zoom:50%;" />

## 6. Dueling DQN

åªè¦ç¨ç¨ä¿®æ”¹ DQN ä¸­ç¥ç»ç½‘ç»œçš„ç»“æ„, å°±èƒ½å¤§å¹…æå‡å­¦ä¹ æ•ˆæœ, åŠ é€Ÿæ”¶æ•›. è¿™ç§æ–°æ–¹æ³•å«åš Dueling DQN. ç”¨ä¸€å¥è¯æ¥æ¦‚æ‹¬ Dueling DQN å°±æ˜¯. å®ƒå°†æ¯ä¸ªåŠ¨ä½œçš„ Q æ‹†åˆ†æˆäº† state çš„ Value åŠ ä¸Š æ¯ä¸ªåŠ¨ä½œçš„ Advantage.

<img src="https://gitee.com/veal98/images/raw/master/img/20201101122106.png" style="zoom:40%;" />

æœ¬æ¥çš„ DQN å°±æ˜¯ç›´æ¥è¾“å‡º Q å‡½æ•°çš„å€¼ã€‚ç°åœ¨è¿™ä¸ª dueling çš„ DQNï¼Œå®ƒä¸ç›´æ¥è¾“å‡º Q å‡½æ•°çš„å€¼ï¼Œå®ƒåˆ†æˆä¸¤æ¡ path å»è¿ç®—ï¼Œ**ç¬¬ä¸€ä¸ª path ç®—å‡ºä¸€ä¸ªæ ‡é‡ $V(s)$ å³æ¯åˆ—çš„å¹³å‡å€¼**ã€‚å› ä¸ºå®ƒè·Ÿè¾“å…¥ s æ˜¯æœ‰å…³ç³»ï¼Œæ‰€ä»¥å«åš V(s)ã€‚**ç¬¬äºŒä¸ªpath ä¼šè¾“å‡ºä¸€ä¸ªå‘é‡ $A(s,a)$ å³åˆ†åˆ«ä½¿ç”¨æ¯åˆ—çš„æ•°æ®å‡å»è¯¥åˆ—çš„å¹³å‡å€¼**ï¼Œè¯¥å‘é‡å…·æœ‰åˆ—é›¶å’Œç‰¹å¾ã€‚

æŠŠ $V(s)$ å’Œ $A(S,a)$ åŠ èµ·æ¥å°±å¾—åˆ° Q å‡½æ•°çš„å€¼

<img src="https://gitee.com/veal98/images/raw/master/img/20201028211522.png" style="zoom: 22%;" />

## 7. åŸºäº CartPole çš„ DQN å®ç°

```python
import tensorflow
from tensorflow import keras
import wandb # åœ¨çº¿æ¨¡å‹å¯è§†åŒ–å·¥å…·
import gym
import argparse # argparseæ˜¯ä¸€ä¸ªPythonæ¨¡å—ï¼šå‘½ä»¤è¡Œé€‰é¡¹ã€å‚æ•°å’Œå­å‘½ä»¤è§£æå™¨ã€‚
import numpy as np
from collections import deque
import random
```

> ğŸ’¡ wandb æ˜¯ä¸€æ¬¾åœ¨çº¿æ¨¡å‹å¯è§†åŒ–å·¥å…·ï¼Œå‚è§ [wandb.com](https://www.wandb.com/)

```python
wandb.init(name='DQN', project="rl_tf2")
```

> ğŸ’¡ `argparse `æ˜¯ä¸€ä¸ªPythonæ¨¡å—ï¼šå‘½ä»¤è¡Œé€‰é¡¹ã€å‚æ•°å’Œå­å‘½ä»¤è§£æå™¨. <u>æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¯¥æ¨¡å—å®šä¹‰å¸¸ç”¨å‚æ•°, ä½¿ç¨‹åºæ˜“è¯»</u>

```python
parser = argparse.ArgumentParser()

parser.add_argument('--gamma', type = float, default = 0.95) # æŠ˜æ‰£ç³»æ•°
parser.add_argument('--lr', type = float, default = 0.005) # å­¦ä¹ ç‡
parser.add_argument('--batch_size', type=int, default = 32) # æ¯æ¬¡è®­ç»ƒ/é‡‡æ ·çš„æ•°æ®é‡
parser.add_argument('--buffer_limit', type=int, default = 10000) # ç»éªŒå›æ”¾æ± å­˜å‚¨çš„æœ€å¤§æ•°æ®é‡
parser.add_argument('--epsilon', type=float, default = 1.0) # Îµ-greedy
# epsilon æ¦‚ç‡ä¼šä» 100% åˆ° 1% è¡°å‡ï¼Œè¶Šåˆ°åé¢è¶Šä½¿ç”¨ Q å€¼æœ€å¤§çš„åŠ¨ä½œ
parser.add_argument('--epsilon_decay', type=float, default = 0.995) # Îµçš„è¡°å‡é€Ÿç‡
parser.add_argument('--epsilon_min', type=float, default = 0.01) # Îµæœ€å°å€¼

args = parser.parse_known_args()[0] # è·å–å‚æ•°
# parser.parse_args() å‡ºé”™äº†ï¼Œä¸çŸ¥é“ä¸ºå•¥
```



```python
# ç»éªŒå›æ”¾æ± 
class ReplayBuffer:
    def __init__(self):
        # åŒå‘é˜Ÿåˆ—,åˆ©ç”¨ ReplayBuffer ç±»ä¸­çš„ Deque å¯¹è±¡æ¥å®ç°ç»éªŒå›æ”¾æ± çš„åŠŸèƒ½
        self.buffer = deque(maxlen = args.buffer_limit)
        
    # å°†æœ€æ–°æ•°æ®(s,a,r,s',done)å­˜å…¥å›æ”¾æ± 
    # done è¡¨ç¤ºæ¸¸æˆæ˜¯å¦ç»“æŸ
    def put(self, state, action, reward, next_state, done):
        self.buffer.append([ state, action, reward, next_state, done])
    
    # ä»å›æ”¾æ± ä¸­éšæœºæŠ½æ ·
    def sample(self):
        # ä»å›æ”¾æ± éšæœºé‡‡æ · batch_size = 32 ä¸ª 5 å…ƒç»„
        sample = random.sample(self.buffer, args.batch_size)
        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))
        states = np.array(states).reshape(args.batch_size, -1)
        next_states = np.array(next_states).reshape(args.batch_size, -1)
        return states, actions, rewards, next_states, done
    
    # å›æ”¾æ± ä¸­çš„æ•°æ®é‡
    def size(self):
        return len(self.buffer)
```





```python
# æ„å»ºæ¨¡å‹
class Model:
    def __init__(self, state_dim, action_dim):
        self.state_dim  = state_dim
        self.action_dim = action_dim
        self.epsilon = args.epsilon
        self.model = self.create_model() # æ„å»ºæ¨¡å‹
    
    # æ„å»ºæ¨¡å‹
    def create_model(self):
        model = keras.models.Sequential([
            keras.layers.Input((self.state_dim, )), # è¾“å…¥å±‚
            keras.layers.Dense(32, activation = 'relu'), # éšè—å±‚
            keras.layers.Dense(16, activation = 'relu'), # éšè—å±‚
            keras.layers.Dense(self.action_dim) # è¾“å‡ºå±‚
        ])
        model.compile(loss='mse', optimizer = keras.optimizers.Adam(args.lr))
        return model
    
    # é¢„æµ‹(è¾“å‡ºçš„æ˜¯å½“å‰çŠ¶æ€ä¸‹å¯¹åº”æ¯ä¸ªåŠ¨ä½œçš„Qå€¼ï¼‰
    def predict(self, state):
        return self.model.predict(state)
    
    # æ ¹æ®å½“å‰çŠ¶æ€ä¸‹çš„æœ€å¤§ Q å€¼é€‰å–åŠ¨ä½œï¼ˆÎµ-greedy)
    def get_action(self, state):
        state = np.reshape(state, [1, self.state_dim])
        self.epsilon *= args.epsilon_decay # epsilon ä¸æ–­è¡°å‡
        self.epsilon = max(self.epsilon, args.epsilon_min)
        q_value = self.predict(state)[0] 
        if np.random.random() < self.epsilon:
            # æœ‰ epsilon çš„æ¦‚ç‡éšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randint(0, self.action_dim - 1)
        return np.argmax(q_value)
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆåˆ©ç”¨ Q ç°å®æ¥è®­ç»ƒç½‘ç»œ)
    def train(self, states, targets):
        self.model.fit(states, targets, epochs = 1, verbose = 0)
```



```python
class Agent:
    def __init__(self, env):
        self.env = env
        self.state_dim = self.env.observation_space.shape[0] # çŠ¶æ€è¡¨ç¤ºï¼Œç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸ªæ•°
        self.action_dim = self.env.action_space.n # åŠ¨ä½œä¸ªæ•°ï¼Œç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸ªæ•°
        
        self.model = Model(self.state_dim, self.action_dim) # Q ä¼°è®¡
        self.target_model = Model(self.state_dim, self.action_dim) # Q ç°å®
        self.target_update() # å°† Q ä¼°è®¡ç½‘ç»œçš„å‚æ•°èµ‹ç»™ Q ç°å®ï¼‰
        
        self.buffer = ReplayBuffer()
    
    # å°† Q ä¼°è®¡ç½‘ç»œçš„å‚æ•°èµ‹ç»™ Q ç°å®
    def target_update(self):
        weights = self.model.model.get_weights()
        self.target_model.model.set_weights(weights)
    
    # è®­ç»ƒç½‘ç»œ ï¼Œæ›´æ–° Q ä¼°è®¡å‚æ•°
    def replay(self):
        for _ in range(10):
            states, actions, rewards, next_states, done = self.buffer.sample() # ä»å›æ”¾æ± é‡‡æ ·
            targets = self.target_model.predict(states) # Q ä¼°è®¡
            next_q_values = self.target_model.predict(next_states).max(axis=1) 
            targets[range(args.batch_size), actions] = rewards + (1-done) * args.gamma * next_q_values  # Q ç°å®
            self.model.train(states, targets)
    
    def train(self, max_episodes=1000):
        # æœ€å¤šè®­ç»ƒ 1000 æ¬¡
        for episode in range(max_episodes):
            done, total_reward = False, 0
            state = self.env.reset()
            while not done:
                action = self.model.get_action(state)
                next_state, reward, done, _ = self.env.step(action)
                self.buffer.put(state, action, reward*0.01, next_state, done)
                total_reward += reward
                state = next_state
                
            # å½“ç»éªŒæ± ä¸­å¤§äº 32 æ¡æ•°æ®æ—¶å†è¿›è¡Œè®­ç»ƒ
            if self.buffer.size() >= args.batch_size:
                self.replay()
                
            self.target_update() # å°† Q ä¼°è®¡ç½‘ç»œçš„å‚æ•°èµ‹ç»™ Q ç°å®
            
            print('Episode{} EpisodeReward={}'.format(episode, total_reward))
            wandb.log({'Reward': total_reward})
            
```

è¿è¡Œç¨‹åºï¼š

```python
def main():
    env = gym.make('CartPole-v1')
    agent = Agent(env)
    agent.train(max_episodes=1000)

if __name__ == "__main__":
    main()
```



## ğŸ“š References

- [Bilibili - æå®æ¯…ã€Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - æå®æ¯…æ·±åº¦å¼ºåŒ–å­¦ä¹ ç¬”è®° - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [å¼ºåŒ–å­¦ä¹ çº²è¦](https://github.com/zhoubolei/introRL)
- [è«çƒ¦ Python â€” å¼ºåŒ–å­¦ä¹ ](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL-methods/)
- [DQNä»å…¥é—¨åˆ°æ”¾å¼ƒ5 æ·±åº¦è§£è¯»DQNç®—æ³•](https://zhuanlan.zhihu.com/p/21421729)
- ğŸ‘ [Github - Deep-Learning-with-TensorFlow-book](https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book)
- [Github - DeepRL-TensorFlow2](https://github.com/marload/DeepRL-TensorFlow2) - ğŸ‹ Simple implementations of various popular Deep Reinforcement Learning algorithms using TensorFlow2
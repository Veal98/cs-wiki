# ğŸ‘‘ ç¬¬ 10 ç«  æ•°æ®èšåˆä¸åˆ†ç»„è¿ç®—

---

å¯¹æ•°æ®é›†è¿›è¡Œåˆ†ç»„å¹¶å¯¹å„ç»„åº”ç”¨ä¸€ä¸ªå‡½æ•°ï¼ˆæ— è®ºæ˜¯èšåˆè¿˜æ˜¯è½¬æ¢ï¼‰ï¼Œé€šå¸¸æ˜¯æ•°æ®åˆ†æå·¥ä½œä¸­çš„é‡è¦ç¯èŠ‚ã€‚åœ¨å°†æ•°æ®é›†åŠ è½½ã€èåˆã€å‡†å¤‡å¥½ä¹‹åï¼Œé€šå¸¸å°±æ˜¯è®¡ç®—åˆ†ç»„ç»Ÿè®¡æˆ–ç”Ÿæˆé€è§†è¡¨ã€‚pandasæä¾›äº†ä¸€ä¸ªçµæ´»é«˜æ•ˆçš„ `gruopby` åŠŸèƒ½ï¼Œå®ƒä½¿ä½ èƒ½ä»¥ä¸€ç§è‡ªç„¶çš„æ–¹å¼å¯¹æ•°æ®é›†è¿›è¡Œåˆ‡ç‰‡ã€åˆ‡å—ã€æ‘˜è¦ç­‰æ“ä½œã€‚

å…³ç³»å‹æ•°æ®åº“å’ŒSQLï¼ˆStructured Query Languageï¼Œç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€ï¼‰èƒ½å¤Ÿå¦‚æ­¤æµè¡Œçš„åŸå› ä¹‹ä¸€å°±æ˜¯å…¶èƒ½å¤Ÿæ–¹ä¾¿åœ°å¯¹æ•°æ®è¿›è¡Œè¿æ¥ã€è¿‡æ»¤ã€è½¬æ¢å’Œèšåˆã€‚ä½†æ˜¯ï¼ŒåƒSQLè¿™æ ·çš„æŸ¥è¯¢è¯­è¨€æ‰€èƒ½æ‰§è¡Œçš„åˆ†ç»„è¿ç®—çš„ç§ç±»å¾ˆæœ‰é™ã€‚åœ¨æœ¬ç« ä¸­ä½ å°†ä¼šçœ‹åˆ°ï¼Œç”±äºPythonå’Œpandaså¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œå¤æ‚å¾—å¤šçš„åˆ†ç»„è¿ç®—ï¼ˆåˆ©ç”¨ä»»ä½•å¯ä»¥æ¥å—pandaså¯¹è±¡æˆ–NumPyæ•°ç»„çš„å‡½æ•°ï¼‰ã€‚åœ¨æœ¬ç« ä¸­ï¼Œä½ å°†ä¼šå­¦åˆ°ï¼š

- ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªé”®ï¼ˆå½¢å¼å¯ä»¥æ˜¯å‡½æ•°ã€æ•°ç»„æˆ–DataFrameåˆ—åï¼‰åˆ†å‰²pandaså¯¹è±¡ã€‚
- è®¡ç®—åˆ†ç»„çš„æ¦‚è¿°ç»Ÿè®¡ï¼Œæ¯”å¦‚æ•°é‡ã€å¹³å‡å€¼æˆ–æ ‡å‡†å·®ï¼Œæˆ–æ˜¯ç”¨æˆ·å®šä¹‰çš„å‡½æ•°ã€‚
- åº”ç”¨ç»„å†…è½¬æ¢æˆ–å…¶ä»–è¿ç®—ï¼Œå¦‚è§„æ ¼åŒ–ã€çº¿æ€§å›å½’ã€æ’åæˆ–é€‰å–å­é›†ç­‰ã€‚
- è®¡ç®—é€è§†è¡¨æˆ–äº¤å‰è¡¨ã€‚
- æ‰§è¡Œåˆ†ä½æ•°åˆ†æä»¥åŠå…¶å®ƒç»Ÿè®¡åˆ†ç»„åˆ†æã€‚

>ğŸ”Š å¯¹**æ—¶é—´åºåˆ—**æ•°æ®çš„èšåˆï¼ˆgroupbyçš„ç‰¹æ®Šç”¨æ³•ä¹‹ä¸€ï¼‰ä¹Ÿç§°ä½œ**é‡é‡‡æ ·ï¼ˆresamplingï¼‰**ï¼Œæœ¬ä¹¦å°†åœ¨ç¬¬11ç« ä¸­å•ç‹¬å¯¹å…¶è¿›è¡Œè®²è§£ã€‚

## 10.1 GroupBy æœºåˆ¶

### 1. åˆ†ç»„åŸºæœ¬æ“ä½œ

Hadley Wickhamï¼ˆè®¸å¤šçƒ­é—¨Rè¯­è¨€åŒ…çš„ä½œè€…ï¼‰åˆ›é€ äº†ä¸€ä¸ªç”¨äºè¡¨ç¤ºåˆ†ç»„è¿ç®—çš„æœ¯è¯­`"split-apply-combine"`ï¼ˆæ‹†åˆ†ï¼åº”ç”¨ï¼åˆå¹¶ï¼‰ã€‚

- ç¬¬ä¸€ä¸ªé˜¶æ®µï¼Œpandaså¯¹è±¡ï¼ˆæ— è®ºæ˜¯Seriesã€DataFrameè¿˜æ˜¯å…¶ä»–çš„ï¼‰ä¸­çš„æ•°æ®ä¼šæ ¹æ®ä½ æ‰€æä¾›çš„ä¸€ä¸ªæˆ–å¤šä¸ªé”®è¢«æ‹†åˆ†ï¼ˆsplitï¼‰ä¸ºå¤šç»„ã€‚æ‹†åˆ†æ“ä½œæ˜¯åœ¨å¯¹è±¡çš„ç‰¹å®šè½´ä¸Šæ‰§è¡Œçš„ã€‚ä¾‹å¦‚ï¼ŒDataFrameå¯ä»¥åœ¨å…¶è¡Œï¼ˆaxis=0ï¼‰æˆ–åˆ—ï¼ˆaxis=1ï¼‰ä¸Šè¿›è¡Œåˆ†ç»„ã€‚
- ç„¶åï¼Œå°†ä¸€ä¸ªå‡½æ•°åº”ç”¨ï¼ˆapplyï¼‰åˆ°å„ä¸ªåˆ†ç»„å¹¶äº§ç”Ÿä¸€ä¸ªæ–°å€¼ã€‚
- æœ€åï¼Œæ‰€æœ‰è¿™äº›å‡½æ•°çš„æ‰§è¡Œç»“æœä¼šè¢«åˆå¹¶ï¼ˆcombineï¼‰åˆ°æœ€ç»ˆçš„ç»“æœå¯¹è±¡ä¸­ã€‚ç»“æœå¯¹è±¡çš„å½¢å¼ä¸€èˆ¬å–å†³äºæ•°æ®ä¸Šæ‰€æ‰§è¡Œçš„æ“ä½œã€‚ä¸‹å›¾å¤§è‡´è¯´æ˜äº†ä¸€ä¸ªç®€å•çš„åˆ†ç»„èšåˆè¿‡ç¨‹ã€‚

<img src="https://gitee.com/veal98/images/raw/master/img/20200616100418.png" style="zoom:50%;" />

åˆ†ç»„é”®å¯ä»¥æœ‰å¤šç§å½¢å¼ï¼Œä¸”ç±»å‹ä¸å¿…ç›¸åŒï¼š

- åˆ—è¡¨æˆ–æ•°ç»„ï¼Œå…¶é•¿åº¦ä¸å¾…åˆ†ç»„çš„è½´ä¸€æ ·ã€‚
- è¡¨ç¤ºDataFrameæŸä¸ªåˆ—åçš„å€¼ã€‚
- å­—å…¸æˆ–Seriesï¼Œç»™å‡ºå¾…åˆ†ç»„è½´ä¸Šçš„å€¼ä¸åˆ†ç»„åä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚
- å‡½æ•°ï¼Œç”¨äºå¤„ç†è½´ç´¢å¼•æˆ–ç´¢å¼•ä¸­çš„å„ä¸ªæ ‡ç­¾ã€‚

æ³¨æ„ï¼Œåä¸‰ç§éƒ½åªæ˜¯å¿«æ·æ–¹å¼è€Œå·²ï¼Œå…¶æœ€ç»ˆç›®çš„ä»ç„¶æ˜¯äº§ç”Ÿä¸€ç»„ç”¨äºæ‹†åˆ†å¯¹è±¡çš„å€¼ã€‚å¦‚æœè§‰å¾—è¿™äº›ä¸œè¥¿çœ‹èµ·æ¥å¾ˆæŠ½è±¡ï¼Œä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘å°†åœ¨æœ¬ç« ä¸­ç»™å‡ºå¤§é‡æœ‰å…³äºæ­¤çš„ç¤ºä¾‹ã€‚é¦–å…ˆæ¥çœ‹çœ‹ä¸‹é¢è¿™ä¸ªéå¸¸ç®€å•çš„è¡¨æ ¼å‹æ•°æ®é›†ï¼ˆä»¥DataFrameçš„å½¢å¼ï¼‰ï¼š

```python
In [10]: df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
   ....:                    'key2' : ['one', 'two', 'one', 'two', 'one'],
   ....:                    'data1' : np.random.randn(5),
   ....:                    'data2' : np.random.randn(5)})

In [11]: df
Out[11]: 
      data1     data2 key1 key2
0 -0.204708  1.393406    a  one
1  0.478943  0.092908    a  two
2 -0.519439  0.281746    b  one
3 -0.555730  0.769023    b  two
4  1.965781  1.246435    a  one
```

å‡è®¾ä½ æƒ³è¦æŒ‰key1è¿›è¡Œåˆ†ç»„ï¼Œå¹¶è®¡ç®—data1åˆ—çš„å¹³å‡å€¼ã€‚å®ç°è¯¥åŠŸèƒ½çš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œè€Œæˆ‘ä»¬è¿™é‡Œè¦ç”¨çš„æ˜¯ï¼š**è®¿é—®data1ï¼Œå¹¶æ ¹æ®key1è°ƒç”¨groupby**ï¼š
```python
In [12]: grouped = df['data1'].groupby(df['key1'])

In [13]: grouped
Out[13]: <pandas.core.groupby.SeriesGroupBy object at 0x7faa31537390>
```

å˜é‡groupedæ˜¯ä¸€ä¸ªGroupByå¯¹è±¡ã€‚å®ƒå®é™…ä¸Šè¿˜æ²¡æœ‰è¿›è¡Œä»»ä½•è®¡ç®—ï¼Œåªæ˜¯å«æœ‰ä¸€äº›æœ‰å…³åˆ†ç»„é”® `df['key1']` çš„ä¸­é—´æ•°æ®è€Œå·²ã€‚æ¢å¥è¯è¯´ï¼Œè¯¥å¯¹è±¡å·²ç»æœ‰äº†æ¥ä¸‹æ¥å¯¹å„åˆ†ç»„æ‰§è¡Œè¿ç®—æ‰€éœ€çš„ä¸€åˆ‡ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨GroupByçš„meanæ–¹æ³•æ¥è®¡ç®—åˆ†ç»„å¹³å‡å€¼ï¼š
```python
In [14]: grouped.mean()
Out[14]: 
key1
a    0.746672
b   -0.537585
Name: data1, dtype: float64
```

ç¨åæˆ‘å°†è¯¦ç»†è®²è§£ `.mean()` çš„è°ƒç”¨è¿‡ç¨‹ã€‚è¿™é‡Œæœ€é‡è¦çš„æ˜¯ï¼Œæ•°æ®ï¼ˆSeriesï¼‰æ ¹æ®åˆ†ç»„é”®è¿›è¡Œäº†èšåˆï¼Œäº§ç”Ÿäº†ä¸€ä¸ªæ–°çš„Seriesï¼Œå…¶ç´¢å¼•ä¸ºkey1åˆ—ä¸­çš„å”¯ä¸€å€¼ã€‚ä¹‹æ‰€ä»¥ç»“æœä¸­ç´¢å¼•çš„åç§°ä¸ºkey1ï¼Œæ˜¯å› ä¸ºåŸå§‹DataFrameçš„åˆ— `df['key1'] `å°±å«è¿™ä¸ªåå­—ã€‚

å¦‚æœæˆ‘ä»¬ä¸€æ¬¡ä¼ å…¥å¤šä¸ªæ•°ç»„çš„åˆ—è¡¨ï¼Œå°±ä¼šå¾—åˆ°ä¸åŒçš„ç»“æœï¼š
```python
In [15]: means = df['data1'].groupby([df['key1'], df['key2']]).mean()

In [16]: means
Out[16]: 
key1  key2
a     one     0.880536
      two     0.478943
b     one    -0.519439
      two    -0.555730
Name: data1, dtype: float64
```

è¿™é‡Œï¼Œæˆ‘é€šè¿‡ä¸¤ä¸ªé”®å¯¹æ•°æ®è¿›è¡Œäº†åˆ†ç»„ï¼Œå¾—åˆ°çš„Serieså…·æœ‰ä¸€ä¸ªå±‚æ¬¡åŒ–ç´¢å¼•ï¼ˆç”±å”¯ä¸€çš„é”®å¯¹ç»„æˆï¼‰ï¼š
```python
In [17]: means.unstack()
Out[17]: 
key2       one       two
key1                    
a     0.880536  0.478943
b    -0.519439 -0.555730
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåˆ†ç»„é”®å‡ä¸º Seriesã€‚å®é™…ä¸Šï¼Œåˆ†ç»„é”®å¯ä»¥æ˜¯ä»»ä½•é•¿åº¦é€‚å½“çš„æ•°ç»„ï¼š
```python
In [18]: states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])

In [19]: years = np.array([2005, 2005, 2006, 2005, 2006])

In [20]: df['data1'].groupby([states, years]).mean()
Out[20]: 
California  2005    0.478943
            2006   -0.519439
Ohio        2005   -0.380219
            2006    1.965781
Name: data1, dtype: float64
```

é€šå¸¸ï¼Œåˆ†ç»„ä¿¡æ¯å°±ä½äºç›¸åŒçš„è¦å¤„ç†DataFrameä¸­ã€‚è¿™é‡Œï¼Œä½ è¿˜å¯ä»¥å°†åˆ—åï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€æ•°å­—æˆ–å…¶ä»–Pythonå¯¹è±¡ï¼‰ç”¨ä½œåˆ†ç»„é”®ï¼š
```python
In [21]: df.groupby('key1').mean()
Out[21]: 
         data1     data2
key1
a     0.746672  0.910916
b    -0.537585  0.525384

In [22]: df.groupby(['key1', 'key2']).mean()
Out[22]: 
              data1     data2
key1 key2                    
a    one   0.880536  1.319920
     two   0.478943  0.092908
b    one  -0.519439  0.281746
     two  -0.555730  0.769023
```

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°äº†ï¼Œç¬¬ä¸€ä¸ªä¾‹å­åœ¨æ‰§è¡Œ `df.groupby('key1').mean()` æ—¶ï¼Œç»“æœä¸­æ²¡æœ‰key2åˆ—ã€‚è¿™æ˜¯å› ä¸º `df['key2']` ä¸æ˜¯æ•°å€¼æ•°æ®ï¼ˆä¿—ç§°â€œ**éº»çƒ¦åˆ—**â€ï¼‰ï¼Œæ‰€ä»¥è¢«ä»ç»“æœä¸­æ’é™¤äº†ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ•°å€¼åˆ—éƒ½ä¼šè¢«èšåˆï¼Œè™½ç„¶æœ‰æ—¶å¯èƒ½ä¼šè¢«è¿‡æ»¤ä¸ºä¸€ä¸ªå­é›†ï¼Œç¨åå°±ä¼šç¢°åˆ°ã€‚

æ— è®ºä½ å‡†å¤‡æ‹¿groupbyåšä»€ä¹ˆï¼Œéƒ½æœ‰å¯èƒ½ä¼šç”¨åˆ°**GroupByçš„ `size` æ–¹æ³•ï¼Œå®ƒå¯ä»¥è¿”å›ä¸€ä¸ªå«æœ‰åˆ†ç»„å¤§å°çš„Series**ï¼š
```python
In [23]: df.groupby(['key1', 'key2']).size()
Out[23]: 
key1  key2
a     one     2
      two     1
b     one     1
      two     1
dtype: int64
```

æ³¨æ„ï¼Œ<u>ä»»ä½•åˆ†ç»„å…³é”®è¯ä¸­çš„ç¼ºå¤±å€¼ï¼Œéƒ½ä¼šè¢«ä»ç»“æœä¸­é™¤å»</u>ã€‚

### 2. å¯¹åˆ†ç»„è¿›è¡Œè¿­ä»£
GroupBy å¯¹è±¡æ”¯æŒè¿­ä»£ï¼Œ**å¯ä»¥äº§ç”Ÿä¸€ç»„äºŒå…ƒå…ƒç»„ï¼ˆç”±åˆ†ç»„åå’Œæ•°æ®å—ç»„æˆï¼‰**ã€‚çœ‹ä¸‹é¢çš„ä¾‹å­ï¼š
```python
In [24]: for name, group in df.groupby('key1'):
   ....:     print(name)
   ....:     print(group)
   ....:
a
      data1     data2 key1 key2
0 -0.204708  1.393406    a  one
1  0.478943  0.092908    a  two
4  1.965781  1.246435    a  one
b
      data1     data2 key1 key2
2 -0.519439  0.281746    b  one
3 -0.555730  0.769023    b  two
```

**å¯¹äºå¤šé‡é”®çš„æƒ…å†µï¼Œå…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ å°†ä¼šæ˜¯ç”±é”®å€¼ç»„æˆçš„å…ƒç»„**ï¼š

```python
In [25]: for (k1, k2), group in df.groupby(['key1', 'key2']):
   ....:     print((k1, k2))
   ....:     print(group)
   ....:
('a', 'one')
      data1     data2 key1 key2
0 -0.204708  1.393406    a  one
4  1.965781  1.246435    a  one
('a', 'two')
      data1     data2 key1 key2
1  0.478943  0.092908    a  two
('b', 'one')
      data1     data2 key1 key2
2 -0.519439  0.281746    b  one
('b', 'two')
     data1     data2 key1 key2
3 -0.55573  0.769023    b  two
```

å½“ç„¶ï¼Œä½ å¯ä»¥å¯¹è¿™äº›æ•°æ®ç‰‡æ®µåšä»»ä½•æ“ä½œã€‚æœ‰ä¸€ä¸ªä½ å¯èƒ½ä¼šè§‰å¾—æœ‰ç”¨çš„è¿ç®—ï¼šå°†è¿™äº›æ•°æ®ç‰‡æ®µåšæˆä¸€ä¸ªå­—å…¸ï¼š
```python
In [26]: pieces = dict(list(df.groupby('key1')))

In [27]: pieces['b']
Out[27]: 
      data1     data2 key1 key2
2 -0.519439  0.281746    b  one
3 -0.555730  0.769023    b  two
```

groupby é»˜è®¤æ˜¯åœ¨ `axis=0` ä¸Šè¿›è¡Œåˆ†ç»„çš„ï¼Œé€šè¿‡è®¾ç½®ä¹Ÿå¯ä»¥åœ¨å…¶ä»–ä»»ä½•è½´ä¸Šè¿›è¡Œåˆ†ç»„ã€‚æ‹¿ä¸Šé¢ä¾‹å­ä¸­çš„df æ¥è¯´ï¼Œæˆ‘ä»¬**å¯ä»¥æ ¹æ® `dtype` å¯¹åˆ—è¿›è¡Œåˆ†ç»„**ï¼š
```python
In [28]: df.dtypes
Out[28]: 
data1    float64
data2    float64
key1      object
key2      object
dtype: object

In [29]: grouped = df.groupby(df.dtypes, axis=1)
```

å¯ä»¥å¦‚ä¸‹æ‰“å°åˆ†ç»„ï¼š
```python
In [30]: for dtype, group in grouped:
   ....:     print(dtype)
   ....:     print(group)
   ....:
float64
      data1     data2
0 -0.204708  1.393406
1  0.478943  0.092908
2 -0.519439  0.281746
3 -0.555730  0.769023
4  1.965781  1.246435
object
  key1 key2
0    a  one
1    a  two
2    b  one
3    b  two
4    a  one
```

### 3. é€‰å–ä¸€åˆ—æˆ–åˆ—çš„å­é›†
å¯¹äºç”±DataFrameäº§ç”Ÿçš„GroupByå¯¹è±¡ï¼Œå¦‚æœç”¨ä¸€ä¸ªï¼ˆå•ä¸ªå­—ç¬¦ä¸²ï¼‰æˆ–ä¸€ç»„ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼‰åˆ—åå¯¹å…¶è¿›è¡Œç´¢å¼•ï¼Œå°±èƒ½å®ç°é€‰å–éƒ¨åˆ†åˆ—è¿›è¡Œèšåˆçš„ç›®çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š
```python
df.groupby('key1')['data1']
df.groupby('key1')[['data2']]
```

æ˜¯ä»¥ä¸‹ä»£ç çš„è¯­æ³•ç³–ï¼š
```python
df['data1'].groupby(df['key1'])
df[['data2']].groupby(df['key1'])
```

å°¤å…¶å¯¹äºå¤§æ•°æ®é›†ï¼Œå¾ˆå¯èƒ½åªéœ€è¦å¯¹éƒ¨åˆ†åˆ—è¿›è¡Œèšåˆã€‚ä¾‹å¦‚ï¼Œåœ¨å‰é¢é‚£ä¸ªæ•°æ®é›†ä¸­ï¼Œå¦‚æœåªéœ€è®¡ç®—data2åˆ—çš„å¹³å‡å€¼å¹¶ä»¥DataFrameå½¢å¼å¾—åˆ°ç»“æœï¼Œå¯ä»¥è¿™æ ·å†™ï¼š
```python
In [31]: df.groupby(['key1', 'key2'])[['data2']].mean()
Out[31]: 
              data2
key1 key2          
a    one   1.319920
     two   0.092908
b    one   0.281746
     two   0.769023
```

â­ **è¿™ç§ç´¢å¼•æ“ä½œæ‰€è¿”å›çš„å¯¹è±¡æ˜¯ä¸€ä¸ªå·²åˆ†ç»„çš„ DataFrameï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯åˆ—è¡¨æˆ–æ•°ç»„ï¼‰æˆ–å·²åˆ†ç»„çš„Seriesï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯æ ‡é‡å½¢å¼çš„å•ä¸ªåˆ—åï¼‰**ï¼š

![](https://gitee.com/veal98/images/raw/master/img/20200616102829.png)

![](https://gitee.com/veal98/images/raw/master/img/20200616102853.png)



```python
In [32]: s_grouped = df.groupby(['key1', 'key2'])['data2']

In [33]: s_grouped
Out[33]: <pandas.core.groupby.SeriesGroupBy object at 0x7faa30c78da0>

In [34]: s_grouped.mean()
Out[34]: 
key1  key2
a     one     1.319920
      two     0.092908
b     one     0.281746
      two     0.769023
Name: data2, dtype: float64
```

###4. é€šè¿‡å­—å…¸æˆ– Series è¿›è¡Œåˆ†ç»„

é™¤æ•°ç»„ä»¥å¤–ï¼Œåˆ†ç»„ä¿¡æ¯è¿˜å¯ä»¥å…¶ä»–å½¢å¼å­˜åœ¨ã€‚æ¥çœ‹å¦ä¸€ä¸ªç¤ºä¾‹ DataFrameï¼š
```python
In [35]: people = pd.DataFrame(np.random.randn(5, 5),
   ....:                       columns=['a', 'b', 'c', 'd', 'e'],
   ....:                       index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])

In [36]: people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values

In [37]: people
Out[37]: 
               a         b         c         d         e
Joe     1.007189 -1.296221  0.274992  0.228913  1.352917
Steve   0.886429 -2.001637 -0.371843  1.669025 -0.438570
Wes    -0.539741       NaN       NaN -1.021228 -0.577087
Jim     0.124121  0.302614  0.523772  0.000940  1.343810
Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757
```

ç°åœ¨ï¼Œå‡è®¾å·²çŸ¥åˆ—çš„åˆ†ç»„å…³ç³»ï¼Œå¹¶å¸Œæœ›æ ¹æ®åˆ†ç»„è®¡ç®—åˆ—çš„å’Œï¼š
```python
In [38]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',
   ....:            'd': 'blue', 'e': 'red', 'f' : 'orange'}
```

ç°åœ¨ï¼Œä½ å¯ä»¥å°†è¿™ä¸ªå­—å…¸ä¼ ç»™ groupbyï¼Œæ¥æ„é€ æ•°ç»„ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç›´æ¥ä¼ é€’å­—å…¸ï¼ˆå­˜åœ¨æœªä½¿ç”¨çš„åˆ†ç»„é”®æ˜¯å¯ä»¥çš„ï¼Œæ¯”å¦‚è¿™é‡Œçš„ f : orangeï¼‰ï¼š
```python
In [39]: by_column = people.groupby(mapping, axis=1)

In [40]: by_column.sum()
Out[40]: 
            blue       red
Joe     0.503905  1.063885
Steve   1.297183 -1.553778
Wes    -1.021228 -1.116829
Jim     0.524712  1.770545
Travis -4.230992 -2.405455
```

Series ä¹Ÿæœ‰åŒæ ·çš„åŠŸèƒ½ï¼Œå®ƒå¯ä»¥è¢«çœ‹åšä¸€ä¸ªå›ºå®šå¤§å°çš„æ˜ å°„ï¼š
```python
In [41]: map_series = pd.Series(mapping)

In [42]: map_series
Out[42]: 
a       red
b       red
c      blue
d      blue
e       red
f    orange
dtype: object

In [43]: people.groupby(map_series, axis=1).count()
Out[43]: 
        blue  red
Joe        2    3
Steve      2    3
Wes        1    2
Jim        2    3
Travis     2    3
```

###5. é€šè¿‡å‡½æ•°è¿›è¡Œåˆ†ç»„
æ¯”èµ·ä½¿ç”¨å­—å…¸æˆ– Seriesï¼Œä½¿ç”¨ Python å‡½æ•°æ˜¯ä¸€ç§æ›´åŸç”Ÿçš„æ–¹æ³•å®šä¹‰åˆ†ç»„æ˜ å°„ã€‚ä»»ä½•è¢«å½“åšåˆ†ç»„é”®çš„å‡½æ•°éƒ½ä¼šåœ¨å„ä¸ªç´¢å¼•å€¼ä¸Šè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œå…¶è¿”å›å€¼å°±ä¼šè¢«ç”¨ä½œåˆ†ç»„åç§°ã€‚å…·ä½“ç‚¹è¯´ï¼Œä»¥ä¸Šä¸€å°èŠ‚çš„ç¤ºä¾‹ DataFrame ä¸ºä¾‹ï¼Œå…¶ç´¢å¼•å€¼ä¸ºäººçš„åå­—ã€‚ä½ å¯ä»¥**è®¡ç®—ä¸€ä¸ªå­—ç¬¦ä¸²é•¿åº¦çš„æ•°ç»„**ï¼Œæ›´ç®€å•çš„æ–¹æ³•æ˜¯ä¼ å…¥ len å‡½æ•°ï¼š
```python
In [44]: people.groupby(len).sum()
Out[44]: 
          a         b         c         d         e
3  0.591569 -0.993608  0.798764 -0.791374  2.119639
5  0.886429 -2.001637 -0.371843  1.669025 -0.438570
6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757
```

**å°†å‡½æ•°è·Ÿæ•°ç»„ã€åˆ—è¡¨ã€å­—å…¸ã€Series æ··åˆä½¿ç”¨ä¹Ÿä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºä»»ä½•ä¸œè¥¿åœ¨å†…éƒ¨éƒ½ä¼šè¢«è½¬æ¢ä¸ºæ•°ç»„**ï¼š

```python
In [45]: key_list = ['one', 'one', 'one', 'two', 'two']

In [46]: people.groupby([len, key_list]).min()
Out[46]: 
              a         b         c         d         e
3 one -0.539741 -1.296221  0.274992 -1.021228 -0.577087
  two  0.124121  0.302614  0.523772  0.000940  1.343810
5 one  0.886429 -2.001637 -0.371843  1.669025 -0.438570
6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757
```

### 6. æ ¹æ®ç´¢å¼•çº§åˆ«åˆ†ç»„
**å±‚æ¬¡åŒ–ç´¢å¼•æ•°æ®é›†æœ€æ–¹ä¾¿çš„åœ°æ–¹å°±åœ¨äºå®ƒèƒ½å¤Ÿæ ¹æ®è½´ç´¢å¼•çš„ä¸€ä¸ªçº§åˆ«è¿›è¡Œèšåˆ**ï¼š

```python
In [47]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
   ....:                                     [1, 3, 5, 1, 3]],
   ....:                                     names=['cty', 'tenor'])

In [48]: hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)

In [49]: hier_df
Out[49]: 
cty          US                            JP          
tenor         1         3         5         1         3
0      0.560145 -1.265934  0.119827 -1.063512  0.332883
1     -2.359419 -0.199543 -1.541996 -0.970736 -1.307030
2      0.286350  0.377984 -0.753887  0.331286  1.349742
3      0.069877  0.246674 -0.011862  1.004812  1.327195
```

è¦æ ¹æ®çº§åˆ«åˆ†ç»„ï¼Œä½¿ç”¨ `level` å…³é”®å­—ä¼ é€’çº§åˆ«åºå·æˆ–åå­—ï¼š
```python
In [50]: hier_df.groupby(level='cty', axis=1).count()
Out[50]: 
cty  JP  US
0     2   3
1     2   3
2     2   3
3     2   3
```

## 10.2 æ•°æ®èšåˆ

### 1. å¸¸è§çš„èšåˆè¿ç®—

ğŸ”´ **èšåˆæŒ‡çš„æ˜¯ä»»ä½•èƒ½å¤Ÿä»æ•°ç»„äº§ç”Ÿæ ‡é‡å€¼çš„æ•°æ®è½¬æ¢è¿‡ç¨‹**ã€‚ä¹‹å‰çš„ä¾‹å­å·²ç»ç”¨è¿‡ä¸€äº›ï¼Œæ¯”å¦‚ `mean`ã€`count`ã€`min` ä»¥åŠ `sum` ç­‰ã€‚è®¸å¤šå¸¸è§çš„èšåˆè¿ç®—éƒ½æœ‰è¿›è¡Œä¼˜åŒ–ã€‚ç„¶è€Œï¼Œé™¤äº†è¿™äº›æ–¹æ³•ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨å…¶å®ƒçš„ã€‚

è¡¨ 10 - 1ï¼šğŸ‘‡


![](https://gitee.com/veal98/images/raw/master/img/20200616105416.png)

ä½ å¯ä»¥ä½¿ç”¨è‡ªå·±å‘æ˜çš„èšåˆè¿ç®—ï¼Œè¿˜å¯ä»¥è°ƒç”¨åˆ†ç»„å¯¹è±¡ä¸Šå·²ç»å®šä¹‰å¥½çš„ä»»ä½•æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œ`quantile` å¯ä»¥è®¡ç®—Seriesæˆ–DataFrameåˆ—çš„æ ·æœ¬åˆ†ä½æ•°ã€‚

è™½ç„¶quantileå¹¶æ²¡æœ‰æ˜ç¡®åœ°å®ç°äºGroupByï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªSeriesæ–¹æ³•ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯èƒ½ç”¨çš„ã€‚å®é™…ä¸Šï¼ŒGroupByä¼šé«˜æ•ˆåœ°å¯¹Seriesè¿›è¡Œåˆ‡ç‰‡ï¼Œç„¶åå¯¹å„ç‰‡è°ƒç”¨ `piece.quantile(0.9)`ï¼Œæœ€åå°†è¿™äº›ç»“æœç»„è£…æˆæœ€ç»ˆç»“æœï¼š
```python
In [51]: df
Out[51]: 
      data1     data2 key1 key2
0 -0.204708  1.393406    a  one
1  0.478943  0.092908    a  two
2 -0.519439  0.281746    b  one
3 -0.555730  0.769023    b  two
4  1.965781  1.246435    a  one

In [52]: grouped = df.groupby('key1')

In [53]: grouped['data1'].quantile(0.9)
Out[53]: 
key1
a    1.668413
b   -0.523068
Name: data1, dtype: float64
```

ğŸš© **å¦‚æœè¦ä½¿ç”¨ä½ è‡ªå·±çš„èšåˆå‡½æ•°ï¼Œåªéœ€å°†å…¶ä¼ å…¥ `aggregate` æˆ–`agg`æ–¹æ³•å³å¯**ï¼š
```python
In [54]: def peak_to_peak(arr):
   ....:     return arr.max() - arr.min()
In [55]: grouped.agg(peak_to_peak)
Out[55]: 
         data1     data2
key1                    
a     2.170488  1.300498
b     0.036292  0.487276
```

ä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œæœ‰äº›æ–¹æ³•ï¼ˆå¦‚ describeï¼‰ä¹Ÿæ˜¯å¯ä»¥ç”¨åœ¨è¿™é‡Œçš„ï¼Œå³ä½¿ä¸¥æ ¼æ¥è®²ï¼Œå®ƒä»¬å¹¶éèšåˆè¿ç®—ï¼š
```python
In [56]: grouped.describe()
Out[56]: 
     data1                                                              \
     count      mean       std       min       25%       50%       75%   
key1                                                                     
a      3.0  0.746672  1.109736 -0.204708  0.137118  0.478943  1.222362   
b      2.0 -0.537585  0.025662 -0.555730 -0.546657 -0.537585 -0.528512   
               data2                                                    \
max count      mean       std       min       25%       50%   
key1                                                                     
a     1.965781   3.0  0.910916  0.712217  0.092908  0.669671  1.246435   
b    -0.519439   2.0  0.525384  0.344556  0.281746  0.403565  0.525384   
                          
           75%       max  
key1                      
a     1.319920  1.393406  
b     0.647203  0.769023
```

åœ¨åé¢çš„10.3èŠ‚ï¼Œæˆ‘å°†è¯¦ç»†è¯´æ˜è¿™åˆ°åº•æ˜¯æ€ä¹ˆå›äº‹ã€‚

>ğŸš¨ è‡ª**å®šä¹‰èšåˆå‡½æ•°è¦æ¯”è¡¨10-1ä¸­é‚£äº›ç»è¿‡ä¼˜åŒ–çš„å‡½æ•°æ…¢å¾—å¤šã€‚è¿™æ˜¯å› ä¸ºåœ¨æ„é€ ä¸­é—´åˆ†ç»„æ•°æ®å—æ—¶å­˜åœ¨éå¸¸å¤§çš„å¼€é”€ï¼ˆå‡½æ•°è°ƒç”¨ã€æ•°æ®é‡æ’ç­‰ï¼‰**ã€‚

### 2. é¢å‘åˆ—çš„å¤šå‡½æ•°åº”ç”¨

å›åˆ°å‰é¢å°è´¹çš„ä¾‹å­ã€‚ä½¿ç”¨ `read_csv` å¯¼å…¥æ•°æ®ä¹‹åï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªå°è´¹ç™¾åˆ†æ¯”çš„åˆ— `tip_pct`ï¼š
```python
In [57]: tips = pd.read_csv('examples/tips.csv')

# Add tip percentage of total bill
In [58]: tips['tip_pct'] = tips['tip'] / tips['total_bill']

In [59]: tips[:6]
Out[59]: 
   total_bill   tip smoker  day    time  size   tip_pct
0       16.99  1.01     No  Sun  Dinner     2  0.059447
1       10.34  1.66     No  Sun  Dinner     3  0.160542
2       21.01  3.50     No  Sun  Dinner     3  0.166587
3       23.68  3.31     No  Sun  Dinner     2  0.139780
4       24.59  3.61     No  Sun  Dinner     4  0.146808
5       25.29  4.71     No  Sun  Dinner     4  0.186240
```

ä½ å·²ç»çœ‹åˆ°ï¼Œå¯¹Seriesæˆ–DataFrameåˆ—çš„èšåˆè¿ç®—å…¶å®å°±æ˜¯ä½¿ç”¨aggregateï¼ˆä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°ï¼‰æˆ–è°ƒç”¨è¯¸å¦‚meanã€stdä¹‹ç±»çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œ**ä½ å¯èƒ½å¸Œæœ›å¯¹ä¸åŒçš„åˆ—ä½¿ç”¨ä¸åŒçš„èšåˆå‡½æ•°**ï¼Œæˆ–ä¸€æ¬¡åº”ç”¨å¤šä¸ªå‡½æ•°ã€‚å…¶å®è¿™ä¹Ÿå¥½åŠï¼Œæˆ‘å°†é€šè¿‡ä¸€äº›ç¤ºä¾‹æ¥è¿›è¡Œè®²è§£ã€‚é¦–å…ˆï¼Œæˆ‘æ ¹æ®å¤©å’Œsmokerå¯¹tipsè¿›è¡Œåˆ†ç»„ï¼š

```python
In [60]: grouped = tips.groupby(['day', 'smoker'])
```

æ³¨æ„ï¼Œå¯¹äºè¡¨10-1ä¸­çš„é‚£äº›æè¿°ç»Ÿè®¡ï¼Œå¯ä»¥å°†å‡½æ•°åä»¥å­—ç¬¦ä¸²çš„å½¢å¼ä¼ å…¥ï¼š

```python
In [61]: grouped_pct = grouped['tip_pct']

In [62]: grouped_pct.agg('mean')
Out[62]: 
day   smoker
Fri   No        0.151650
      Yes       0.174783
Sat   No        0.158048
      Yes       0.147906
Sun   No        0.160113
      Yes       0.187250
Thur  No        0.160298
      Yes       0.163863
Name: tip_pct, dtype: float64
```

å¦‚æœä¼ å…¥ä¸€ç»„å‡½æ•°æˆ–å‡½æ•°åï¼Œå¾—åˆ°çš„DataFrameçš„åˆ—å°±ä¼šä»¥ç›¸åº”çš„å‡½æ•°å‘½åï¼š

```python
In [63]: grouped_pct.agg(['mean', 'std', peak_to_peak]) # peak_to_peak æ˜¯ä¸Šé¢æˆ‘ä»¬è‡ªå®šä¹‰çš„å‡½æ•°
Out[63]: 
                 mean       std  peak_to_peak
day  smoker                                  
Fri  No      0.151650  0.028123      0.067349
     Yes     0.174783  0.051293      0.159925
Sat  No      0.158048  0.039767      0.235193
     Yes     0.147906  0.061375      0.290095
Sun  No      0.160113  0.042347      0.193226
     Yes     0.187250  0.154134      0.644685
Thur No      0.160298  0.038774      0.193350
     Yes     0.163863  0.039389      0.151240
```

è¿™é‡Œï¼Œæˆ‘ä»¬ä¼ é€’äº†ä¸€ç»„èšåˆå‡½æ•°è¿›è¡Œèšåˆï¼Œç‹¬ç«‹å¯¹æ•°æ®åˆ†ç»„è¿›è¡Œè¯„ä¼°ã€‚

ä½ å¹¶éä¸€å®šè¦æ¥å—GroupByè‡ªåŠ¨ç»™å‡ºçš„é‚£äº›åˆ—åï¼Œç‰¹åˆ«æ˜¯lambdaå‡½æ•°ï¼Œå®ƒä»¬çš„åç§°æ˜¯ `'<lambda>'`ï¼Œè¿™æ ·çš„è¾¨è¯†åº¦å°±å¾ˆä½äº†ï¼ˆé€šè¿‡å‡½æ•°çš„__name__å±æ€§çœ‹çœ‹å°±çŸ¥é“äº†ï¼‰ã€‚å› æ­¤ï¼Œ**å¦‚æœä¼ å…¥çš„æ˜¯ä¸€ä¸ªç”±(name,function)å…ƒç»„ç»„æˆçš„åˆ—è¡¨ï¼Œåˆ™å„å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ å°±ä¼šè¢«ç”¨ä½œDataFrameçš„åˆ—å**ï¼ˆå¯ä»¥å°†è¿™ç§äºŒå…ƒå…ƒç»„åˆ—è¡¨çœ‹åšä¸€ä¸ªæœ‰åºæ˜ å°„ï¼‰ï¼š

```python
In [64]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])
Out[64]: 
                  foo       bar
day  smoker                    
Fri  No      0.151650  0.028123
     Yes     0.174783  0.051293
Sat  No      0.158048  0.039767
     Yes     0.147906  0.061375
Sun  No      0.160113  0.042347
     Yes     0.187250  0.154134
Thur No      0.160298  0.038774
     Yes     0.163863  0.039389
```

å¯¹äºDataFrameï¼Œä½ è¿˜æœ‰æ›´å¤šé€‰æ‹©ï¼Œä½ å¯ä»¥å®šä¹‰ä¸€ç»„åº”ç”¨äºå…¨éƒ¨åˆ—çš„ä¸€ç»„å‡½æ•°ï¼Œæˆ–**ä¸åŒçš„åˆ—åº”ç”¨ä¸åŒçš„å‡½æ•°**ã€‚å‡è®¾æˆ‘ä»¬æƒ³è¦å¯¹tip_pctå’Œtotal_billåˆ—è®¡ç®—ä¸‰ä¸ªç»Ÿè®¡ä¿¡æ¯ï¼š

```python
In [65]: functions = ['count', 'mean', 'max']

In [66]: result = grouped['tip_pct', 'total_bill'].agg(functions)

In [67]: result
Out[67]: 
            tip_pct                     total_bill                  
              count      mean       max      count       mean    max
day  smoker                                                         
Fri  No           4  0.151650  0.187735          4  18.420000  22.75
     Yes         15  0.174783  0.263480         15  16.813333  40.17
Sat  No          45  0.158048  0.291990         45  19.661778  48.33
     Yes         42  0.147906  0.325733         42  21.276667  50.81
Sun  No          57  0.160113  0.252672         57  20.506667  48.17
     Yes         19  0.187250  0.710345         19  24.120000  45.35
Thur No          45  0.160298  0.266312         45  17.113111  41.19
     Yes         17  0.163863  0.241255         17  19.190588  43.11
```

å¦‚ä½ æ‰€è§ï¼Œç»“æœDataFrameæ‹¥æœ‰å±‚æ¬¡åŒ–çš„åˆ—ï¼Œè¿™ç›¸å½“äºåˆ†åˆ«å¯¹å„åˆ—è¿›è¡Œèšåˆï¼Œç„¶åç”¨concatå°†ç»“æœç»„è£…åˆ°ä¸€èµ·ï¼Œä½¿ç”¨åˆ—åç”¨ä½œkeyså‚æ•°ï¼š

```python
In [68]: result['tip_pct']
Out[68]: 
             count      mean       max
day  smoker                           
Fri  No          4  0.151650  0.187735
     Yes        15  0.174783  0.263480
Sat  No         45  0.158048  0.291990
     Yes        42  0.147906  0.325733
Sun  No         57  0.160113  0.252672
     Yes        19  0.187250  0.710345
Thur No         45  0.160298  0.266312
     Yes        17  0.163863  0.241255
```

è·Ÿå‰é¢ä¸€æ ·ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥ä¼ å…¥å¸¦æœ‰è‡ªå®šä¹‰åç§°çš„ä¸€ç»„å…ƒç»„ï¼š

```python
In [69]: ftuples = [('Durchschnitt', 'mean'),('Abweichung', np.var)]

In [70]: grouped['tip_pct', 'total_bill'].agg(ftuples)
Out[70]: 
                 tip_pct              total_bill            
            Durchschnitt Abweichung Durchschnitt  Abweichung
day  smoker                                                 
Fri  No         0.151650   0.000791    18.420000   25.596333
     Yes        0.174783   0.002631    16.813333   82.562438
Sat  No         0.158048   0.001581    19.661778   79.908965
     Yes        0.147906   0.003767    21.276667  101.387535
Sun  No         0.160113   0.001793    20.506667   66.099980
     Yes        0.187250   0.023757    24.120000  109.046044
Thur No         0.160298   0.001503    17.113111   59.625081
     Yes        0.163863   0.001551    19.190588   69.808518
```

ç°åœ¨ï¼Œå‡è®¾ä½ æƒ³è¦å¯¹ä¸€ä¸ªåˆ—æˆ–ä¸åŒçš„åˆ—åº”ç”¨ä¸åŒçš„å‡½æ•°ã€‚å…·ä½“çš„åŠæ³•æ˜¯å‘agg**ä¼ å…¥ä¸€ä¸ªä»åˆ—åæ˜ å°„åˆ°å‡½æ•°çš„<u>å­—å…¸</u>**ï¼š

```python
In [71]: grouped.agg({'tip' : np.max, 'size' : 'sum'})
Out[71]: 
               tip  size
day  smoker             
Fri  No       3.50     9
     Yes      4.73    31
Sat  No       9.00   115
     Yes     10.00   104
Sun  No       6.00   167
     Yes      6.50    49
Thur No       6.70   112
     Yes      5.00    40

In [72]: grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],
   ....:              'size' : 'sum'})
Out[72]: 
              tip_pct                               size
                  min       max      mean       std  sum
day  smoker                                             
Fri  No      0.120385  0.187735  0.151650  0.028123    9
     Yes     0.103555  0.263480  0.174783  0.051293   31
Sat  No      0.056797  0.291990  0.158048  0.039767  115
     Yes     0.035638  0.325733  0.147906  0.061375  104
Sun  No      0.059447  0.252672  0.160113  0.042347  167
     Yes     0.065660  0.710345  0.187250  0.154134   49
Thur No      0.072961  0.266312  0.160298  0.038774  112
     Yes     0.090014  0.241255  0.163863  0.039389   40
```

åªæœ‰å°†å¤šä¸ªå‡½æ•°åº”ç”¨åˆ°è‡³å°‘ä¸€åˆ—æ—¶ï¼ŒDataFrameæ‰ä¼šæ‹¥æœ‰å±‚æ¬¡åŒ–çš„åˆ—ã€‚

### 3. ä»¥â€œæ²¡æœ‰è¡Œç´¢å¼•â€çš„å½¢å¼è¿”å›èšåˆæ•°æ®

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰ç¤ºä¾‹ä¸­çš„èšåˆæ•°æ®éƒ½æœ‰ç”±å”¯ä¸€çš„åˆ†ç»„é”®ç»„æˆçš„ç´¢å¼•ï¼ˆå¯èƒ½è¿˜æ˜¯å±‚æ¬¡åŒ–çš„ï¼‰ã€‚ç”±äºå¹¶ä¸æ€»æ˜¯éœ€è¦å¦‚æ­¤ï¼Œæ‰€ä»¥ä½ å¯ä»¥å‘groupbyä¼ å…¥ `as_index=False` ä»¥ç¦ç”¨è¯¥åŠŸèƒ½ï¼š

```python
In [73]: tips.groupby(['day', 'smoker'], as_index=False).mean()
Out[73]: 
    day smoker  total_bill       tip      size   tip_pct
0   Fri     No   18.420000  2.812500  2.250000  0.151650
1   Fri    Yes   16.813333  2.714000  2.066667  0.174783
2   Sat     No   19.661778  3.102889  2.555556  0.158048
3   Sat    Yes   21.276667  2.875476  2.476190  0.147906
4   Sun     No   20.506667  3.167895  2.929825  0.160113
5   Sun    Yes   24.120000  3.516842  2.578947  0.187250
6  Thur     No   17.113111  2.673778  2.488889  0.160298
7  Thur    Yes   19.190588  3.030000  2.352941  0.163863
```

å½“ç„¶ï¼Œå¯¹ç»“æœè°ƒç”¨reset_indexä¹Ÿèƒ½å¾—åˆ°è¿™ç§å½¢å¼çš„ç»“æœã€‚ä½¿ç”¨as_index=Falseæ–¹æ³•å¯ä»¥é¿å…ä¸€äº›ä¸å¿…è¦çš„è®¡ç®—ã€‚

## 10.3 applyï¼šä¸€èˆ¬æ€§çš„â€œæ‹†åˆ†ï¼åº”ç”¨ï¼åˆå¹¶â€

### 1. apply åŸºæœ¬ç”¨æ³•

æœ€é€šç”¨çš„GroupByæ–¹æ³•æ˜¯ `apply`ï¼Œæœ¬èŠ‚å‰©ä½™éƒ¨åˆ†å°†é‡ç‚¹è®²è§£å®ƒã€‚å¦‚å›¾æ‰€ç¤ºï¼Œ**apply ä¼šå°†å¾…å¤„ç†çš„å¯¹è±¡æ‹†åˆ†æˆå¤šä¸ªç‰‡æ®µï¼Œç„¶åå¯¹å„ç‰‡æ®µè°ƒç”¨ä¼ å…¥çš„å‡½æ•°ï¼Œæœ€åå°è¯•å°†å„ç‰‡æ®µç»„åˆåˆ°ä¸€èµ·**ã€‚

<img src="https://gitee.com/veal98/images/raw/master/img/20200616111517.png" style="zoom:50%;" />

å›åˆ°ä¹‹å‰é‚£ä¸ªå°è´¹æ•°æ®é›†ï¼Œå‡è®¾ä½ æƒ³è¦æ ¹æ®åˆ†ç»„é€‰å‡ºæœ€é«˜çš„ 5 ä¸ª tip_pct å€¼ã€‚é¦–å…ˆï¼Œç¼–å†™ä¸€ä¸ªé€‰å–æŒ‡å®šåˆ—å…·æœ‰æœ€å¤§å€¼çš„è¡Œçš„å‡½æ•°ï¼š
```python
In [74]: def top(df, n=5, column='tip_pct'):
   ....:     return df.sort_values(by=column)[-n:]

In [75]: top(tips, n=6)
Out[75]: 
     total_bill   tip smoker  day    time  size   tip_pct
109       14.31  4.00    Yes  Sat  Dinner     2  0.279525
183       23.17  6.50    Yes  Sun  Dinner     4  0.280535
232       11.61  3.39     No  Sat  Dinner     2  0.291990
67         3.07  1.00    Yes  Sat  Dinner     1  0.325733
178        9.60  4.00    Yes  Sun  Dinner     2  0.416667
172        7.25  5.15    Yes  Sun  Dinner     2  0.710345
```

ç°åœ¨ï¼Œå¦‚æœå¯¹smokeråˆ†ç»„å¹¶ç”¨è¯¥å‡½æ•°è°ƒç”¨applyï¼Œå°±ä¼šå¾—åˆ°ï¼š
```python
In [76]: tips.groupby('smoker').apply(top)
Out[76]: 
            total_bill   tip smoker   day    time  size   tip_pct
smoker                                                           
No     88        24.71  5.85     No  Thur   Lunch     2  0.236746
       185       20.69  5.00     No   Sun  Dinner     5  0.241663
       51        10.29  2.60     No   Sun  Dinner     2  0.252672
       149        7.51  2.00     No  Thur   Lunch     2  0.266312
       232       11.61  3.39     No   Sat  Dinner     2  0.291990
Yes    109       14.31  4.00    Yes   Sat  Dinner     2  0.279525
       183       23.17  6.50    Yes   Sun  Dinner     4  0.280535
       67         3.07  1.00    Yes   Sat  Dinner     1  0.325733
       178        9.60  4.00    Yes   Sun  Dinner     2  0.416667
       172        7.25  5.15    Yes   Sun  Dinner     2  0.710345
```

è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿtopå‡½æ•°åœ¨DataFrameçš„å„ä¸ªç‰‡æ®µä¸Šè°ƒç”¨ï¼Œç„¶åç»“æœç”±pandas.concatç»„è£…åˆ°ä¸€èµ·ï¼Œå¹¶ä»¥åˆ†ç»„åç§°è¿›è¡Œäº†æ ‡è®°ã€‚äºæ˜¯ï¼Œæœ€ç»ˆç»“æœå°±æœ‰äº†ä¸€ä¸ªå±‚æ¬¡åŒ–ç´¢å¼•ï¼Œå…¶å†…å±‚ç´¢å¼•å€¼æ¥è‡ªåŸDataFrameã€‚

å¦‚æœä¼ ç»™applyçš„å‡½æ•°èƒ½å¤Ÿæ¥å—å…¶ä»–å‚æ•°æˆ–å…³é”®å­—ï¼Œåˆ™å¯ä»¥å°†è¿™äº›å†…å®¹æ”¾åœ¨å‡½æ•°ååé¢ä¸€å¹¶ä¼ å…¥ï¼š
```python
In [77]: tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')
Out[77]: 
                 total_bill    tip smoker   day    time  size   tip_pct
smoker day                                                             
No     Fri  94        22.75   3.25     No   Fri  Dinner     2  0.142857
       Sat  212       48.33   9.00     No   Sat  Dinner     4  0.186220
       Sun  156       48.17   5.00     No   Sun  Dinner     6  0.103799
       Thur 142       41.19   5.00     No  Thur   Lunch     5  0.121389
Yes    Fri  95        40.17   4.73    Yes   Fri  Dinner     4  0.117750
       Sat  170       50.81  10.00    Yes   Sat  Dinner     3  0.196812
       Sun  182       45.35   3.50    Yes   Sun  Dinner     3  0.077178
       Thur 197       43.11   5.00    Yes  Thur   Lunch     4  0.115982
```

>ğŸš© é™¤è¿™äº›åŸºæœ¬ç”¨æ³•ä¹‹å¤–ï¼Œèƒ½å¦å……åˆ†å‘æŒ¥ apply çš„å¨åŠ›å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºä½ çš„åˆ›é€ åŠ›ã€‚ä¼ å…¥çš„é‚£ä¸ªå‡½æ•°èƒ½åšä»€ä¹ˆå…¨ç”±ä½ è¯´äº†ç®—ï¼Œå®ƒåªéœ€è¿”å›ä¸€ä¸ª pandas å¯¹è±¡æˆ–æ ‡é‡å€¼å³å¯ã€‚æœ¬ç« åç»­éƒ¨åˆ†çš„ç¤ºä¾‹ä¸»è¦ç”¨äºè®²è§£å¦‚ä½•åˆ©ç”¨ groupby è§£å†³å„ç§å„æ ·çš„é—®é¢˜ã€‚

å¯èƒ½ä½ å·²ç»æƒ³èµ·æ¥äº†ï¼Œä¹‹å‰æˆ‘åœ¨ GroupBy å¯¹è±¡ä¸Šè°ƒç”¨è¿‡ describeï¼š
```python
In [78]: result = tips.groupby('smoker')['tip_pct'].describe()

In [79]: result
Out[79]: 
        count      mean       std       min       25%       50%       75%  \
smoker                                                                      
No      151.0  0.159328  0.039910  0.056797  0.136906  0.155625  0.185014   
Yes      93.0  0.163196  0.085119  0.035638  0.106771  0.153846  0.195059   
             max  
smoker

No      0.291990  
Yes     0.710345  

In [80]: result.unstack('smoker')
Out[80]: 
       smoker
count  No        151.000000
       Yes        93.000000
mean   No          0.159328
       Yes         0.163196
std    No          0.039910
       Yes         0.085119
min    No          0.056797
       Yes         0.035638
25%    No          0.136906
       Yes         0.106771
50%    No          0.155625
       Yes         0.153846
75%    No          0.185014
       Yes         0.195059
max    No          0.291990
       Yes         0.710345
dtype: float64
```

åœ¨GroupByä¸­ï¼Œå½“ä½ è°ƒç”¨è¯¸å¦‚describeä¹‹ç±»çš„æ–¹æ³•æ—¶ï¼Œå®é™…ä¸Šåªæ˜¯åº”ç”¨äº†ä¸‹é¢ä¸¤æ¡ä»£ç çš„å¿«æ·æ–¹å¼è€Œå·²ï¼š
```python
f = lambda x: x.describe()
grouped.apply(f)
```

### 2. ç¦æ­¢åˆ†ç»„é”®

ä»ä¸Šé¢çš„ä¾‹å­ä¸­å¯ä»¥çœ‹å‡ºï¼Œåˆ†ç»„é”®ä¼šè·ŸåŸå§‹å¯¹è±¡çš„ç´¢å¼•å…±åŒæ„æˆç»“æœå¯¹è±¡ä¸­çš„å±‚æ¬¡åŒ–ç´¢å¼•ã€‚å°†`group_keys=False` ä¼ å…¥`groupby`å³å¯ç¦æ­¢è¯¥æ•ˆæœï¼š
```python
In [81]: tips.groupby('smoker', group_keys=False).apply(top)
Out[81]: 
     total_bill   tip smoker   day    time  size   tip_pct
88        24.71  5.85     No  Thur   Lunch     2  0.236746
185       20.69  5.00     No   Sun  Dinner     5  0.241663
51        10.29  2.60     No   Sun  Dinner     2  0.252672
149        7.51  2.00     No  Thur   Lunch     2  0.266312
232       11.61  3.39     No   Sat  Dinner     2  0.291990
109       14.31  4.00    Yes   Sat  Dinner     2  0.279525
183       23.17  6.50    Yes   Sun  Dinner     4  0.280535
67         3.07  1.00    Yes   Sat  Dinner     1  0.325733
178        9.60  4.00    Yes   Sun  Dinner     2  0.416667
172        7.25  5.15    Yes   Sun  Dinner     2  0.710345
```

![](https://gitee.com/veal98/images/raw/master/img/20200616112652.png)

![](https://gitee.com/veal98/images/raw/master/img/20200616112706.png)

### 3. åˆ†ä½æ•°å’Œæ¡¶åˆ†æ

æˆ‘æ›¾åœ¨ç¬¬8ç« ä¸­è®²è¿‡ï¼Œpandasæœ‰ä¸€äº›èƒ½æ ¹æ®æŒ‡å®šé¢å…ƒæˆ–æ ·æœ¬åˆ†ä½æ•°å°†æ•°æ®æ‹†åˆ†æˆå¤šå—çš„å·¥å…·ï¼ˆæ¯”å¦‚cutå’Œqcutï¼‰ã€‚å°†è¿™äº›å‡½æ•°è·Ÿgroupbyç»“åˆèµ·æ¥ï¼Œå°±èƒ½éå¸¸è½»æ¾åœ°å®ç°å¯¹æ•°æ®é›†çš„æ¡¶ï¼ˆbucketï¼‰æˆ–åˆ†ä½æ•°ï¼ˆquantileï¼‰åˆ†æäº†ã€‚ä»¥ä¸‹é¢è¿™ä¸ªç®€å•çš„éšæœºæ•°æ®é›†ä¸ºä¾‹ï¼Œæˆ‘ä»¬**åˆ©ç”¨cutå°†å…¶è£…å…¥é•¿åº¦ç›¸ç­‰çš„æ¡¶ä¸­**ï¼š
```python
In [82]: frame = pd.DataFrame({'data1': np.random.randn(1000),
   ....:                       'data2': np.random.randn(1000)})

In [83]: quartiles = pd.cut(frame.data1, 4) # åˆ†æˆé•¿åº¦ç›¸ç­‰çš„å››ä¸ªåŒºé—´

In [84]: quartiles[:10]
Out[84]: 
0     (-1.23, 0.489]
1    (-2.956, -1.23]
2     (-1.23, 0.489]
3     (0.489, 2.208]
4     (-1.23, 0.489]
5     (0.489, 2.208]
6     (-1.23, 0.489]
7     (-1.23, 0.489]
8     (0.489, 2.208]
9     (0.489, 2.208]
Name: data1, dtype: category
Categories (4, interval[float64]): [(-2.956, -1.23] < (-1.23, 0.489] < (0.489, 2.
208] < (2.208, 3.928]]
```

ç”±cutè¿”å›çš„Categoricalå¯¹è±¡å¯ç›´æ¥ä¼ é€’åˆ°groupbyã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åƒä¸‹é¢è¿™æ ·å¯¹data2åˆ—åšä¸€äº›ç»Ÿè®¡è®¡ç®—ï¼š
```python
In [85]: def get_stats(group):
   ....:     return {'min': group.min(), 'max': group.max(),
   ....:             'count': group.count(), 'mean': group.mean()}

In [86]: grouped = frame.data2.groupby(quartiles)

In [87]: grouped.apply(get_stats).unstack()
Out[87]: 
                 count       max      mean       min
data1                                               
(-2.956, -1.23]   95.0  1.670835 -0.039521 -3.399312
(-1.23, 0.489]   598.0  3.260383 -0.002051 -2.989741
(0.489, 2.208]   297.0  2.954439  0.081822 -3.745356
(2.208, 3.928]    10.0  1.765640  0.024750 -1.929776
```

**è¿™äº›éƒ½æ˜¯é•¿åº¦ç›¸ç­‰çš„æ¡¶ã€‚è¦æ ¹æ®æ ·æœ¬åˆ†ä½æ•°å¾—åˆ°å¤§å°ç›¸ç­‰çš„æ¡¶ï¼Œä½¿ç”¨qcutå³å¯**ã€‚**ä¼ å…¥ `labels=False` å³å¯åªè·å–åˆ†ä½æ•°çš„ç¼–å·ï¼š**

```python
# Return quantile numbers
In [88]: grouping = pd.qcut(frame.data1, 10, labels=False)

In [89]: grouped = frame.data2.groupby(grouping)

In [90]: grouped.apply(get_stats).unstack()
Out[90]: 
       count       max      mean       min
data1                                     
0      100.0  1.670835 -0.049902 -3.399312
1      100.0  2.628441  0.030989 -1.950098
2      100.0  2.527939 -0.067179 -2.925113
3      100.0  3.260383  0.065713 -2.315555
4      100.0  2.074345 -0.111653 -2.047939
5      100.0  2.184810  0.052130 -2.989741
6      100.0  2.458842 -0.021489 -2.223506
7      100.0  2.954439 -0.026459 -3.056990
8      100.0  2.735527  0.103406 -3.745356
9      100.0  2.377020  0.220122 -2.064111
```

æˆ‘ä»¬ä¼šåœ¨ç¬¬12ç« è¯¦ç»†è®²è§£pandasçš„ `Categorical` ç±»å‹ã€‚

### 4. ç¤ºä¾‹ï¼šç”¨ç‰¹å®šäºåˆ†ç»„çš„å€¼å¡«å……ç¼ºå¤±å€¼

å¯¹äºç¼ºå¤±æ•°æ®çš„æ¸…ç†å·¥ä½œï¼Œæœ‰æ—¶ä½ ä¼šç”¨dropnaå°†å…¶æ›¿æ¢æ‰ï¼Œè€Œæœ‰æ—¶åˆ™å¯èƒ½ä¼šå¸Œæœ›ç”¨ä¸€ä¸ªå›ºå®šå€¼æˆ–ç”±æ•°æ®é›†æœ¬èº«æ‰€è¡ç”Ÿå‡ºæ¥çš„å€¼å»å¡«å……NAå€¼ã€‚è¿™æ—¶å°±å¾—ä½¿ç”¨fillnaè¿™ä¸ªå·¥å…·äº†ã€‚åœ¨ä¸‹é¢è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ç”¨å¹³å‡å€¼å»å¡«å……NAå€¼ï¼š
```python
In [91]: s = pd.Series(np.random.randn(6))

In [92]: s[::2] = np.nan

In [93]: s
Out[93]: 
0         NaN
1   -0.125921
2         NaN
3   -0.884475
4         NaN
5    0.227290
dtype: float64

In [94]: s.fillna(s.mean())
Out[94]: 
0   -0.261035
1   -0.125921
2   -0.261035
3   -0.884475
4   -0.261035
5    0.227290
dtype: float64
```

å‡è®¾ä½ éœ€è¦å¯¹ä¸åŒçš„åˆ†ç»„å¡«å……ä¸åŒçš„å€¼ã€‚ä¸€ç§æ–¹æ³•æ˜¯å°†æ•°æ®åˆ†ç»„ï¼Œå¹¶ä½¿ç”¨applyå’Œä¸€ä¸ªèƒ½å¤Ÿå¯¹å„æ•°æ®å—è°ƒç”¨fillnaçš„å‡½æ•°å³å¯ã€‚ä¸‹é¢æ˜¯ä¸€äº›æœ‰å…³ç¾å›½å‡ ä¸ªå·çš„ç¤ºä¾‹æ•°æ®ï¼Œè¿™äº›å·åˆè¢«åˆ†ä¸ºä¸œéƒ¨å’Œè¥¿éƒ¨ï¼š
```python
In [95]: states = ['Ohio', 'New York', 'Vermont', 'Florida',
   ....:           'Oregon', 'Nevada', 'California', 'Idaho']

In [96]: group_key = ['East'] * 4 + ['West'] * 4

In [97]: data = pd.Series(np.random.randn(8), index=states)

In [98]: data
Out[98]: 
Ohio          0.922264
New York     -2.153545
Vermont      -0.365757
Florida      -0.375842
Oregon        0.329939
Nevada        0.981994
California    1.105913
Idaho        -1.613716
dtype: float64
```

['East'] * 4äº§ç”Ÿäº†ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…æ‹¬äº†['East']ä¸­å…ƒç´ çš„å››ä¸ªæ‹·è´ã€‚å°†è¿™äº›åˆ—è¡¨ä¸²è”èµ·æ¥ã€‚

å°†ä¸€äº›å€¼è®¾ä¸ºç¼ºå¤±ï¼š
```python
In [99]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan

In [100]: data
Out[100]: 
Ohio          0.922264
New York     -2.153545
Vermont            NaN
Florida      -0.375842
Oregon        0.329939
Nevada             NaN
California    1.105913
Idaho              NaN
dtype: float64

In [101]: data.groupby(group_key).mean()
Out[101]: 
East   -0.535707
West    0.717926
dtype: float64
```

æˆ‘ä»¬å¯ä»¥ç”¨åˆ†ç»„å¹³å‡å€¼å»å¡«å……NAå€¼:
```python
In [102]: fill_mean = lambda g: g.fillna(g.mean())

In [103]: data.groupby(group_key).apply(fill_mean)
Out[103]: 
Ohio          0.922264
New York     -2.153545
Vermont      -0.535707
Florida      -0.375842
Oregon        0.329939
Nevada        0.717926
California    1.105913
Idaho         0.717926
dtype: float64
```

å¦å¤–ï¼Œä¹Ÿå¯ä»¥åœ¨ä»£ç ä¸­é¢„å®šä¹‰å„ç»„çš„å¡«å……å€¼ã€‚ç”±äºåˆ†ç»„å…·æœ‰ä¸€ä¸ªnameå±æ€§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ‹¿æ¥ç”¨ä¸€ä¸‹ï¼š
```python
In [104]: fill_values = {'East': 0.5, 'West': -1}

In [105]: fill_func = lambda g: g.fillna(fill_values[g.name])

In [106]: data.groupby(group_key).apply(fill_func)
Out[106]: 
Ohio          0.922264
New York     -2.153545
Vermont       0.500000
Florida      -0.375842
Oregon        0.329939
Nevada       -1.000000
California    1.105913
Idaho        -1.000000
dtype: float64
```

### 5. ç¤ºä¾‹ï¼šéšæœºé‡‡æ ·å’Œæ’åˆ—

å‡è®¾ä½ æƒ³è¦ä»ä¸€ä¸ªå¤§æ•°æ®é›†ä¸­éšæœºæŠ½å–ï¼ˆè¿›è¡Œæ›¿æ¢æˆ–ä¸æ›¿æ¢ï¼‰æ ·æœ¬ä»¥è¿›è¡Œè’™ç‰¹å¡ç½—æ¨¡æ‹Ÿï¼ˆMonte Carlo simulationï¼‰æˆ–å…¶ä»–åˆ†æå·¥ä½œã€‚â€œæŠ½å–â€çš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œè¿™é‡Œä½¿ç”¨çš„æ–¹æ³•æ˜¯å¯¹Seriesä½¿ç”¨sampleæ–¹æ³•ï¼š
```python
# Hearts, Spades, Clubs, Diamonds
suits = ['H', 'S', 'C', 'D']
card_val = (list(range(1, 11)) + [10] * 3) * 4
base_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']
cards = []
for suit in ['H', 'S', 'C', 'D']:
    cards.extend(str(num) + suit for num in base_names)

deck = pd.Series(card_val, index=cards)
```

ç°åœ¨æˆ‘æœ‰äº†ä¸€ä¸ªé•¿åº¦ä¸º52çš„Seriesï¼Œå…¶ç´¢å¼•åŒ…æ‹¬ç‰Œåï¼Œå€¼åˆ™æ˜¯21ç‚¹æˆ–å…¶ä»–æ¸¸æˆä¸­ç”¨äºè®¡åˆ†çš„ç‚¹æ•°ï¼ˆä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å½“Açš„ç‚¹æ•°ä¸º1ï¼‰ï¼š
```python
In [108]: deck[:13]
Out[108]: 
AH      1
2H      2
3H      3
4H      4
5H      5
6H      6
7H      7
8H      8
9H      9
10H    10
JH     10
KH     10
QH     10
dtype: int64
```

ç°åœ¨ï¼Œæ ¹æ®æˆ‘ä¸Šé¢æ‰€è®²çš„ï¼Œä»æ•´å‰¯ç‰Œä¸­æŠ½å‡º5å¼ ï¼Œä»£ç å¦‚ä¸‹ï¼š
```python
In [109]: def draw(deck, n=5):
   .....:     return deck.sample(n)

In [110]: draw(deck)
Out[110]: 
AD     1
8C     8
5H     5
KC    10
2C     2
dtype: int64
```

å‡è®¾ä½ æƒ³è¦ä»æ¯ç§èŠ±è‰²ä¸­éšæœºæŠ½å–ä¸¤å¼ ç‰Œã€‚ç”±äºèŠ±è‰²æ˜¯ç‰Œåçš„æœ€åä¸€ä¸ªå­—ç¬¦ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ®æ­¤è¿›è¡Œåˆ†ç»„ï¼Œå¹¶ä½¿ç”¨applyï¼š
```python
In [111]: get_suit = lambda card: card[-1] # last letter is suit

In [112]: deck.groupby(get_suit).apply(draw, n=2)
Out[112]: 
C  2C     2
   3C     3
D  KD    10
   8D     8
H  KH    10
   3H     3
S  2S     2
   4S     4
dtype: int64
```

æˆ–è€…ï¼Œä¹Ÿå¯ä»¥è¿™æ ·å†™ï¼š
```python
In [113]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)
Out[113]: 
KC    10
JC    10
AD     1
5D     5
5H     5
6H     6
7S     7
KS    10
dtype: int64
```

### 6. ç¤ºä¾‹ï¼šåˆ†ç»„åŠ æƒå¹³å‡æ•°å’Œç›¸å…³ç³»æ•°

æ ¹æ®groupbyçš„â€œæ‹†åˆ†ï¼åº”ç”¨ï¼åˆå¹¶â€èŒƒå¼ï¼Œå¯ä»¥è¿›è¡ŒDataFrameçš„åˆ—ä¸åˆ—ä¹‹é—´æˆ–ä¸¤ä¸ªSeriesä¹‹é—´çš„è¿ç®—ï¼ˆæ¯”å¦‚åˆ†ç»„åŠ æƒå¹³å‡ï¼‰ã€‚ä»¥ä¸‹é¢è¿™ä¸ªæ•°æ®é›†ä¸ºä¾‹ï¼Œå®ƒå«æœ‰åˆ†ç»„é”®ã€å€¼ä»¥åŠä¸€äº›æƒé‡å€¼ï¼š
```python
In [114]: df = pd.DataFrame({'category': ['a', 'a', 'a', 'a',
   .....:                                 'b', 'b', 'b', 'b'],
   .....:                    'data': np.random.randn(8),
   .....:                    'weights': np.random.rand(8)})

In [115]: df
Out[115]: 
  category      data   weights
0        a  1.561587  0.957515
1        a  1.219984  0.347267
2        a -0.482239  0.581362
3        a  0.315667  0.217091
4        b -0.047852  0.894406
5        b -0.454145  0.918564
6        b -0.556774  0.277825
7        b  0.253321  0.955905
```

ç„¶åå¯ä»¥åˆ©ç”¨categoryè®¡ç®—åˆ†ç»„åŠ æƒå¹³å‡æ•°ï¼š
```python
In [116]: grouped = df.groupby('category')

In [117]: get_wavg = lambda g: np.average(g['data'], weights=g['weights'])

In [118]: grouped.apply(get_wavg)
Out[118]:
category
a    0.811643
b   -0.122262
dtype: float64
```

å¦ä¸€ä¸ªä¾‹å­ï¼Œè€ƒè™‘ä¸€ä¸ªæ¥è‡ªYahoo!Financeçš„æ•°æ®é›†ï¼Œå…¶ä¸­å«æœ‰å‡ åªè‚¡ç¥¨å’Œæ ‡å‡†æ™®å°”500æŒ‡æ•°ï¼ˆç¬¦å·SPXï¼‰çš„æ”¶ç›˜ä»·ï¼š
```python
In [119]: close_px = pd.read_csv('examples/stock_px_2.csv', parse_dates=True,
   .....:                        index_col=0)

In [120]: close_px.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2214 entries, 2003-01-02 to 2011-10-14
Data columns (total 4 columns):
AAPL    2214 non-null float64
MSFT    2214 non-null float64
XOM     2214 non-null float64
SPX     2214 non-null float64
dtypes: float64(4)
memory usage: 86.5 KB

In [121]: close_px[-4:]
Out[121]: 
              AAPL   MSFT    XOM      SPX
2011-10-11  400.29  27.00  76.27  1195.54
2011-10-12  402.19  26.96  77.16  1207.25
2011-10-13  408.43  27.18  76.37  1203.66
2011-10-14  422.00  27.27  78.11  1224.58
```

æ¥åšä¸€ä¸ªæ¯”è¾ƒæœ‰è¶£çš„ä»»åŠ¡ï¼šè®¡ç®—ä¸€ä¸ªç”±æ—¥æ”¶ç›Šç‡ï¼ˆé€šè¿‡ç™¾åˆ†æ•°å˜åŒ–è®¡ç®—ï¼‰ä¸SPXä¹‹é—´çš„å¹´åº¦ç›¸å…³ç³»æ•°ç»„æˆçš„DataFrameã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå®ç°åŠæ³•ï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œç”¨å®ƒè®¡ç®—æ¯åˆ—å’ŒSPXåˆ—çš„æˆå¯¹ç›¸å…³ç³»æ•°ï¼š
```python
In [122]: spx_corr = lambda x: x.corrwith(x['SPX'])
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨pct_changeè®¡ç®—close_pxçš„ç™¾åˆ†æ¯”å˜åŒ–ï¼š
```python
In [123]: rets = close_px.pct_change().dropna()
```

æœ€åï¼Œæˆ‘ä»¬ç”¨å¹´å¯¹ç™¾åˆ†æ¯”å˜åŒ–è¿›è¡Œåˆ†ç»„ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªä¸€è¡Œçš„å‡½æ•°ï¼Œä»æ¯è¡Œçš„æ ‡ç­¾è¿”å›æ¯ä¸ªdatetimeæ ‡ç­¾çš„yearå±æ€§ï¼š
```python
In [124]: get_year = lambda x: x.year

In [125]: by_year = rets.groupby(get_year)

In [126]: by_year.apply(spx_corr)
Out[126]: 
          AAPL      MSFT       XOM  SPX
2003  0.541124  0.745174  0.661265  1.0
2004  0.374283  0.588531  0.557742  1.0
2005  0.467540  0.562374  0.631010  1.0
2006  0.428267  0.406126  0.518514  1.0
2007  0.508118  0.658770  0.786264  1.0
2008  0.681434  0.804626  0.828303  1.0
2009  0.707103  0.654902  0.797921  1.0
2010  0.710105  0.730118  0.839057  1.0
2011  0.691931  0.800996  0.859975  1.0
```

å½“ç„¶ï¼Œä½ è¿˜å¯ä»¥è®¡ç®—åˆ—ä¸åˆ—ä¹‹é—´çš„ç›¸å…³ç³»æ•°ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—Appleå’ŒMicrosoftçš„å¹´ç›¸å…³ç³»æ•°ï¼š
```python
In [127]: by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))
Out[127]: 
2003    0.480868
2004    0.259024
2005    0.300093
2006    0.161735
2007    0.417738
2008    0.611901
2009    0.432738
2010    0.571946
2011    0.581987
dtype: float64
```

### 7. ç¤ºä¾‹ï¼šç»„çº§åˆ«çš„çº¿æ€§å›å½’

é¡ºç€ä¸Šä¸€ä¸ªä¾‹å­ç»§ç»­ï¼Œä½ å¯ä»¥ç”¨groupbyæ‰§è¡Œæ›´ä¸ºå¤æ‚çš„åˆ†ç»„ç»Ÿè®¡åˆ†æï¼Œåªè¦å‡½æ•°è¿”å›çš„æ˜¯pandaså¯¹è±¡æˆ–æ ‡é‡å€¼å³å¯ã€‚ä¾‹å¦‚ï¼Œæˆ‘å¯ä»¥å®šä¹‰ä¸‹é¢è¿™ä¸ªregresså‡½æ•°ï¼ˆåˆ©ç”¨statsmodelsè®¡é‡ç»æµå­¦åº“ï¼‰å¯¹å„æ•°æ®å—æ‰§è¡Œæ™®é€šæœ€å°äºŒä¹˜æ³•ï¼ˆOrdinary Least Squaresï¼ŒOLSï¼‰å›å½’ï¼š
```python
import statsmodels.api as sm
def regress(data, yvar, xvars):
    Y = data[yvar]
    X = data[xvars]
    X['intercept'] = 1.
    result = sm.OLS(Y, X).fit()
    return result.params
```

ç°åœ¨ï¼Œä¸ºäº†æŒ‰å¹´è®¡ç®—AAPLå¯¹SPXæ”¶ç›Šç‡çš„çº¿æ€§å›å½’ï¼Œæ‰§è¡Œï¼š
```python
In [129]: by_year.apply(regress, 'AAPL', ['SPX'])
Out[129]: 
           SPX  intercept
2003  1.195406   0.000710
2004  1.363463   0.004201
2005  1.766415   0.003246
2006  1.645496   0.000080
2007  1.198761   0.003438
2008  0.968016  -0.001110
2009  0.879103   0.002954
2010  1.052608   0.001261
2011  0.806605   0.001514
```

## 10.4 é€è§†è¡¨å’Œäº¤å‰è¡¨

### 1. é€è§†è¡¨ pivot_table

ğŸ”´ **é€è§†è¡¨ï¼ˆpivot tableï¼‰**æ˜¯å„ç§ç”µå­è¡¨æ ¼ç¨‹åºå’Œå…¶ä»–æ•°æ®åˆ†æè½¯ä»¶ä¸­ä¸€ç§å¸¸è§çš„æ•°æ®æ±‡æ€»å·¥å…·ã€‚**å®ƒæ ¹æ®ä¸€ä¸ªæˆ–å¤šä¸ªé”®å¯¹æ•°æ®è¿›è¡Œèšåˆï¼Œå¹¶æ ¹æ®è¡Œå’Œåˆ—ä¸Šçš„åˆ†ç»„é”®å°†æ•°æ®åˆ†é…åˆ°å„ä¸ªçŸ©å½¢åŒºåŸŸä¸­**ã€‚åœ¨Pythonå’Œpandasä¸­ï¼Œå¯ä»¥é€šè¿‡æœ¬ç« æ‰€ä»‹ç»çš„groupbyåŠŸèƒ½ä»¥åŠï¼ˆèƒ½å¤Ÿåˆ©ç”¨å±‚æ¬¡åŒ–ç´¢å¼•çš„ï¼‰é‡å¡‘è¿ç®—åˆ¶ä½œé€è§†è¡¨ã€‚**DataFrameæœ‰ä¸€ä¸ª `pivot_table` æ–¹æ³•ï¼Œæ­¤å¤–è¿˜æœ‰ä¸€ä¸ªé¡¶çº§çš„ `pandas.pivot_table` å‡½æ•°**ã€‚é™¤èƒ½ä¸ºgroupbyæä¾›ä¾¿åˆ©ä¹‹å¤–ï¼Œpivot_tableè¿˜å¯ä»¥æ·»åŠ åˆ†é¡¹å°è®¡ï¼Œä¹Ÿå«åšmarginsã€‚

å›åˆ°å°è´¹æ•°æ®é›†ï¼Œå‡è®¾æˆ‘æƒ³è¦æ ¹æ®dayå’Œsmokerè®¡ç®—åˆ†ç»„å¹³å‡æ•°ï¼ˆpivot_tableçš„é»˜è®¤èšåˆç±»å‹ï¼‰ï¼Œå¹¶å°†dayå’Œsmokeræ”¾åˆ°è¡Œä¸Šï¼š
```python
In [130]: tips.pivot_table(index=['day', 'smoker'])
Out[130]: 
                 size       tip   tip_pct  total_bill
day  smoker                                          
Fri  No      2.250000  2.812500  0.151650   18.420000
     Yes     2.066667  2.714000  0.174783   16.813333
Sat  No      2.555556  3.102889  0.158048   19.661778
     Yes     2.476190  2.875476  0.147906   21.276667
Sun  No      2.929825  3.167895  0.160113   20.506667
     Yes     2.578947  3.516842  0.187250   24.120000
Thur No      2.488889  2.673778  0.160298   17.113111
     Yes     2.352941  3.030000  0.163863   19.190588
```

å¯ä»¥ç”¨groupbyç›´æ¥æ¥åšã€‚ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬åªæƒ³èšåˆtip_pctå’Œsizeï¼Œè€Œä¸”æƒ³æ ¹æ®timeè¿›è¡Œåˆ†ç»„ã€‚æˆ‘å°†smokeræ”¾åˆ°åˆ—ä¸Šï¼ŒæŠŠdayæ”¾åˆ°è¡Œä¸Šï¼š
```python
In [131]: tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'],
   .....:                  columns='smoker')
Out[131]: 
                 size             tip_pct          
smoker             No       Yes        No       Yes
time   day                                         
Dinner Fri   2.000000  2.222222  0.139622  0.165347
       Sat   2.555556  2.476190  0.158048  0.147906
       Sun   2.929825  2.578947  0.160113  0.187250
       Thur  2.000000       NaN  0.159744       NaN
Lunch  Fri   3.000000  1.833333  0.187735  0.188937
       Thur  2.500000  2.352941  0.160311  0.163863
```

è¿˜å¯ä»¥å¯¹è¿™ä¸ªè¡¨ä½œè¿›ä¸€æ­¥çš„å¤„ç†ï¼Œ**ä¼ å…¥`margins=True`æ·»åŠ åˆ†é¡¹å°è®¡ã€‚è¿™å°†ä¼šæ·»åŠ æ ‡ç­¾ä¸º`All`çš„è¡Œå’Œåˆ—ï¼Œå…¶å€¼å¯¹åº”äºå•ä¸ªç­‰çº§ä¸­æ‰€æœ‰æ•°æ®çš„åˆ†ç»„ç»Ÿè®¡**ï¼š
```python
In [132]: tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'],
   .....:                  columns='smoker', margins=True)
Out[132]: 
                 size                       tip_pct                    
smoker             No       Yes       All        No       Yes       All
time   day                                                             
Dinner Fri   2.000000  2.222222  2.166667  0.139622  0.165347  0.158916
       Sat   2.555556  2.476190  2.517241  0.158048  0.147906  0.153152
       Sun   2.929825  2.578947  2.842105  0.160113  0.187250  0.166897
       Thur  2.000000       NaN  2.000000  0.159744       NaN  0.159744
Lunch  Fri   3.000000  1.833333  2.000000  0.187735  0.188937  0.188765
       Thur  2.500000  2.352941  2.459016  0.160311  0.163863  0.161301
All          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803
```

è¿™é‡Œï¼Œ**Allå€¼ä¸ºå¹³å‡æ•°**ï¼šä¸å•ç‹¬è€ƒè™‘çƒŸæ°‘ä¸éçƒŸæ°‘ï¼ˆAllåˆ—ï¼‰ï¼Œä¸å•ç‹¬è€ƒè™‘è¡Œåˆ†ç»„ä¸¤ä¸ªçº§åˆ«ä¸­çš„ä»»ä½•å•é¡¹ï¼ˆAllè¡Œï¼‰ã€‚

è¦**ä½¿ç”¨å…¶ä»–çš„èšåˆå‡½æ•°ï¼Œå°†å…¶ä¼ ç»™`aggfunc`å³å¯**ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨countæˆ–lenå¯ä»¥å¾—åˆ°æœ‰å…³åˆ†ç»„å¤§å°çš„äº¤å‰è¡¨ï¼ˆè®¡æ•°æˆ–é¢‘ç‡ï¼‰ï¼š
```python
In [133]: tips.pivot_table('tip_pct', index=['time', 'smoker'], columns='day',
   .....:                  aggfunc=len, margins=True)
Out[133]: 
day             Fri   Sat   Sun  Thur    All
time   smoker                               
Dinner No       3.0  45.0  57.0   1.0  106.0
       Yes      9.0  42.0  19.0   NaN   70.0
Lunch  No       1.0   NaN   NaN  44.0   45.0
       Yes      6.0   NaN   NaN  17.0   23.0
All            19.0  87.0  76.0  62.0  244.0
```

å¦‚æœå­˜åœ¨ç©ºçš„ç»„åˆï¼ˆä¹Ÿå°±æ˜¯NAï¼‰ï¼Œä½ å¯èƒ½ä¼šå¸Œæœ›è®¾ç½®ä¸€ä¸ª`fill_value`ï¼š
```python
In [134]: tips.pivot_table('tip_pct', index=['time', 'size', 'smoker'],
   .....:                  columns='day', aggfunc='mean', fill_value=0)
Out[134]: 
day                      Fri       Sat       Sun      Thur
time   size smoker                                        
Dinner 1    No      0.000000  0.137931  0.000000  0.000000
            Yes     0.000000  0.325733  0.000000  0.000000
       2    No      0.139622  0.162705  0.168859  0.159744
            Yes     0.171297  0.148668  0.207893  0.000000
       3    No      0.000000  0.154661  0.152663  0.000000
            Yes     0.000000  0.144995  0.152660  0.000000
       4    No      0.000000  0.150096  0.148143  0.000000
            Yes     0.117750  0.124515  0.193370  0.000000
       5    No      0.000000  0.000000  0.206928  0.000000
Yes     0.000000  0.106572  0.065660  0.000000
...                      ...       ...       ...       ...
Lunch  1    No      0.000000  0.000000  0.000000  0.181728
            Yes     0.223776  0.000000  0.000000  0.000000
       2    No      0.000000  0.000000  0.000000  0.166005
            Yes     0.181969  0.000000  0.000000  0.158843
       3    No      0.187735  0.000000  0.000000  0.084246
            Yes     0.000000  0.000000  0.000000  0.204952
       4    No      0.000000  0.000000  0.000000  0.138919
            Yes     0.000000  0.000000  0.000000  0.155410
       5    No      0.000000  0.000000  0.000000  0.121389
       6    No      0.000000  0.000000  0.000000  0.173706
[21 rows x 4 columns]
```

pivot_table çš„å‚æ•°è¯´æ˜è¯·å‚è§ä¸‹è¡¨ï¼š

![](https://gitee.com/veal98/images/raw/master/img/20200616114027.png)

### 2. äº¤å‰è¡¨ crosstab

ğŸ”´ **äº¤å‰è¡¨ï¼ˆcross-tabulationï¼Œç®€ç§°crosstabï¼‰æ˜¯ä¸€ç§ç”¨äºè®¡ç®—åˆ†ç»„é¢‘ç‡çš„ç‰¹æ®Šé€è§†è¡¨**ã€‚çœ‹ä¸‹é¢çš„ä¾‹å­ï¼š
```python
In [138]: data
Out[138]:
   Sample Nationality    Handedness
0       1         USA  Right-handed
1       2       Japan   Left-handed
2       3         USA  Right-handed
3       4       Japan  Right-handed
4       5       Japan   Left-handed
5       6       Japan  Right-handed
6       7         USA  Right-handed
7       8         USA   Left-handed
8       9       Japan  Right-handed
9      10         USA  Right-handed
```

ä½œä¸ºè°ƒæŸ¥åˆ†æçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³è¦æ ¹æ®å›½ç±å’Œç”¨æ‰‹ä¹ æƒ¯å¯¹è¿™æ®µæ•°æ®è¿›è¡Œç»Ÿè®¡æ±‡æ€»ã€‚è™½ç„¶å¯ä»¥ç”¨pivot_tableå®ç°è¯¥åŠŸèƒ½ï¼Œä½†æ˜¯`pandas.crosstab`å‡½æ•°ä¼šæ›´æ–¹ä¾¿ï¼š
```python
In [139]: pd.crosstab(data.Nationality, data.Handedness, margins=True)
Out[139]: 
Handedness   Left-handed  Right-handed  All
Nationality
Japan                  2             3    5
USA                    1             4    5
All                    3             7   10
```

crosstabçš„å‰ä¸¤ä¸ªå‚æ•°å¯ä»¥æ˜¯æ•°ç»„æˆ–Seriesï¼Œæˆ–æ˜¯æ•°ç»„åˆ—è¡¨ã€‚å°±åƒå°è´¹æ•°æ®ï¼š
```python
In [140]: pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)
Out[140]: 
smoker        No  Yes  All
time   day                
Dinner Fri     3    9   12
       Sat    45   42   87
       Sun    57   19   76
       Thur    1    0    1
Lunch  Fri     1    6    7
       Thur   44   17   61
All          151   93  244
```

---

# ğŸ“š References

- ğŸ“•  [ã€Šåˆ©ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æ-ç¬¬2ç‰ˆ-ä¸­æ–‡è¯‘ç‰ˆã€‹](https://www.jianshu.com/p/04d180d90a3f)

  <img src="https://gitee.com/veal98/images/raw/master/img/20200607091609.png" style="zoom:50%;" />

- ğŸš [Gihubã€ŠPythonæ•°æ®åˆ†æã€‹é…å¥—æºç ](https://github.com/wesm/pydata-book)